
[TOC]

# 第9章 EM算法及其推广

EM算法是一种迭代算法，用于含有隐变量（hidden variable）的概率模型参数的极大似然估计，或极大后验概率估计
EM算法的每次迭代由两步组成：

- E步，求期望（expectation）
- M步，求极大（maximization）

## 9.1 EM算法的引入

### 9.1.1 EM算法
**例子：**
假设3枚硬币，分别记作A,B,C。这些硬币正面出现的概率分别是$\pi, p, q$。
进行如下掷硬币试验：

- 先掷硬币A，根据其结果选出硬币B或硬币C，正面B，反面C
- 然后掷选出的硬币，掷硬币的结果，出现正面记作1，反面记作0
- 独立地重复n次试验（例子中n=10）

观测结果如下：1,1,0,1,0,0,1,0,1,1
假设只能观测到掷硬币的结果，不能观测过程，问如何估计三硬币正面出现的概率，即三硬币模型的参数
**解：**
三硬币模型可以写作
$$\begin{align}
P(y|\theta) & = \sum_z P(y,z|\theta) = \sum_z P(z|\theta) P(y|z,\theta) \\
&= \pi p^y (1-p)^{1-y} + (1-\pi) q^y (1-q)^{1-y}
\tag{9.1}
\end{align}$$
这里：y是观测变量；z是隐变量（硬币A的结果）；$\theta = (\pi,p,q)$是模型参数
注：y可观测，z不可观测
将观测数据表示为$Y=(Y_1,Y_2,\cdots,Y_n)^T$，未观测数据表示为$Z=(Z_1,Z_2,\cdots,Z_n)^T$，则观测数据的似然函数是：
$$
P(Y|\theta) = \sum_Z P(Z|\theta) P(Y|Z,\theta)
\tag{9.2}
$$
即：
$$
P(Y|\theta) = \prod_{j=1}^n [\pi p^{y_j}(1-p)^{1-y_j} + (1-\pi) q^{y_j}(1-q)^{1-y_j}]
\tag{9.3}
$$
考虑求模型参数$\theta = (\pi ,p,q)$的极大似然估计，即
$$
\hat{\theta} = \arg \max_\theta \log P(Y|\theta)
\tag{9.4}
$$
这个问题没有解析解，只能通过迭代法——EM算法求解
首先选取参数初值，记作$\theta^{(0)} = (\pi^{(0)},p^{(0)},q^{(0)})$，然后通过EM迭代计算参数估计值，知道收敛
第i次迭代参数的估计值是$\theta^{(i)} = (\pi^{(i)},p^{(i)},q^{(i)})$
其第i+1次迭代如下：
**E步：**计算模型参数$\theta^{(i)} = (\pi^{(i)},p^{(i)},q^{(i)})$下观测数据$y_j$来自掷硬币B的概率
$$
\mu_j^{(i+1)} = 
\frac{\pi^{(i)}\left(p^{(i)}\right)^{y_{j}}\left(1-p^{(i)}\right)^{1-y_{j}}}{\pi^{(i)}\left(p^{(i)}\right)^{y_{j}}\left(1-p^{(i)}\right)^{1-y_{j}}+\left(1-\pi^{(i)}\right)\left(q^{(i)}\right)^{y_{j}}\left(1-q^{(i)}\right)^{1-y_{j}}}
\tag{9.5}
$$
**M步：**计算模型参数的新估计值
$$\begin{align}
\pi^{(i+1)} = \frac{1}{n} \sum_{j=1}^n \mu_j^{(i+1)} \tag{9.6} \\
p^{(i+1)} = \frac{\sum_{j=1}^n \mu_j^{(i+1)} y_j}{\sum_{j=1}^n \mu_j^{(i+1)}} \tag{9.7} \\
q^{(i+1)}=\frac{\sum_{j=1}^{n}\left(1-\mu_{j}^{(i+1)}\right) y_{j}}{\sum_{j=1}^{n}\left(1-\mu_{j}^{(i+1)}\right)} \tag{9.8} \\
\end{align}$$
进行数字计算，假设模型参数初值为
$$
\pi^{(0)}=0.5,\,p^{(0)}=0.5,\,q^{(0)}=0.5
$$
由(9.5)，对$y_j=1$与$y_j=0$，均有$\mu_j^{(1)} = 0.5$
利用迭代公式(9.6)~(9.8)，得到
$$
\pi^{(1)}=0.5,\,p^{(1)}=0.6,\,q^{(1)}=0.6
$$
由(9.5)，
$$
\mu_j^{(2)} = 0.5,\,j=1,2,\cdots,10
$$
继续迭代，得：
$$
\pi^{(2)}=0.5,\,p^{(2)}=0.6,\,q^{(2)}=0.6
$$
于是得到模型参数的极大似然估计：
$$
\hat{\pi} = 0.5,\, \hat{p} = 0.6,\, \hat{q} = 0.6
$$
注：初值不同，得到的参数估计值也可能不同

#### **算法9.1（EM算法）**
**输入：** 观测变量数据Y，隐变量数据Z，联合分布$P(Y,Z|\theta)$，条件分布$P(Z|Y,\theta)$
**输出：** 模型参数$\theta$

1. 选择参数的初值$\theta^{(0)}$，开始迭代
2. E步
记$\theta^{(i)}$为第i词迭代参数$\theta$的估计值，在第i+1词迭代的E步，计算：
$$\begin{align}
Q(\theta, \theta^{(i)}) &= E_Z [\log P(Y,Z|\theta) | Y,\theta^{(i)}] \\ 
&= \sum_Z \log P(Y,Z|\theta) P(Z| Y,\theta^{(i)})
\tag{9.9}
\end{align}$$
这里，$P(Z|Y,\theta^{(i)})$是在给定观测数据Y和当前的参数估计$\theta^{(i)}$下，隐变量数据Z的条件概率分布
3. M步
求使$Q(\theta, \theta^{(i)})$极大化的$\theta$，确定第$i+1$次迭代的参数的估计值$\theta^{(i+1)}$
$$
\theta^{(i+1)} = \arg \max_{\theta} Q(\theta, \theta^{(i)})
\tag{9.10}
$$
4. 重复(2)和(3)，直到收敛
式(9.9)的函数$Q(\theta, \theta^{(i)})$是EM算法的核心，称为Q函数（Q function）

#### **定义9.1（Q函数）**
完全数据的对数似然函数$\log P(Y,Z|\theta)$关于在给定观测数据Y和当前参数$\theta^{(i)}$下，对未观测数据$Z$的条件概率分布$P(Z|Y,\theta^{(i)})$的期望称为Q函数，即：
$$
Q(\theta, \theta^{(i)}) =  E_Z [\log P(Y,Z|\theta) | Y,\theta^{(i)}]
\tag{9.11}
$$
下面关于EM算法作几点说明：

- 步骤(1)：参数的初值可以任意选择，但需注意EM算法对初值是敏感的
- 步骤(2)：E步求$Q(\theta, \theta^{(i)})$，Q函数式中，Z是未观测数据，Y是观测数据。注意，$Q(\theta, \theta^{(i)})$的第1个变元表示要极大化的参数，第2个变元表示参数的当前估计值，每次迭代实际在求Q函数及其极大
- 步骤(3)：M步求$Q(\theta, \theta^{(i)})$的极大化，得到$\theta^{(i)}$，完成一次迭代$\theta^{(i)} \to \theta^{(i+1)}$
- 步骤(4)：给出停止迭代的条件，一般是较小的正数$\epsilon_1, \epsilon_2$，若满足
$$
\| \theta^{(i+1)} - \theta^{(i)} \| < \epsilon_1
$$
**或**
$$
\| $Q(\theta^{(i+1)}, \theta^{(i)})$ - $Q(\theta^{(i)}, \theta^{(i)})$ \| < \epsilon_2
$$
则停止迭代

### 9.1.2 EM算法的导出
【注】这里的推导有点问题，看不懂，其中利用Jensen不等式推导的式子中，最后一个等号一步可能是错的
在另外的专题中写推导过程

简单来说，利用Jesnsen不等式推导出似然函数的残差的下界，然后逼近

### 9.1.3 EM算法在非监督学习中的应用
训练数据是(X,Y)，但Y未标注时，
可以认为非监督学习训练数据是联合概率分布产生的数据。X为观测数据，Y为未观测数据

## 9.2 EM算法的收敛性
#### **定理9.1**
设$P(Y|\theta)$为观测数据的似然函数，$\theta^{(i)}\,(i=1,2,\cdots)$为EM算法得到的参数估计序列，$P(Y|\theta^{(i)})\,(i=1,2,\cdots)$为对应的似然函数序列，则$P(Y|\theta^{(i)})$是单调递增的，即：
$$
P(Y|\theta^{(i+1)}) \ge P(Y|\theta^{(i)})
\tag{9.18}
$$

#### **定理9.2**
设$L(\theta) = \log P(Y|\theta)$为观测数据的对数似然函数，$\theta^{(i)}\,(i=1,2,\cdots)$为EM算法得到的参数估计序列，$L(\theta^{(i)})\,(i=1,2,\cdots)$为对应的对数似然函数序列

1. 如果$P(Y|\theta)$有上界，则$L(\theta^{(i)}) = \log P(Y| \theta^{(i)})$收敛到某一值$L^*$
2. 在函数$Q(\theta,\theta^\prime)$与$L(\theta)$满足一定条件下，由EM算法得到的参数估计序列$\theta^{(i)}$的收敛值$\theta^*$是$L(\theta)的稳定点

## 9.3 EM算法在高斯混合模型学习中的应用
### 9.3.1 高斯混合模型
#### 定义9.2（高斯混合模型）
高斯混合模型是指具有如下形式的概率分布模型
$$
P(y|\theta) = \sum_{k=1}^K \alpha_k \phi(y|\theta_k)
\tag{9.24}
$$
其中，$\alpha_k$是系数，$\alpha_k \ge 0$，$\sum_{k=1}^K \alpha_k = 1$；
$\phi(y|\theta_k)$是高斯分布密度，$\theta_k = (\mu_k, \sigma_k^2)$，
$$
\phi(y|\theta_k) = - \frac{1}{\sqrt{2 \pi} \sigma_k} \exp \left( - \frac{(y-\mu_k)^2}{2 \sigma_k^2} \right)
\tag{9.25}
$$
称为第k个分模型

本质上，就是由多个高斯分布组成的分布
一般混合模型可以由任意概率分布密度代替式(9.25)中的高斯分布密度

### 9.3.2 高斯混合模型参数估计的EM算法
假设观测数据$y_1, y_2,\cdots,y_N$，由高斯混合模型生成：
$$
P(y|\theta) = \sum_{k=1}^K \alpha_k \phi(y|\theta_k)
\tag{9.26}
$$
其中，$\theta = (\alpha_1,\alpha_2,\cdots,\alpha_K;\theta_1,\theta_2,\cdots,\theta_K)$

通过EM算法估计高斯混合模型的参数$\theta$

1. 明确隐变量，写出完全数据的对数似然函数
可以设想观测数据$y_j, j=1,2,\cdots,N$是这样产生的：首先依概率$\alpha_k$选择第k个高斯分布分模型$\phi(y|\theta_k)$；然后依第k个分模型的概率分布$\phi(y|\theta_k)$生成观测数据$y_j$。
这时观测数据$y_j,j=1,2,\cdots,N$是已知的；反映观测数据$y_j$来自第k个分模型的数据是未知的，$k=1,2,\cdots,K$，以隐变量$\gamma_{jk}$表示，其定义如下：
$$
\gamma_{jk} = 
\begin{cases}
1, &第j个观测来自第k个分模型\\
0, &否则
\end{cases}
\\
j = 1,2,\cdots,N;\, k=1,2,\cdots,K
\tag{9.27}
$$
$\gamma_{jk}$是0-1随机变量
有了观测数据$y_j$及未观测数据$\gamma_{jk}$，那么完全数据是：
$$
(y_j, \gamma_{j1},\gamma_{j2},\cdots,\gamma_{jK}),k=1,2,\cdots,N
$$
于是可以写出完全数据的似然函数
$$\begin{align}
P(y,\gamma|\theta) 
&=\prod_{j=1}^N P(y_j, \gamma_{j1},\gamma_{j2},\cdots,\gamma_{jK}),j=1,2,\cdots,N \\
&=\prod_{k=1}^K \prod_{j=1}^N \left[ \alpha_k \phi(y_j | \theta_k) \right]^{\gamma_{jk}} \\
&=\prod_{k=1}^K \alpha_k^{n_k} \prod_{j=1}^N \left[\phi(y_j | \theta_k) \right]^{\gamma_{jk}} \\
&=\prod_{k=1}^K \alpha_k^{n_k} \prod_{j=1}^N \left[ \frac{1}{\sqrt{2 \pi} \sigma_k} \exp \left( - \frac{(y_j-\mu_k)^2}{2 \sigma_k^2} \right) \right]^{\gamma_{jk}}
\end{align}$$
式中，$n_k = \sum_{j=1}^N \gamma_{jk}, \sum_{k=1}^K n_k = N$
那么，完全数据的对数似然函数：
$$
\log P(y,\gamma|\theta) = \sum_{k=1}^K \left\{ n_k \log \alpha_k + \sum_{j=1}^N \gamma_{jk} \left[ \log \left( \frac{1}{\sqrt{2 \pi}} \right) - \log \sigma_k - \frac{1}{2 \sigma_k^2} (y_j -\mu_k)^2  \right] \right\}
\tag{9.28}
$$
2. EM算法的E步，确定Q函数
$$\begin{align}
Q(\theta,\theta^{(i)}) 
&= E[\log P(y,\gamma|\theta) | y, \theta^{(i)}] \\
&= E \left\{ \sum_{k=1}^K \left\{ n_k \log \alpha_k + \sum_{j=1}^N \gamma_{jk} \left[ \log \left( \frac{1}{\sqrt{2 \pi}} \right) - \log \sigma_k - \frac{1}{2 \sigma_k^2} (y_j -\mu_k)^2  \right] \right\}  \right\} \\
&= \sum_{k=1}^K \left\{ \sum_{j=1}^N (E \gamma_{jk}) \log \alpha_k + \sum_{j=1}^N (E\gamma_{jk}) \left[ \log \left( \frac{1}{\sqrt{2 \pi}} \right) - \log \sigma_k - \frac{1}{2 \sigma_k^2} (y_j -\mu_k)^2  \right] \right\}
\end{align}$$
这里需要计算$E(\gamma_{jk}|y,\theta)$，记作$\hat{\gamma}_{jk}$
$$\begin{align}
\hat{\gamma}_{jk}
&= E(\gamma_{jk}|y,\theta) \\
&= \frac{P(\gamma_{jk} = 1, y_j | \theta)}{\sum_{k=1}^K P(\gamma_{jk} = 1, y_j | \theta)} \\
&= \frac{P(y_j |\gamma_{jk} = 1,  \theta) P(\gamma_{jk}=1|\theta)}{\sum_{k=1}^K P(y_j |\gamma_{jk} = 1,  \theta) P(\gamma_{jk}=1|\theta)} \\
&= \frac{\alpha_k \phi(y_j | \theta_k)}{\sum_{k=1}^K \alpha_k \phi(y_j | \theta_k)}\\
j=1,2,\cdots,N; k=1,2,\cdots,K
\end{align}$$
$\hat{gamma}_{jk}$是当前模型参数下，第j个观测数据来自第k个模型的概率，称为分模型k对观测数据$y_j$的响应度
将$\hat{gamma}_{jk}$及$n_k = \sum_{j=1}^N E \gamma_{jk}$代入(9.28)得：
$$
Q(\theta, \theta^{(i)}) = \sum_{k=1}^K \left\{ n_k \log \alpha_k + \sum_{j=1}^N \hat{\gamma}_{jk} \left[ \log \left( \frac{1}{\sqrt{2 \pi}} \right) - \log \sigma_k - \frac{1}{2 \sigma_k^2} (y_j -\mu_k)^2  \right] \right\}
\tag{9.29}
$$
3. 确定EM算法的M步
迭代的M步是求函数$Q(\theta,\theta^{(i)})$对$\theta$对极大值，即求新一轮迭代的模型参数：
$$
\theta^{(i+1)} = \arg \max_\theta Q(\theta, \theta^{i})
$$
用$\hat{\mu}_k, \hat{\sigma}_k^2$及$\hat{\alpha}_k$,k=1,2,\cdots,K，表示$\theta^{(i+1)}$的各参数。
求$\hat{\mu}_k, \hat{\sigma}_k^2$只需将(9.29)分别对$\mu_k, \sigma_k^2$求偏导数并令其为0，即可得到；求$\hat{\alpha}_k$是在$\sum_{k=1}^K \alpha_k = 1$条件下求偏导数并令其为0得到的
结果如下：
$$
\hat{\mu}_{k}=\frac{\sum_{j=1}^{N} \hat{\gamma}_{j k} y_{j}}{\sum_{j=1}^{N} \hat{\gamma}_{j k}}, k=1,2,\cdots,K
\tag{9.30}
$$
$$
\hat{\sigma}_{k}^{2}=\frac{\sum_{j=1}^{N} \hat{\gamma}_{j k}\left(y_{j}-\mu_{k}\right)^{2}}{\sum_{j=1}^{N} \hat{\gamma}_{j k}} , k=1,2,\cdots,K
\tag{9.31}
$$
$$
\hat{\alpha}_{k}=\frac{n_{k}}{N}=\frac{\sum_{j=1}^{N} \hat{\gamma}_{j k}}{N} , k=1,2,\cdots,K
\tag{9.32}
$$
重复直到其对数似然函数值不再模型变化为止

#### 算法9.2（高斯混合模型参数估计的EM算法）
**输入：** 观测数据$y_1,y_2,\cdots,y_N$，高斯混合模型
**输出：** 高斯混合模型参数

1. 去参数的初始值开始迭代
2. E步：依据当前模型参数，计算分模型k对观测数据$y_j$的响应度
$$\begin{align}
\hat{\gamma}_{jk} = \frac{\alpha_k \phi(y_j | \theta_k)}{\sum_{k=1}^K \alpha_k \phi(y_j | \theta_k)} \\
j=1,2,\cdots,N; k=1,2,\cdots,K
\end{align}$$
3. M步：计算新一轮迭代的模型参数
$$\begin{align}
\hat{\mu}_{k}=\frac{\sum_{j=1}^{N} \hat{\gamma}_{j k} y_{j}}{\sum_{j=1}^{N} \hat{\gamma}_{j k}}, k=1,2,\cdots,K \\
\hat{\sigma}_{k}^{2}=\frac{\sum_{j=1}^{N} \hat{\gamma}_{j k}\left(y_{j}-\mu_{k}\right)^{2}}{\sum_{j=1}^{N} \hat{\gamma}_{j k}} , k=1,2,\cdots,K \\
\hat{\alpha}_{k}=\frac{n_{k}}{N}=\frac{\sum_{j=1}^{N} \hat{\gamma}_{j k}}{N} , k=1,2,\cdots,K
\end{align}$$
4. 重复2和3，直到收敛

## 9.4 EM算法的推广
EM算法还可以解释F函数（F function）的极大-极大算法（maximization maximization algorithm），基于这个解释有若干变形和推广
如广义期望极大（generalized expectation maximization, GEM）算法

### 9.4.1 F函数的极大-极大算法
首先引进F函数并讨论其性质

#### 定义9.3（F函数）
假设隐变量数据Z的概率分布为$\tilde{P}(Z)$，定义分布$\tilde{P}$与参数$\theta$的函数$F(\tilde{P},\theta)$如下：
$$
F(\tilde{P},\theta) = E_{\tilde{P}} [\log P(Y,Z | \theta)] + H(\tilde{P})
\tag{9.33}
$$
称为F函数。式中，$H(\tilde{P}) = -E_{\tilde{P}} \log \tilde{P}(Z)$是分布$\tilde{P}(Z)$的熵
在定义9.3中，通常假设$P(Y,Z|\theta)$是$\theta$的连续函数，因而$F(\tilde{P},\theta)$是$\tilde{P}$和$\theta$的连续函数。
函数$F(\tilde{P},\theta)$还有以下重要性质：

#### 引理9.1
对于固定的$\theta$，存在唯一的分布$\tilde{P}_\theta$极大化$F(\tilde{P},\theta)$，这时，$\tilde{P}_\theta$由下式给出：
$$
\tilde{P}_\theta (Z) = P(Z|Y,\theta)
\tag{9.34}
$$
并且$\tilde{P}_\theta$随$\theta$连续变化

#### 引理9.2
若$\tilde{P}_\theta(Z) = P(Z|Y,\theta)$，则：
$$
F(\tilde{P}, \theta) = \log P(Y|\theta)
\tag{9.36}
$$

#### 定理9.3
设$L(\theta)=\log P(Y|\theta)$作为观测数据的对数似然函数，$\theta^{(i)},i=1,2,\cdots$为EM算法得到的参数估计序列，函数$F(\tilde{P},\theta)$由式(9.33)定义。
如果$F(\tilde{P},\theta)$在$\tilde{P}^*$和$\theta^*$有局部极大值，那么$L(\theta)$也在$\theta^*$有局部最大值
类似地，如果$F(\tilde{P},\theta)$在$\tilde{P}^*$和$\theta^*$达到全局最大值，那么$L(\theta)$也在$\theta^*$达到全局最大值

#### 定理9.4
EM算法的一次迭代可由F函数的极大-极大算法实现
设$\theta^{(i)}$为第i次迭代参数$\theta$的估计，$\tilde{P}^{(i)}$为第i次迭代函数$\tilde{P}$的估计。在第i+1次迭代的两步为：

1. 对固定的$\theta^{(i)}$，求$\tilde{P}^{(i+1)}$使$F(\tilde{P},\theta^{(i)})$极大化
2. 对固定迭代$\tilde{P}^{(i+1)}$，求$\theta^{(i+1)}$使$F(\tilde{P}^{(i+1)},\theta)$极大化


### 9.4.2 GEM算法
#### 算法9.3（GEM算法1）

**输入：** 观测数据，F函数
**输出：** 模型参数

1. 初始化参数$\theta^{(0)}$，开始迭代
2. 第i+1次迭代，第1步：记$\theta^{(i)}$为参数$\theta$的估计值，$\tilde{P}^{(i)}$为函数$\tilde{P}$的估计
求$\tilde{P}^{(i+1)}$使$\tilde{P}$极大化$F(\tilde{P},\theta^{(i)})$
3. 第2步：求$\theta^{(i+1)}$使$F(\tilde{P}^{(i+1)},\theta)$极大化
4. 重复2和3，直到收敛

在GEM算法1中，有时求$Q(\theta,\theta^{(i)})$的极大化是很困难的
算法2和3并不直接求$\theta^{(i+1)}$使$Q(\theta,\theta^{(i)})$达到极大的$\theta$，而是找一个$\theta^{i+1}$使得$Q(\theta^{(i+1)},\theta^{(i)})>Q(\theta^{(i)},\theta^{(i)})$

#### 算法9.4（GEM算法2）
**输入：** 观测数据，Q函数
**输出：** 模型参数

1. 初始化参数$\theta^{(0)}$，开始迭代
2. 第i+1次迭代，第1步：记$\theta^{(i)}$为参数$\theta$的估计值，计算：
$$\begin{align}
Q(\theta, \theta^{(i)}) &= E_Z [\log P(Y,Z| \theta) | Y, \theta^{(i)}] \\
&= \sum_Z P(Z|Y, \theta^{(i)}) \log P(Y,Z|\theta)
\end{align}$$
3. 第2步：求$\theta^{(i+1)}$使
$$
Q(\theta^{(i+1)},\theta^{(i)})>Q(\theta^{(i)},\theta^{(i)})
$$
4. 重复2和3，直到收敛

当参数$\theta$的维数为$d(d \ge 2)$时，可采用一种特殊的GEN算法，它将EM算法的M步分解为$d$次条件极大化，每次只改变参数向量的一个分量，其余分量不改变


#### 算法9.5（GEM算法3）
**输入：** 观测数据，Q函数
**输出：** 模型参数

1. 初始化参数$\theta^{(0)} = (\theta_1^{(0)},\theta_2^{(0)},\cdots,\theta_d^{(0)})$，开始迭代
2. 第i+1次迭代，第1步：
记$\theta^{(i)} = (\theta_1^{(i)},\theta_2^{(i)},\cdots,\theta_d^{(i)})$为参数$\theta = (\theta_1, \theta_2, \cdots, \theta_d)$的估计值，计算
$$\begin{align}
Q(\theta, \theta^{(i)}) &= E_Z [\log P(Y,Z| \theta) | Y, \theta^{(i)}] \\
&= \sum_Z P(Z|Y, \theta^{(i)}) \log P(Y,Z|\theta)
\end{align}$$
3. 第2步：进行d次条件极大化
首先，在$\theta_2^{(i)},\cdots,\theta_k^{(i)}$保持不变的条件下，求使$Q(\theta, \theta^{(i)})$达到极大的$\theta_1^{(i+1)}$
然后，在$\theta_1 = \theta_1^{(i+1)}$，$\theta_j = \theta_j^{(i)}$，$j=3,4,\cdots,k$的条件下求使$Q(\theta,\theta^{(i)})$达到极大的$\theta_2^{(i+1)}$；
继续如此，经过d次条件极大化，得到$\theta^{(i+1)} = (\theta_1^{(i+1)},\theta_2^{(i+1)},\cdots,\theta_d^{(i+1)})$使得
$$
Q(\theta^{(i+1)},\theta^{(i)})>Q(\theta^{(i)},\theta^{(i)})
$$
4. 重复2和3，直到收敛


