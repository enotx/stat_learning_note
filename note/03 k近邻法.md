# 第3章 k近邻法
k近邻法（k-nearest neighbor，k-NN）

## 3.1 k近邻算法
#### **算法3.1 （k近邻算法）**
输入：

- 训练数据集：$T = \{ (x_1,y_1),(x_2,y_2), \cdots, (x_N,y_N) \}$
其中，$x_i \in \mathcal{x} \subseteq \mathbb{R}^n$为实例的特征向量，$y_i \in \mathcal{Y} = {c_1, c_2, \cdots, c_K}$为实例的类别，$i = 1,2, \cdots,N$；
- 实例特征向量x

输出：实例x所属的类y

(1) 根据给定的距离度量，在训练集T中找出与x最邻近的k个点，涵盖这k个点的x的邻域记作$N_k(x)$
(2) 在$N_k(x)$中根据分类决策规则（如多数表决）决定x的类别y：
$$
y = \arg \max_{c_j} \sum_{x_i \in N_k(x)} I(y_i = c_j), i=1,2,\cdots, N; j=1,2,\cdots,K
\tag{3.1}
$$
其中，I为指示函数，即当$y_i=c_j$时$I=1$，否则为0
特殊情况：k=1时，称为最近邻算法
【注】k近邻法没有显式的学习过程

## 3.2 k近邻模型
模型三个基本要素：

1. 距离度量
2. k值
3. 分类决策规则

### 3.2.1 模型

### 3.2.2 距离度量

一般用欧式距离，也可以用：更一般的$L_p$距离/Minkowski距离

设特征空间$\mathcal{X}$是n维实数向量空间$\mathbb{R}^n$，$x_i,x_j \in \mathcal{X}$，$x_i = (x_i^{(1)},x_i^{(2)},\cdots,x_i^{(n)})^T$，$x_j = (x_j^{(1)},x_j^{(2)},\cdots,x_j^{(n)})^T$。$x_i, x_j$的$L_p$距离定义为：
$$
L_p(x_i,x_j) = \left( \sum_{l=1}^n \vert x_i^{(l)} - x_j^{(l)} \vert ^p \right) ^{\frac{1}{p}}
\tag{3.2}
$$
这里$p \ge 1$
当$p=2$时，称为欧氏距离（Euclidean distance），即：
$$
L_2(x_i,x_j) = \left( \sum_{l=1}^n \vert x_i^{(l)} - x_j^{(l)} \vert ^2 \right) ^{\frac{1}{2}}
\tag{3.3}
$$
当$p=1$时，称为曼哈顿距离（Manhattan distance），即：
$$
L_1(x_i,x_j) = \sum_{l=1}^{n} |x_i^{(l)} - x_j^{(l)}|
\tag{3.4}
$$
当$p = \infty$时，它是各个坐标距离的最大值（切比雪夫距离？），即：
$$
L_1(x_i,x_j) = \max_l |x_i^{(l)} - x_j^{(l)}|
\tag{3.5}
$$

### 3.2.3 k值的选择

### 3.2.4 分类决策规则
通常是多数表决规则（majority voting rule）：
如果分类的损失函数是0-1损失函数，分类函数为：
$$
f:\mathbb{R}^n \to \left\{ c_1, c_2, \cdots, c_K \right\}
$$
那么误分类的概率是：
$$
P(Y \ne f(X)) = 1-P(Y=f(X))
$$

对给定的实例$x \in \mathcal{X}$，其最邻近的k个训练实例点构成集合$N_k(x)$
如果涵盖$N_k(x)$的区域的类别是$c_j$，那么误分类率是：
$$
\frac{1}{k} \sum_{x_i \in N_k(x)} I(y_i \ne c_j) = 1- \frac{1}{k} \sum_{x_i \in N_k(x)} I (y_i = c_j)
$$
要使误分类率最小，即经验风险最小，就要使$\sum_{x_i \in N_k(x)} I(y_i = c_j)$最大
所以多数表姐规则等价于经验风险最小化


## 3.3 k近邻法的实现：kd树
### 3.3.1 构造kd树
原理：不断用垂直于坐标轴的超平面将k维空间划分，构成一系列的k维超矩阵区域，kd树的每一个节点对应于一个k维超矩形区域

构造方法如下：

1. 构造根结点，使根节点对应于k维空间中包含所有实例点的超矩形区域；
2. 通过以下递归方法，不断对k维空间切分，生成子结点
3. 在超矩形区域（结点）上选择一个坐标轴和坐标轴上的一个切分点，确定一个超平面（过切分点，垂直于坐标轴）——超平面将超矩形切分为两个子区域（子结点）
4. 直到子区域内没有实例时为止（终止的节点为叶结点）

通常，依次选择坐标轴，选择训练实例点在坐标轴上的中位数为切分点——得到平衡的kd树，但未必效率最优

#### **算法3.2（构造平衡kd树）**
**输入：** k维空间数据集$T={x_1,x_2, \cdots, x_N}$
其中$x_i = (x_i^{(1)},x_i^{(2)},\cdots, x_i^{(k)})^T$，$i=1,2,\cdots,N$;
**输出：** kd树

1. 开始：构造根结点，根结点对应包含T的k维空间的超矩形区域
选择$x^{(1)}$为坐标轴，以T中所有实例的$x^{(1)}$坐标的中位数为切分点，切分成两个子区域，即深度为1的左右子结点
将落在切分超平面上的实例点保存在根结点
2. 重复：对深度为j的结点，选择$x^{(l)}$为切分的坐标轴，$l=j(\mod k) + 1$
以该结点的区域中所有实例的$x^{(l)}$坐标的中位数为切分点
生成深度为j+1的左右子结点
3. 直到两个子区域没有实例存在时停止，从而形成kd树的区域划分

### 3.3.2 搜索kd树
给定一个目标点，搜索其最近邻
1. 首先找到包含目标点的叶结点；
2. 然后从该叶结点出发，依次退回父结点；
3. 不断查找目标点最邻近的结点，当确定不可能存在更近的结点时终止
观察以目标点为中心，过最近点的超球体，可以搜索到超平面不与超球体相交为止

#### **算法3.3 （用kd树的最近邻搜索）**
**输入：** 已构造的kd树，目标点x
**输出：** x的最近邻

1. kd树种找到包含目标结点x的叶结点：从根结点出发，递归地向下访问kd树，若目标点x当前维的坐标小于切分点的坐标，则移动到左子结点，否则右，直到子节点为叶结点为止
2. 以此叶结点为“当前最近点”
3. 递归地向上回退，在每个结点进行以下操作：
- 如果该结点保存的实例点比当前最近点距离目标点更近，则以该实例点为当前最近点
- 当前最近点一定存在于该结点一个子结点对应的区域。检查该子结点的父结点的另一个子结点对应的区域是否有更近的点。具体地：见擦汗另一子结点对应的区域是否与目标点为球心、以目标点与当前最近点见距离为半径的超球体相交。如果相交，可能再另一个子结点对应的区域存在距离更目标点更近的点，移动到另一个子结点，接着递归地进行最近邻搜索；如不相交，向上回退
- 当退回到根结点时，搜索结束，最后的“当前最近点”即为x的最近邻点

较为适合训练实例数远大于空间维数时，平均复杂度是$O(\log N)$

【注】有两个问题暂时没有想得非常清楚：

1. 如果在切分时，同一个中位数超平面有两个或更多实例点怎么办？—— 倾向于没有影响，可以随便选一个，只需要构造平衡的kd树就可以了
2. 如果目标点x就在树中？会有什么影响么——这种情况应该可以不考虑？

