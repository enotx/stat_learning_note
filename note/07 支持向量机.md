$$
\DeclareMathOperator*{\sign}{\text{sign}}
$$


[TOC]

# 第7章 支持向量机

support vector machines, SVM是一种二类分类模型

## 7.1 线性可分支持向量机与硬间隔最大化
由简至繁的模型：
- 线性可分支持向量机（linear support vector machine in linearly separable case）
- 线性支持向量机（linear support vector machine）
- 非线性支持向量机（non-linear support vector machine）


### 7.1.1 线性可分支持向量机

考虑一个二类分类问题
假设输入空间与特征空间为两个不同的空间。输入空间为欧氏空间或离散集合，特征空间为欧式空间或希尔伯特空间。
线性可分支持向量机、线性支持向量机假设这两个空间的元素一一对应，并将输入空间中的输入映射为特征空间中的特征向量。
非线性支持向量机利用一个从输入空间到输出空间的非线性映射，将输入映射为特征向量。
所以，输入都由输入空间转换到特征空间，支持向量机的学习是在特征空间进行的

假设给定的一个特征空间上的训练数据集
$$
T = \{ (x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N) \}
$$
其中，$x_i \in \mathcal{X} = \mathbb{R}^n$，$y_i \in \mathcal{Y} = {+1, -1}$，$i = 1,2,\cdots,N$，$x_i$为第i个特征向量（也称实例），$y_i$为$x_i$的类标记
当$y_i = +1$时，称$x_i$为正例，否则称为负例，$(x_i,y_i)$称为样本点。再假设训练数据集是线性可分的

#### **定义7.1（线性可分支持向量机）**
给定线性可分训练数据集，通过间隔最大化、或等价地求解相应的凸二次规划问题，学习得到分离超平面为：
$$
w^* \cdot x + b^* = 0
\tag{7.1}
$$
以及相应的分类决策函数
$$
f(x) = \sign(w^* \cdot x + b^*)
\tag{7.2}
$$
称为线性可分支持向量机

### 7.1.2 函数间隔和几何间隔
一般而言，实例点距离分离超平面的远近可以表示分类预测的确信程度

#### **定义7.2（函数间隔）**
对于给定的训练数据集T和超平面$(w,b)$
定义超平面$(w,b)$关于样本点$(x_i,y_i)$的函数间隔为：
$$
\hat{\gamma}_i = y_i (w \cdot x_i + b)
\tag{7.3}
$$
定义超平面$(w,b)$关于训练数据集T的函数间隔为超平面$(w,b)$关于T中所有样本点$(x_i,y_i)$的函数间隔之最小值，即
$$
\hat{\gamma} = \min_{i = 1,\cdots,N} \hat{\gamma_i}
\tag{7.4}
$$
还需对超平面的法向量$w$加以约束，如规范化，$\lVert w \rVert = 1$，使间隔确定。此时，函数间隔成为几何间隔

#### **定义7.3（几何间隔）**
对于给定的训练数据集T和超平面$(w,b)$
定义超平面$(w,b)$关于样本点$(x_i,y_i)$的几何间隔为
$$
\gamma_i = y_i \left( \frac{w}{\lVert w \rVert} \cdot x + \frac{b}{\lVert b \rVert} \right)
\tag{7.5}
$$
定义超平面$(w,b)$关于训练数据集T的几何间隔为，超平面$(w,b)$关于T中所有样本点$(x_i,y_i)$的几何间隔之最小值，即
$$
\gamma = \min_{i = 1,\cdots,N} \hat{\gamma_i}
\tag{7.6}
$$
超平面$(w,b)$关于样本点$(x_i,y_i)$的几何距离一般是实例点到超平面的带符号距离（signed distance），当样本点被超平面正确分类时，就是实例点到超平面的距离

从(7.3)~(7.6)可知，函数间隔和几何间隔有下面的关系：
$$
\begin{align}
\gamma_i = \frac{\hat{\gamma}_i}{\lVert w \rVert} \tag{7.7}\\
\gamma = \frac{\hat{\gamma}}{\lVert w \rVert} \tag{7.8}
\end{align}
$$
如果$\lVert w \rVert = 1$，则函数间隔和几何间隔相等。如果超平面参数w和b成比例改变，函数间隔改变，而几何间隔不变

### 7.1.3 间隔最大化

几何间隔最大的分离超平面是唯一的，又称为硬间隔最大化（对应：训练数据集近似线性可分时的软间隔最大化）

#### 1. 最大间隔分离超平面
最大间隔分离超平面的问题，可以表示为下面的约束最优化问题：
$$\begin{align}
\max_{w,b} \quad &\gamma \tag{7.9}\\
s.t. \quad &y_i \left( \frac{w}{\lVert w \rVert} \cdot x + \frac{b}{\lVert b \rVert} \right) \ge \gamma,\ i = 1,2,\cdots,N
\tag{7.10}
\end{align}$$
考虑到(7.8)，可以将问题改写为
$$\begin{align}
\max_{w,b} \quad & \frac{\hat{\gamma}}{\lVert w \rVert} \tag{7.11}\\
s.t.\quad & y_i (w \cdot x_i + b) \ge \hat{\gamma},\ i = 1,2,\cdots,N \tag{7.12}
\end{align}$$
函数间隔$\hat{\gamma}$不影响最优化问题的解。函数间隔的改变对上面最优化问题的不等式约束没有影响，对目标函数的优化也没有影响
这样，就可以取$\hat{\gamma} = 1$，将$\hat{\gamma} = 1$代入上面的最优化问题。且：最大化$\frac{1}{\lVert w \rVert}$和最小化$\frac{1}{2} {\lVert w \rVert}^2$等价
于是得到下面的：线性可分支持向量机的最优化问题
$$\begin{align}
\min_{w,b} \quad & \frac{1}{2} {\lVert w \rVert}^2 \tag{7.13}\\
s.t.\quad & y_i (w \cdot x_i + b) -1 \ge 0,\ i = 1,2,\cdots,N \tag{7.14}
\end{align}$$
这是一个凸二次规划（convex quadratic programming）问题

凸优化问题是指约束最优化问题
$$\begin{align}
\min_w \quad & f(w) \tag{7.15} \\
s.t. \quad & g_i(w) \le 0,\, i=1,2,\cdots,k \tag{7.16} \\
& h_i(w) = 0,\, i=1,2,\cdots,l \tag{7.17}
\end{align}$$
其中，目标函数$f(w)$和约束函数$g_i(w)$都是$\mathbb{R}^n$上的连续可微的凸函数，约束函数$h_i(w)$是$\mathbb{R}^n$上的仿射函数
当目标函数$f(w)$是二次函数且约束函数$g_i(x)$是仿射函数时，上述凸最优化问题成为凸二次规划问题

#### **算法7.1（线性可分支持向量机学习算法——最大间隔法）**
**输入：** 线性可分训练数据集$T = \{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}$，其中$x_i \in \mathcal{X} = \mathbb{R}^n$，$y_i \in \mathcal{Y} = {-1, +1}$，$i = 1,2,\cdots,N$
**输出：** 最大间隔分离超平面和分类决策函数

1. 构造并求解约束最优化问题i
$$\begin{align}
\min_{w,b} \quad & \frac{1}{2} {\lVert w \rVert}^2 \tag{7.13}\\
s.t.\quad & y_i (w \cdot x_i + b) -1 \ge 0,\ i = 1,2,\cdots,N \tag{7.14}
\end{align}$$
求得最优解$w^*, b^*$
2. 由此得到分离超平面
$$
w^* \cdot x + b^* = 0
$$
分类决策函数
$$
f(x) = \sign(w^* \cdot x + b^*)
$$

#### 2. 最大间隔分离超平面的存在唯一性
#### **定理7.1（最大间隔分离超平面的存在唯一性）**

#### 3. 支持向量和间隔边界
在线性可分的情况下，训练数据集的样本点中与分离超平面距离最近的样本点的实例称为支持向量（support vector）。支持向量是使约束条件式(7.14)等号成立的点，即：
$$
y_i (w \cdot x + b) - 1 = 0
$$
对$y_i = +1$的正例点，支持向量在超平面
$$
H_1: w \cdot x + b = 1
$$
上，对$y_i = -1$的负例点，支持向量在超平面
$$
H_2: w \cdot x + b = -1
$$
上
$H_1$和$H_2$上的点就是支持向量
注意到$H_1$和$H_2$平行，且没有实例点落在其中间，形成的长带宽度（$H_1$和$H_2$之间的距离称为间距margin）。间距依赖于分离超平面的法向量$w$，等于$\frac{2}{\lVert w \rVert}$。$H_1$和$H_2$称为间距边界
支持向量机是由很少的“重要的”的训练样本确定（也就是支持向量的实例点）

### 7.1.4 学习的对偶算法
用拉格朗日乘子法来求解上面的凸优化问题。
这样做的优点：
1. 对偶问题往往更容易求解
2. 自然引入核函数，进而推广到非线性分类问题

首先构建拉格朗日函数（Lagrange function）为此，对每个不等式约束(7.14)，引进拉格朗日乘子（Lagrange mulitplier）$\alpha_i \ge 0，i=1,2,\cdots,N$
定义拉格朗日函数：
$$
L(w,b,\alpha) = \frac{1}{2} {\lVert w \rVert}^2 - \sum_{i=1}^N \alpha_i y_i(w \cdot x_i + b) + \sum_{i=1}^N \alpha_i
\tag{7.18}
$$
其中，$\alpha = (\alpha_1,\alpha_,\cdots,\alpha_N)^T$为拉格朗日乘子向量
根据拉格朗日对偶性，原始问题的对偶问题是极大极小问题：
$$
\max_\alpha \min_{w,b} L(w,b,\alpha)
$$
$\therefore$需要先求$L(w,b,\alpha)$对w,b的极小，再求对$\alpha$的极大

1. 求$\min_{w,b} L(w,b,\alpha)$
将拉格朗日函数$L(w,b,\alpha)$分别对w,b求偏导数并令其等于0
$$\begin{align}
\nabla_w L(w,b,\alpha) &= w- \sum_{i=1}^N \alpha_i y_i x_i =0 \\
\nabla_b L(w,b,\alpha) &= -\sum_{i=1}^N \alpha_i y_i =0
\end{align}$$
得：
$$\begin{align}
w = \sum_{i=1}^N \alpha_i y_i x_i \tag{7.19} \\
\sum_{i=1}^N \alpha_i y_i = 0 \tag{7.20}
\end{align}$$
将式(7.19)代入拉格朗日函数(7.18)，并利用(7.20)
得：
$$\begin{aligned}
L(w, b, \alpha) 
&=\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)-\sum_{i=1}^{N} \alpha_{i} y_{i}\left(\left(\sum_{j=1}^{N} \alpha_{j} y_{j} x_{j}\right) \cdot x_{i}+b\right)+\sum_{i=1}^{N} \alpha_{i} \\ 
&=-\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)+\sum_{i=1}^{N} \alpha_{i}
\end{aligned}$$
即：
$$
\min_{w,b} L(w,b,\alpha) = -\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)+\sum_{i=1}^{N} \alpha_{i}
$$

2. 求$\min_{w,b} L(w,b,\alpha)$对$\alpha$的极大，即对偶问题
$$\begin{align}
\max_{\alpha} \quad & -\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)+\sum_{i=1}^{N} \alpha_{i} \tag{7.21}\\
s.t.\quad &  \sum_{i=1}^N \alpha_i y_i = 0 \\
& \alpha_i \ge 0,\, i=1,2,\cdots,N
\end{align}$$
将式(7.21)的目标函数由极大转换成极小，就得到下面与之等价的对偶最优化问题：
$$\begin{align}
\min_{\alpha} \quad & \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right) - \sum_{i=1}^{N} \alpha_{i} \tag{7.22}\\
s.t.\quad &  \sum_{i=1}^N \alpha_i y_i = 0 \tag{7.23} \\
& \alpha_i \ge 0,\, i=1,2,\cdots,N \tag{7.24}
\end{align}$$

对线性可分训练集，假设对偶最优化问题(7.22)~(7.24)对$\alpha$的解$\alpha^* = (\alpha_1^*, \alpha_2^*,\cdots,\alpha_N^*)^T$，可以由$\alpha^*$求得原始最优化问题(7.13)~(7.14)对于(w,b)的解$(w^*,b^*)$。有下面的定理

#### **定理7.2**
设$\alpha^* = (\alpha_1^*, \alpha_2^*,\cdots,\alpha_N^*)^T$是对偶化问题(7.22)~(7.24)对$\alpha$的解，则存在下标j，使得$\alpha_j^* > 0$，并可按下式求得原始最优化问题(7.13)~(7.14)的解$w^*,b^*$：
$$\begin{align}
w^* = \sum_{i=1}^N \alpha_i^* y_i x_i \tag{7.25} \\
b^* = y_j - \sum_{i=1}^N \alpha_i^* y_i (x_i \cdot x_i) \tag{7.26}
\end{align}$$

#### **算法7.2（线性可分支持向量机学习算法）**
**输入：** 线性可分训练集$T = \{ (x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N) \}$，其中$x_i \in \mathcal{X} = \mathbb{R}^n$，$y_i \in \mathcal{Y} = {-1, +1}$，$i = 1,2,\cdots,N$
**输出：** 分离超平面和分类决策函数

1. 构造并求解约束最优化问题
$$\begin{align}
\min_{\alpha} \quad & \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right) - \sum_{i=1}^{N} \alpha_{i} \\
s.t.\quad &  \sum_{i=1}^N \alpha_i y_i = 0 \\
& \alpha_i \ge 0,\, i=1,2,\cdots,N 
\end{align}$$
求得最优解$\alpha^* = (\alpha_1^*, \alpha_2^*,\cdots,\alpha_N^*)^T$
2. 计算
$$
w^* = \sum_{i=1}^N \alpha_i^* y_i x_i
$$
并选择$\alpha^*$的一个正分量$\alpha_j^* \gt 0$，计算
$$
b^* = y_i - \sum_{i=1}^N \alpha_i^* y_i (x_i \cdot x_j)
$$
3. 求得分离超平面
$$
w^* \cdot x + b^* = 0
$$
分类决策函数
$$
f(x) = \sign (w^* \cdot x + b^*)
$$

在线性可分支持向量机中，由(7.25)(7.26)可知，$w^*$和$b^*$只依赖于训练数据集中对应于$\alpha_i^* > 0$的样本点$(x_i, y_i)$，而其他样本点对$w^*$和$b^*$没有影响
我们将训练数据集中对应于$\alpha_i^* > 0$的实例点$x_i \in \mathbb{R}^n$称为支持向量

#### **定义7.4（支持向量）**

## 7.2 线性支持向量机与软间隔最大化

### 7.2.1 线性支持向量机
将线性可分支持向量机扩展到线性不可分问题，需要修改硬间隔最大化，使其成为软间隔最大化
假设给定一个特征空间上的训练数据集
$$
T = \{ (x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N) \}
$$
其中，$x_i \in \mathcal{X} = \mathbb{R}^n$，$y_i \in \mathcal{Y} = {+1, -1}$，$i = 1,2,\cdots,N$，$x_i$为第i个特征向量，$y_i$为$x_i$的类标记。再假设训练数据集不是线性可分的。
通常情况下，训练数据中有一些特异点（outlier），将这些特异点除去后，剩下大部分的养不点组成的集合是线性可分的

线性不可分意味着某些样本点$(x_i,y_i)$，不能满足函数间隔大于等于1的约束条件(7.14)。为了解决这个问题，可以对每个样本点$(x_i,y_i)$引进一个松弛变量$\xi_i \ge 0$，使函数间隔加上松弛变量大于等于1。这样，约束条件变为：
$$
y_i (w \cdot x_i + b) \ge 1 - \xi_i
$$
同时，对每个松弛变量$\xi_i$，支付一个代价$\xi_i$。目标函数由原来的$\frac{1}{2} \lVert w \rVert ^2$变成
$$
\frac{1}{2} \lVert w \rVert ^2 + C \sum_{i=1}^N \xi_i
\tag{7.31}
$$
其中，C>0称为惩罚函数，一般由应用问题决定。C值越大，对误分类的惩罚增大
(7.31)包含两层含义，使$\frac{1}{2} \lVert w \rVert ^2$、即间隔尽量大；使误分类点的个数尽量小。C是调和二者的系数

线性不可分的线性支持向量机的学习问题变成如下凸二次规划（convex quadratic programming）问题（原始问题）：
$$\begin{align}
\min_{\alpha} \quad & \frac{1}{2} \lVert w \rVert ^2 + C \sum_{i=1}^N \xi_i \tag{7.32} \\
s.t.\quad & y_i (w \cdot x_i + b) \ge 1-\xi_i,\, i=1,2,\cdots,N \tag{7.33} \\
& \xi_i \ge 0,\, i=1,2,\cdots,N  \tag{7.34}
\end{align}$$
关于$(w,b,\xi)$的解是存在的。可以证明$w$的解是唯一的，但$b$的解不唯一，$b$的解存在于一个区间

#### **定义7.5（线性支持向量机）** 
对于给定的线性不可分的训练数据集，通过求解凸二次规划问题，即软间隔最大化问题(7.32)~(7.34)，得到的分离超平面为：
$$
w^* \cdot x + b^* = 0
\tag{7.35}
$$
以及相应的分类决策函数
$$
f(x) = \sign(w^* \cdot x + b^*)
\tag{7.36}
$$
称为线性支持向量机

### 7.2.2 学习的对偶算法
原始问题(7.32)~(7.34)的对偶问题是
$$\begin{align}
\min_{\alpha} \quad & \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right) - \sum_{i=1}^{N} \alpha_{i} \tag{7.37} \\
s.t.\quad &  \sum_{i=1}^N \alpha_i y_i = 0 \tag{7.38} \\
& 0 \le \alpha_i \le C,\, i=1,2,\cdots,N \tag{7.39}
\end{align}$$

原始最优化问题的拉格朗日函数是：
$$\begin{align}
&L(w,b,\xi,\alpha,\mu) \\ 
=\ & \frac{1}{2} \lVert w \rVert ^2 + C \sum_{i=1}^N \xi_i - \sum_{i=1}^N \alpha_i (y_i (w \cdot x_i +b) -1 + \xi_i) - \sum_{i=1}^N \mu_i \xi_i
\end{align}
\tag{7.40}
$$
其中$\alpha_i \ge 0, \mu_i \ge 0$
对偶问题是拉格朗日函数的极大极小问题
首先求$L(w,b,\xi,\alpha,\mu)$对$w,b,\xi$的极小：
$$\begin{align}
\nabla_w L(w,b,\xi,\alpha,\mu) = w - \sum_{i=1}^N \alpha_i y_i x_i = 0
\nabla_b L(w,b,\xi,\alpha,\mu) = - \sum_{i=1}^N \alpha_i y_i = 0
\nabla_{\xi} L(w,b,\xi,\alpha,\mu) = C - \alpha_i - \mu_i = 0
\end{align}$$
得：
$$\begin{align}
w = \sum_{i=1}^N \alpha_i y_i x_i \tag{7.41} \\
\sum_{i=1}^N \alpha_i y_i = 0 \tag{7.42} \\
C - \alpha_i - \mu_i = 0 \tag{7.43}
\end{align}$$
将(7.41)~(7.43)代入(7.40)，得：
$$
\min_{w,b,\xi} L(w,b,\xi,\alpha,\mu) = - \frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j (x_i \cdot x_j) + \sum_{i=1}^N \alpha_i
$$
再对$\min_{w,b,\xi} L(w,b,\xi,\alpha,\mu)$求$\alpha$的极大，即得对偶问题：
$$\begin{align}
\max_\alpha \quad & - \frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j (x_i \cdot x_j) + \sum_{i=1}^N \alpha_i \tag{7.44} \\
s.t. \quad & \sum_{i=1}^N \alpha_i y_i = 0 \tag{7.45} \\
& C - \alpha_i - \mu_i = 0 \tag{7.46} \\
& \alpha_i \ge 0 \tag{7.47} \\
& \mu_i \ge 0 ,\, i=1,2,\cdots,N \tag{7.48}
\end{align}$$
将对偶最优化问题(7.44)~(7.48)进行变化，利用等式约束(7.46)消去$\mu_i$，从而只留下变量$\alpha_i$，并将约束(7.46)~(7.48)写成：
$$
0 \le \alpha_i \le C \tag{7.49}
$$
再对目标函数求极大转为求极小，即得对偶问题(7.37)~(7.39)

#### **定理7.3**
设$\alpha^* = (\alpha_1^*, \alpha_2^*,\cdots,\alpha_N^*)^T$是对偶问题(7.37)~(7.39)的一个解，若存在$\alpha^*$的一个分量$\alpha_j^*, 0 \lt \alpha_j^* \lt C$，则原始问题(7.32)~(7.34)的解$w^*,b^*$可按下式求得：
$$\begin{align}
w^* = \sum_{i=1}^N \alpha_i^* y_i x_i \tag{7.50} \\
b^* = y_j - \sum_{i=1}^{N} y_i \alpha_i^* (x_i \cdot x_j) \tag{7.51}
\end{align}$$

由定理可知，分离超平面可以写成：
$$
\sum_{i=1}^N \alpha_i^* y_i (x\cdot x_i) + b^* = 0 \tag{7.55}
$$
分类决策函数可以写成
$$
f(x) = \sign \left( \sum_{i=1}^N \alpha_i^* y_i (x\cdot x_i) + b^* \right)
\tag{7.56}
$$
式(7.56)为线性支持向量机的对偶形式

#### **算法7.3（线性支持向量机的学习算法）**
**输入：** 线性可分训练集$T = \{ (x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N) \}$，其中$x_i \in \mathcal{X} = \mathbb{R}^n$，$y_i \in \mathcal{Y} = {-1, +1}$，$i = 1,2,\cdots,N$
**输出：** 分离超平面和分类决策函数

1. 选择惩罚参数C>0，构造并求解凸二次规划问题：
$$\begin{align}
\min_{\alpha} \quad & \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right) - \sum_{i=1}^{N} \alpha_{i}  \\
s.t.\quad &  \sum_{i=1}^N \alpha_i y_i = 0 \\
& 0 \le \alpha_i \le C,\, i=1,2,\cdots,N 
\end{align}$$
求得最优解$\alpha^* = (\alpha_1^*, \alpha_2^*,\cdots,\alpha_N^*)^T$
2. 计算$w^* = \sum_{i=1}^N \alpha_i^* y_i x_i$
选择$\alpha^*$的一个分量$\alpha_j^*$适合条件$0 \lt \alpha_j^* \lt C$，计算
$$
b^* = y_j - \sum_{i=1}^N y_i \alpha_i^*(x_i \cdot x_j)
$$
3. 求得分离超平面
$$
w^* \cdot x + b^* = 0
$$
分类决策函数：
$$
f(x) = \sign(w^* \cdot x + b^*)
$$
步骤2中，对于任一合适条件的$\alpha_j^*$，都可以按(7.51)求出$b^*$，但b的取值并不唯一。实际计算中，可以取所有符合条件的样本点上的平均值

### 7.2.3 支持向量
在线下不可分的情况下，对偶问题(7.37)~(7.39)的解$\alpha^* = (\alpha_1^*, \alpha_2^*,\cdots,\alpha_N^*)^T$中对应于$\alpha_i^* >0$的样本点$(x_i,y_i)$的实例$x_i$称为支持向量（软间隔的支持向量）
软间隔的支持向量$x_i$或者在间隔边界上；或者在间隔边界和分离超平面之间；或者在分离超平面误分一侧。
- 若$\alpha_i^* < C$，则$\xi_i = 0$，支持向量$x_i$恰好落在间隔边界上；
- 若$\alpha_i^* = C, 0 \lt \xi_i \lt 1$，则分类准确，$x_i$在间隔边界与分离超平面之间；
- 若$\alpha_i^* = C, \xi_i = 1$，则$x_i$在分离超平面上
- 若$\alpha_i^* = C, \xi_i > 1$，则$x_i$位于分离超平面误分一侧

### 7.2.4 合页损失函数
线性支持向量机学习还有另外一种解释，就是最小化以下目标函数：
$$
\sum_{i=1}^N \left[ 1 - y_i(w \cdot x_i + b) \right]_+ + \lambda \lVert w \rVert ^2
\tag{7.57}
$$
目标函数的第一项是经验损失或经验风险
函数：
$$
L(y(w \cdot x + b)) = \left[ 1-y(w \cdot x +b) \right]_+
\tag{7.58}
$$
称为合页损失函数（hinge loss function，也翻译为铰链损失），下标“+”表示以下取正值的函数
$$
[z]_+ =
\begin{cases}
z, &z>0\\
0, &z \le 0
\end{cases}
\tag{7.59}
$$
也就是说：当样本点被准确分类且函数间隔（=确信度）$y_i (w \cdot x_i + b)$大于1时，损失为0，否则损失为$1-y_i (w \cdot x_i + b)$
第二项是正则化项，系数为$\lambda$的$w$的$L_2$范数

#### **定理7.4**
线性支持向量机原始最优化问题：
$$\begin{align}
\min_{} \quad & \frac{1}{2} \lVert w \rVert ^2 + C \sum_{i=1}^N \xi_i \tag{7.60}  \\
s.t.\quad & y_i(w \cdot x_i +b) \ge 1 - \xi_i,\, 1=1,2,\cdots,N \tag{7.61} \\
& \xi_i \ge 0,\, i=1,2,\cdots,N \tag{7.62}
\end{align}$$
等价于最优化问题
$$
\min_{w,b} \sum_{i=1}^N \left[ 1 - y_i(w \cdot x_i + b) \right]_+ + \lambda \lVert w \rVert ^2 \tag{7.63}
$$

【注】证明过程没有看懂

相比感知机的损失函数$[y_i (w \cdot x_i +b)]_+$，合页损失函数不但要求分类准确，且确信度需足够高；即：合页损失函数对学习有更高的要求

## 7.3 非线性支持向量机与核函数

### 7.3.1 核技巧

#### 1. 非线性分类问题
对给定的一个训练数据集$T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}$，其中实例$x_i \in mathcal{X} = \mathbb{R}^n$，对应的标记有两类$y_i \in \mathcal{Y} = \{-1,+1\}，i=1,2,\cdots,N$。如果$\mathcal{R}^n$中的一个**超曲面**将正负例分开，则称为非线性可分问题
所采取的方法是进行一个非线性变换，将非线性问题变换为线性问题。书中示例：
设原空间为$\mathcal{X} \subset \mathbb{R}^2$，$x = (x^{(1)},x^{(2)})^T \in \mathcal{X}$，新空间为$mathcal{Z} \subset \mathbb{R}^2$，$z = (z^{(1)},z^{(2)})^T \in \mathcal{Z}$，定义从元空间到新空间的变换（映射）：
$$
z = \phi(x) = ((x^{(1)},x^{(2)})^2)^T
$$
经过变换$z = \phi(x)$，原空间$\mathcal{X} \subset \mathbb{R}^2$变换为新空间$\mathcal{Z} \subset \mathbb{R}^2$，原空间中的点相应的变为新空间中的点，原空间中的椭圆
$$
w_1 (x^{(1)})^2 + w_2 (x^{(2)})^2 + b = 0
$$
变换为新空间中的直线
$$
w_1 z^{(1)} + w_2 z^{(2)} + b = 0
$$
把原空间的非线性可分问题，变成了新空间的线性可分问题

#### 2. 核函数的定义
#### **定义7.6（核函数）**
设$mathcal{X}$是输入空间（欧式空间$\mathbb{R}^n$的子集或离散集合），又设$\mathcal{H}$为特征空间（希尔伯特空间），如果存在一个从$\mathcal{X}$到$\mathcal{H}$的映射
$$
\phi(x)：\mathcal{X} \to \mathcal{H}
\tag{7.65}
$$
使得对所有$x,z \in \mathcal{X}$，函数$K(x,z)$满足条件
$$
K(x,z) = \phi(x) \cdot \phi(z)
\tag{7.66}
$$
则称$K(x,z)$为核函数，$\phi(x)$为映射函数，式中$\phi(x) \cdot \phi(z)$是内积
核技巧的想法是：在学习与预测中只定义核函数K(x,z)，而不显式地定义映射函数$\phi$

#### 3. 核技巧在支持向量机中的应用
在线性支持向量机的对偶问题中，无论目标函数还是决策函数，都只设计输入实例和实例之间的内积
在对偶的目标函数(7.37)中，内积$x_i \cdot x_j$可以用核函数$K(x_i, x_j) \phi(x_i) \cdot \phi(x_j)$来代替
此时，对偶问题id目标函数变为；
$$
W(\alpha) = \frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j K(x_i,x_j) - \sum_{i=1}^N \alpha_i
\tag{7.67}
$$
同样，分类决策函数中的内积也可以用核函数代替，而分类决策函数式成为
$$
\begin{align}
f(x) &= \sign \left( \sum_{i=1}^{N_S} \alpha_i^* y_i \phi(x_i) \cdot \phi(x) + b^* \right) \\
&= \sign \left( \sum_{i=1}^{N_S} \alpha_i^* y_i K(x_i,x) + b^* \right)
\end{align}
\tag{7.68}
$$
等价于经过预设函数$\phi$，将原来的输入空间变换到一个新的特征空间，将输入空间的内积$x_i \cdot x_j$变换为特征空间中的内积$\phi(x_i) \cdot \phi(x_j)$。在新的特征空间里从训练样本中学习线性支持向量机
在实际应用中，往往依赖领域知识直接选择核函数，核函数选择的有效性需要通过实验验证

### 7.2.3 正定核

在不用构造映射$\phi(x)$的情况下，如何判定一个给定的函数$K(x,z)$是不是核函数

假设$K(x,z)$是定义在$\mathcal{X} times \mathcal{X}$的对称函数，并对任意的$x_1, x_2, \cdots, x_m \in \mathcal{X}$，$X(x,z)$关于$x_1,x_2,\cdots,x_m$的Gram矩阵是半正定的。
可以依据函数$K(x,z)$，构成一个希尔伯特空间，其步骤是：首先定义映射$\phi$并构成向量空间$\mathcal{S}$；然后在$\mathcal{S}$上定义内积构成内积空间；最后将$\mathcal{S}$完备化构成希尔伯特空间

【注】以下的三步，其实就是赋范空间$\to$欧式空间$\to$希尔伯特空间一步步的定义加强？

#### 1. 定义映射，构成向量空间$\mathcal{S}$
先定义映射
$$
\phi: x \to K(\cdot, x)
\tag{7.69}
$$
根据这一映射，对任意$x_i \in \mathcal{X}, \alpha_i \in \mathbb{R}, i=1,2,\cdots,m$，定义线性组合
$$
f(\cdot) = \sum_{i=1}^N \alpha_i K(\cdot,x_i)
\tag{7.70}
$$
考虑由线性组合元素的集合$\mathcal{S}$。
由于$\mathcal{S}$对假发和数乘运算是封闭的，所以$\mathcal{S}$构成一个向量空间

#### 2. 在$\mathcal{S}$上定义内积，使其成为一个内积空间
在$\mathcal{S}$上定义一个运算*：
对任意$f,g \in \mathcal{S}$
$$
f(\cdot) = \sum_{i=1}^m \alpha_i K(\cdot, x_i)
\tag{7.71}
$$
$$
g(\cdot) = \sum_{j=1}^l \beta_j K(\cdot, x_j)
\tag{7.72}
$$
定义运算*：
$$
f*g = \sum_{i=1}^m \sum_{j=1}^l \alpha_i \beta_j K(x_i,z_j)
\tag{7.73}
$$

要证明运算*是空间的内积，要证：
$$\begin{align}
& (1) \ (cf)*g = c(f*g), c \in \mathbb{R} \tag{7.74} \\
& (2) \ (f+g)*h = f*h +g*H, h \in \mathcal{S} \tag{7.75} \\
& (3) \ f*g = g*f \tag{7.76} \\
& (4) \ f*f \ge 0 \tag{7.77} \\
&f*f = 0 \iff f=0 \tag{7.78}
\end{align}$$

证略
待补课
Gram矩阵


#### 3. 将内积空间$\mathcal{S}$完备化为希尔伯特空间
由内积的定义可以得到范数：
$$
\lVert f \rVert = \sqrt{f \cdot f}
\tag{7.82}
$$
因此，$\mathcal{S}$是一个赋范空间
*（根据泛函分析理论）*，对于不完备的赋范向量空间$\mathcal{S}$，一定可以使之完备化，得到完备的赋范向量空间$mathcal{H}$
一个内积空间，当作为赋范向量空间完备时，就是希尔伯特空间

这一希尔伯特空间$\mathcal{H}$称为再生核希尔伯特空间（reproducing kernel Hilbert space, RKHS）。这是由于核K具有再生性，即满足：
$$
K(\cdot , x) \cdots f = f(x)
\tag{7.83}
$$
及
$$
K(\cdot, x) \cdot K(\cdot, z) = K(x,z)
\tag{7.84}
$$

#### 4. 正定核的充要条件
#### **定理7.5（正定核的充要条件）**
设$K: \mathcal{X} \times \mathcal{X} \to \mathbb{R}$是对称函数，则$K(x,z)$为正定核函数的充要条件是对任意$x_i \in \mathcal{X}, i=1,2,\cdots,m$，$K(x,z)$对应的Gram矩阵
$$
K = \left[ K(x_i,x_j) \right]_{m \times m}
\tag{7.85}
$$
是半正定矩阵

#### **定义7.7（正定核的等价定义）**
设$\mathcal{X} \subset \mathbb{R}^n$，$K(x,z)$是定义在$\mathcal{X} \times \mathcal{X}$上对的对称函数，如果对任意$x_i \in \mathcal{X},i=1,2,\cdot,m$，$K(x,z)$对应的Gram矩阵
$$
K = \left[ K(x_i,x_j) \right]_{m \times m}
\tag{7.87}
$$
是半正定矩阵，则称$K(x,z)$是正定核

这一定义在构造核函数时很有用
但对具体的函数K，检验其是否为正定核并不容易，实际问题中往往应用已有的核函数

### 7.3.3 常用核函数
#### 1. 多项式核函数（polynomial kernel function）
$$
K(x,z) = (x \cdot z + 1)^p
\tag{7.88}
$$
对应的支持向量机是一个p次多项式分类器。在此情形下，分类决策函数成为
$$
f(x) = \sign \left( \sum_{i=1}^{N_S} \alpha_i^* y_i (x_i \cdot x +1)^p + b^* \right)
\tag{7.89}
$$

#### 2. 高斯核函数（Gaussian kernel function）
$$
K(x, z)=\exp \left(-\frac{\|x-z\|^{2}}{2 \sigma^{2}}\right)
$$
对应的支持向量机是高斯径向基函数（radial basis function）分类器。在此情形下，分类决策函数成为：
$$
f(x)=\sign \left( \sum_{i=1}^{N_S} a_{i}^{*} y_{i} \exp \left(-\frac{\|x-z\|^{2}}{2 \sigma^{2}}\right)+b^{*}\right)
$$

#### 3. 字符串核函数（string kernel function）
核函数也可定义在离散数据集合上。字符串核实定义在字符串集合上的核函数
考虑一个有限字符表$\Sigma$。字符串s是从$\Sigma$中取出的有限个字符的序列，包含空字符串

- 字符串的长度用|s|表示
- 它的元素记为$s(1)s(2)\cdots s(|s|)$
- 两个字符串s和t的连接2记作st
- 所有长度为n的字符串的集合记作$\Sigma^n$
- 所有字符串的集合记作$\Sigma^* = \bigcup_{n=0}^\infty \Sigma^n$

考虑字符串s的子串u。给定一个指标序列$i=(i_1,i_2,\cdots,i_{|s|}), i \le i_1 \lt i_2 \lt \cdots \lt i_{|u|} \le |s|$，s的子串定义为$u= s(i) = s(i_1)s(i_2)\cdots s(i_{|u|})$，其长度记作$l(i) = i_{|u|}-i_1+1$
如果i是连续的，则$l(i)=|u|$，否则，$l(i)>|u|$

假设$\mathcal{S}$是长度大于或等于n字符串的集合，s是S的元素
现在简历字符串集合S到特征空间$\mathcal{H}_n = R^{\Sigma^n}$的映射$\phi_n(s)$
$R^{\Sigma^n}$表示定义在$\Sigma^n$上的诗书空间，其每一维对应一个字符串$u \in \Sigma^m$
映射$\phi_n(s)$将字符串s对应于空间$R^{\Sigma^n}$的一个向量，其在u维上的取值为：
$$
[\phi_n(s)]_u = \sum_{i:s(i)=u} \lambda^{l(i)}
$$
这里$0<\lambda \le  1$是一个衰减函数，l(i)表示字符串i的长度，求和在s中所有与u相同的子串上进行
两个字符串s和t的字符串核函数是基于映射$\phi_n$的特征空间中的内积
$$\begin{align}
k_n(s,t) &=\sum_{u \in \Sigma^m}[\phi_n(s)]_u [\phi_n(t)]_u \\
&= \sum_{u\in \Sigma^m} \sum_{(i,j):s(i)=t(j)=u} \lambda^{l(i)} \lambda^{l(j)}
\end{align}
\tag{7.93}
$$
字符串核函数$k_n(s,t)$给出了字符串s和t中长度等于n的所有子串组成的特征向量的余弦相似度（cosine similarity）
字符串核函数可以由动态规划快速地计算


### 7.3.4 非线性支持向量分类机
#### **定义7.8（非线性支持向量机）**
从非线性分类训练集，通过核函数与软间隔最大化，或凸二次规划(7.95)~(7.97)，学习到的分类决策函数
$$
f(x) = \sign \left( \sum_{i=1}^N \alpha_i^* y_i K(x,x_i) + b^* \right)
\tag{7.94}
$$
称为非线性支持向量，$K(x,z)$是正定核函数

#### **算法7.4（非线性支持向量机学习算法）**
**输入：**训练数据集$T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}$，其中$x_i \in mathcal{X} = \mathbb{R}^n, y_i \in \mathcal{Y}=\{-1,+1\}, i=1,2,\cdots,N$
**输出：**分类决策函数

1. 选取适当的核函数$K(x,z)$和适当的参数C，构造并求解最优化问题
$$\begin{align}
\min_{\alpha} \quad & \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j} K(x_i,x_j) - \sum_{i=1}^{N} \alpha_{i}  \tag{7.95} \\
s.t.\quad &  \sum_{i=1}^N \alpha_i y_i = 0 \tag{7.96} \\
& 0 \le \alpha_i \le C,\, i=1,2,\cdots,N \tag{7.97}
\end{align}$$
求得最优解$\alpha^* = (\alpha_1^*,\alpha_2^*,\cdots,\alpha_N^*)^T$
2. 选择$\alpha^*$的一个正分量$0<\alpha_j^*<C$，计算
$$
b^* = y_j - \sum_{i=1}^N \alpha_i^* y_i K(x_i,x_j)
$$
3. 构造决策函数：
$$
f(x) = \sign \left( \sum_{i=1}^N \alpha_i^* y_i K(x,x_i) + b^* \right)
$$

当$K(x,z)$是正定核函数时，问题是凸二次规划问题，解存在

## 7.4 序列最小最优化算法
在样本容量大时，高效实现支持向量机学习的算法那：序列最小最优化（sequential minimal optimization，SMO）算法

SMO算法要解如下图二次规划的对偶问题：
$$\begin{align}
\min_{\alpha} \quad & \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j} K(x_i,x_j) - \sum_{i=1}^{N} \alpha_{i}  \tag{7.98} \\
s.t.\quad &  \sum_{i=1}^N \alpha_i y_i = 0 \tag{7.99} \\
& 0 \le \alpha_i \le C,\, i=1,2,\cdots,N \tag{7.100}
\end{align}$$
这个问题中，变量是拉格朗日乘子，一个变量$\alpha_i$对应一个样本点(x_i,y_i)；变量总数等于训练样本容量N

SMO算法是启发式算法，基本思路为：
如果所有变量的解都满足此最优化问题KKT条件，那么最优化问题的解就得到了
否则，选择两个变量，固定其他变量，针对这两个变量构建一个二次规划问题
这个二次规划问题关于这个两个变量的解应该更接近原始二次规划问题的解（因为这会使得原始二次规划问题的目标函数值变得更小）
更重要的是，子问题可以通过解析法求解
子问题有两个变量：一个是违反KKT条件最严重的一个；另一个由约束条件自动确定
如此，SMO算法将原问题不断分解为子问题并对子问题求解，进而达到求解原问题的目的
注意，子问题的两个变量只有一个是自由变量。假设$\alpha_1,\alpha_2$为两个变量，$\alpha_3,\alpha_4,\cdots,\alpha_N$固定，那么由(7.99)可知：
$$
\alpha_1 = - y_1 \sum_{i=2}^N \alpha_i y_i
$$
如果$\alpha_2$确定，则$\alpha_1$也就确定。故子问题会同时更新两个变量

整个SMO算法包括两个部分：求解两个变量二次规划的解析方法和选择变量的启发式方法

### 7.4.1 两个变量二次规划的求解方法
【注】$y_i \in \{ -1,+1 \}$，所以才说$\alpha_i$一定是平行于对角线

假设$\alpha_1,\alpha_2$为两个变量，$\alpha_3,\alpha_4,\cdots,\alpha_N$固定，于是SMO的最优化问题(7.98)~(7.100)的子问题可以写成：
$$\begin{align}
\min_{\alpha_1,\alpha_2} \quad & W(\alpha_1, \alpha_2) = \frac{1}{2}K_{11} \alpha_1 ^ 2 + \frac{1}{2}K_{22} \alpha_2 ^ 2 + y_i y_2 K_{12} \alpha_1 \alpha_2 - (\alpha_1 + \alpha_2) + y_i \alpha_1 \sum_{i=3}^{N} y_i \alpha_i K_{i1} + y_2 \alpha_2 \sum_{i=3}^N y_i \alpha_i K_{i2} \tag{7.101} \\
s.t. & \alpha_1 y_1 + \alpha_2 y_2 = - \sum_{i=3}^N y_i \alpha_i = \zeta \tag{7.102} \\
& 0 \le \alpha_i \le C ,\, i=1,2 \tag{7.103}
\end{align}$$
其中$K_{ij} = K(x_i,x_j),\,i,j=1,2,\cdots,N$，$\zeta$是常数。目标函数式(7.101)中省略了不含$\alpha_1,\alpha_2$的常数项

假设(7.101)~(7.103)的初始可行解为$\alpha_1^{\text{old}}$,$\alpha_2^{\text{old}}$，最优解为$\alpha_1^{\text{new}}$,$\alpha_2^{\text{new}}$。并且假设在沿着约束方向，未经剪辑时$\alpha_2$的最优解是$\alpha_2^{\text{new,unc}}$
未经剪辑，指的是未考虑不等式约束(7.103)

![](leanote://file/getImage?fileId=5c81e53a7a3a154f27000002)

由于$\alpha_2^{\text{new}}$需要满足(7.103)，所以其取值范围必须满足条件：
$$
L \le \alpha_2^{\text{new}} \le H
$$
其中L与H是$\alpha_2^{\text{new}}$所在对角线端点的界。
如果$y_1 \ne y_2$，则
$$\begin{align}
L = \max (0, \alpha_2^{\text{old}} - \alpha_1^{\text{old}}) \\
H = \min (C , C + \alpha_2^{\text{old}} - \alpha_1^{\text{old}})
\end{align}$$
如果$y_1 = y_2$，则
$$\begin{align}
L = \max (0, \alpha_2^{\text{old}} + \alpha_1^{\text{old}} - C) \\
H = \min (C , \alpha_2^{\text{old}} + \alpha_1^{\text{old}})
\end{align}$$
【注】这里指的是正方形的边线，必须结合图形考虑

下面，首先考虑未经剪辑时，$\alpha_2$的最优解$\alpha_2^{\text{new,unc}}$；然后再求解剪辑后的解$\alpha_2^{\text{new}}$
记
$$
g(x) = \sum_{i=1}^N \alpha_i y_i K(x_i,x)+b \tag{7.104}
$$
令
$$\begin{align}
E_i &= g(x_i) - y_i \\
&= \left( \sum_{j=1}^N \alpha_j y_j K(x_j,x_i) +b \right) - y_i ,\, i=1,2 \tag{7.105}
\end{align}$$
当i=1,2时，$E_j$为函数$g(x)$对输入$x_i$的预测值与真实输出y_i之差

#### **定理7.6**
最优化问题(7.101)~(7.103)沿着约束方向未经剪辑时的解释
$$
\alpha_2^{\text{new,unc}} =\alpha_2^{\text{old}} + \frac{y_2(E_1-E_2)}{\eta} \tag{7.106}
$$
其中，
$$
\eta = K_{11} + K_{22} -2 K_{12} = \| \Phi(x_1) - \Phi(x_2) \| ^2 \tag{7.107}
$$
$\Phi(x)$是输入空间到特征空间的映射，$E_i, i=1,2$由式(7.105)给出
经剪辑后$\alpha_2$的解是：
$$
\alpha_2^{\text{new}} = 
\begin{cases}
H, & \alpha_2^{\text{new,unc}} > H \\
\alpha_2^{\text{new,unc}}, & L \le \alpha_2^{\text{new,unc}} \le H \\
L, & \alpha_2^{\text{new,unc}} < L
\end{cases}
\tag{7.108}
$$
由$\alpha_2^{\text{new}}$求得$\alpha_1^{\text{new}}$是
$$
\alpha_1^{\text{new}} = \alpha_1^{\text{old}} + y_1 y_2 (\alpha_2^{\text{old}} - \alpha_2^{\text{new}})
\tag{7.109}
$$


### 7.4.2 变量的选择方法

#### 1. 第1个变量的选择
SMO称选择第1个变量的过程为**外层循环**
在训练样本中选择违反KKT条件最严重的样本点，并将其对应的变量作为第1个变量
具体地，训练样本点$(x_i,y_i)$是否满足KKT条件，即
$$\begin{align}
\alpha_i = 0 \iff y_i g(x_i) \ge 1 \tag{7.111} \\
0 < \alpha_i < C \iff y_i g(x_i) = 1 \tag{7.112} \\
\alpha_i = C \iff y_i g(x_i) \le 1 \tag{7.113}
\end{align}$$
其中，$g(x_i) = \sum_{j=1}^N \alpha_i y_i K(x_i,x_j) +b$
该检验是在$\epsilon$范围内进行的
在检验过程中，外层循环首先遍历所有满足条件$0 < \alpha_i < C$，即在间隔边界上的支持向量点，检验他们是否满足KKT条件
如果都满足，则遍历整个训练集，检验他们是否都满足KKT条件

#### 2. 第2个变量的选择
SMO称第二个变量的过程为**内层循环**
第2个变量的标准是希望$\alpha_2$有足够大的变化
由(7.106)和(7.108)可知，$\alpha_2^{\text{new}}$依赖于$|E_1-E_2|$，为加快计算速度，一种简单的做法是选择$\alpha_2$使其$|E_1-E_2|$最大。为节省计算时间，可以提前计算所有$E_i$值
特殊情况下，如果内层循环通过以上方法选择的$\alpha_2$不能使目标函数由足够的下降，那么采用以下启发式规则继续选择$\alpha_2$：遍历在间隔边界上的支持向量点，一次将其对应的变量作为$\alpha_2$试用，直到目标函数由足够的下降
若找不到合适的$\alpha_2$，那么遍历训练数据集；若仍找不到，则放弃第1个$\alpha_1$，再通过外层循环找另一个$alpha_1$

#### 3. 计算阈值$b$和差值$E_i$
每次完成两个变量的优化后，都要重新计算阈值b
当$0 < \alpha_1^{\text{new}} < C$时，由KKT条件(7.112)可知：
$$
\sum_{i=1}^N \alpha_i y_i K_{i1} +b = y_1
$$
于是
$$
b_1^{\text{new}} = y_1 - \sum_{i=3}^N \alpha_i y_i K_{i1} - \alpha_1^{\text{new}} y_1 K_{11} -\alpha_2^{\text{new}} y_2 K^{21}
\tag{7.114}
$$
由$E_1$的定义式(7.105)有
$$
E_1 = \sum_{i=3}^N \alpha_i y_i K_{i1} + \alpha_1^{\text{old}} y_1 K_{11} + \alpha_2^{\text{old}} y_2 K_{21} -y_1
$$
代入(7.114)可得
$$
b_1^{\text{new}} = - E_1 - y_1 K_{11}(\alpha_1^{\text{new}}-\alpha_1^{\text{old}}) - y_2 K_{21} (\alpha_2^{\text{new}}-\alpha_2^{\text{old}}) + b^{\text{old}}
\tag{7.115}
$$
同样，如果$0 < \alpha_2^{text{new}} < C$，那么：
$$
b_2^{\text{new}} = - E_2 - y_1 K_{12}(\alpha_1^{\text{new}}-\alpha_1^{\text{old}}) - y_2 K_{22} (\alpha_2^{\text{new}}-\alpha_2^{\text{old}}) + b^{\text{old}}
\tag{7.116}
$$
如果$\alpha_1^{\text{new}},\alpha_2^{\text{new}}$同时满足$0<\alpha_i^{\text{new}}<C,\,i=1,2$，那么$b_1^{\text{new}} = b_2^{\text{new}}$
如果$\alpha_i^{\text{new}}$是0或C，那么$b_1^{\text{new}}, b_2^{\text{new}}$和他们中间的数都是符合KKT条件的，选择他们的重点作为$b^{\text{new}}$

每次完成两个变量的优化后，HIA必须更新对应的$E_i$值，并将其保存起来
$$
E_i^{\text{new}} = \sum_S y_j \alpha_j K(x_i,x_j) + b^{\text{new}} -y_i
\tag{7.117}
$$
其中，S是所有支持向量$x_j$的集合
【注】这里的支持向量怎么求？$\alpha$在特定条件下的向量？

### 7.4.3 SMO算法
#### **算法7.5（SMO算法）**

**输入：** 训练数据集$T = \{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}$，其中$x_i \in \mathcal{X} = \mathbb{R}^n$，$y_i \in \mathcal{Y} = {-1, +1}$，$i = 1,2,\cdots,N$，精度$\epsilon$
**输出：** 近似解$\hat{\alpha}$

1. 取初值$\alpha^{(0)}=0$，令$k=0$
2. 选取优化变量$\alpha_1^{(k)},\alpha_2^{(k)}$，解析求解两个变量的最优化问题(7.101)~(7.103)，求得最优解$\alpha_1^{(k+1)},\alpha_2^{(k+1)}$，更新$\alpha$为$\alpha^{(k+1)}$
3. 若在精度$\epsilon$范围内满足停机条件
$$\begin{align}
& \sum_{i=1}^N \alpha_i y_i =0 \\
& 0 \le \alpha_i \le C ,\, i=1,2,\cdots,N \\
& y_i \cdot g(x_i) = 
\begin{cases}
\ge 1, &\{x_i| \alpha_i =0 \} \\
= 1, &\{x_i| 0 < \alpha_i <C \} \\
\le 1, &\{x_i| \alpha_i = C \}
\end{cases}
\end{align}$$
其中
$$
g(x_i) = \sum_{j=1}^N \alpha_j y_j K(x_j,x_i) + b
$$
则转4；否则令k=k+1，转2
4. 取$\hat{\alpha} = \alpha^{(k+1)}$

【注】$\{x_i| \alpha_i =0 \}$指的应该是$\alpha_i =0$时，对应的$x_i$的集合



