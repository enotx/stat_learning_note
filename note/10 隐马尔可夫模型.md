# 第10章 隐马尔可夫模型

## 10.1 隐马尔可夫模型的基本概念
### 10.1.1 隐马尔可夫模型的定义

#### 定义10.1（隐马尔可夫模型）
隐马尔可夫模型是关于时序的概率模型，描述由一个隐藏的马尔可夫链随机生成不可观测的状态随机序列，再由各个状态生成一个观测二产生观测随机序列的过程
隐马尔可夫链随机生成的状态的序列，称为状态序列（state sequence）；
每个状态生成一个观测，而由此产生的观测的随机序列，称为观测序列（observation sequence）
序列的每一个位置又可以看作是一个时刻

隐马尔可夫模型由初始概率分布、状态转移概率分布以及观测概率分布确定。隐马尔可夫模型的形式定义如下：
设Q是所有可能的状态的集合，V是所有可能的观测的集合
$$
Q=\{q_1,q_2,\cdots,q_N \},\, V=\{v_1,v_2,\cdots,v_M\}
$$
其中，N是可能的状态数，M是可能的观测数
I是长度的T的状态序列，O是对应的观测序列
$$
I = (i_1,i_2,\cdots,i_T),\, O=(o_1,o_2,\cdots,o_T)
$$
A是状态转移概率矩阵：
$$
A = \left[ \alpha_{ij} \right]_{N \times N}
\tag{10.1}
$$
其中，
$$
\alpha_{ij} = P(i_{t+1} = q_j | i_t = q_i),\,i=1,2,\cdots,N;j=1,2,\cdots,N
\tag{10.2}
$$
是在时刻$t$处在状态$q_i$的条件下的时刻$t+1$转移到状态$q_j$的概率
B是观测概率矩阵：
【注】原书如此，但使得状态转移概率分布a的下标有点混乱

$$
B = \left[ b_j(k) \right]_{N \times M}
\tag{10.3}
$$
其中，
$$
b_j(k) = P(o_t = v_k | i_t = q_j),\, k=1,2,\cdots,M; j=1,2,\cdots,N
\tag{10.4}
$$
是在时刻t出于状态$q_j$的条件下生成观测$v_k$的概率
$\pi$是初始状态概率向量：
$$
\pi = (\pi_i)
\tag{10.5}
$$
其中，
$$
\pi_i = P(i_t = q_i),\,i=1,2,\cdots,N
\tag{10.6}
$$
是时刻$t=1$处于状态$q_i$的概率

隐马尔可夫模型由初始状态概率由向量$\pi$，状态转移概率矩阵A和观测概率矩阵B决定，$\pi$和A决定状态序列，B决定观测序列。因此，隐马尔可夫模型$\lambda$可以用三元符号表示，即：
$$
\lambda = (A,B,\pi)
\tag{10.7}
$$
$A,B,\pi$称为隐马尔可夫模型的三要素

状态转移概率矩阵A与初始状态概率向量$\pi$确定了隐马尔可夫链，**生成不可观测的状态序列**，即I
观测概率矩阵B确定了如何从状态生成观测，与状态徐磊综合确定了如何产生观测序列

隐马尔可夫模型作了两个基本假设：
1. 其次马尔可夫性假设
即：假设隐藏的马尔可夫链在任意时刻t的状态只依赖于前一时刻的状态，与其他时刻的状态及观测无关，也与时刻t无关
$$
P(i_t | i_{t-1},o_{t-1},\cdots,i_1,o_1)=P(i|i_{t-1}),\, t=1,2,\cdots,T
\tag{10.8}
$$
2. 观测独立性假设
即：假设任意时刻的观测只依赖于该时刻的马尔可夫链的状态，与其他观测及状态无关
$$
P(o_t | i_T,o_T,i_{T-1},o_{T-1},\cdots,i_{t+1},o_{t+1},i_t,i_{t-1},o_{t-1},\cdots,i_1,o_1) = P(o_t|i_i)
\tag{10.9}
$$
隐马尔可夫模型可以用于标注，这时状态对应着标记。标注问题是给定观测序列预测其对应的标记序列。可以假设标注问题的数据是由隐马尔可夫模型生成的。这样我们可以利用隐马尔可夫模型的学习与预测算法进行标注


### 10.1.2 观测序列的生成过程

根据隐马尔可夫模型定义，可以将一个长度为T的观测序列$O=(o_1,o_2,\cdots,o_T)$的生成过程描述如下：

#### 算法10.1（观测序列的生成）
** 输入：** 隐马尔可夫模型$\lambda = (A,B, \pi)$，观测序列长度T
** 输出：** 观测序列$O=(o_1,o_2,\cdots,o_T)$

1. 按照初始状态分布$\pi$产生状态$i_1$
2. 令t=1
3. 按照状态$i_t$的观测概率分布$b_{i_t}(k)$生成$o_t$
4. 按照状态$i_t$的状态转移概率分布$\{ a_{i_t i_{t+1}} \}$产生状态$i_{t+1}$，$i_{t+1} = 1,2,\cdots,N$
5. 令$t=t+1$；如果$t<T$，转3，否则终止


### 10.1.3 隐马尔可夫模型的3个基本问题

1. 概率计算问题
已知：模型$\lambda=(A,B,\pi)$和观测序列$O=(o_1,o_2,\cdots,o_T)$
求：模型$\lambda$下观测序列O出现的概率$P(O|\lambda)$
2. 学习问题
已知：观测序列O
求：模型$\lambda$参数，使得在该模型下观测序列概率$P(O|\lambda)$最大。即，用极大似然估计的方法估计参数
3. 预测问题
也称为解码（decoding）问题
已知：模型$\lambda=(A,B,\pi)$和观测序列O
求：对给定观测序列条件概率$P(I|O)$最大的状态序列$I=(i_1,i_2,\cdots,i_T)$

## 10.2 概率计算算法

### 10.2.1 直接计算法
复杂度太高，不可行

### 10.2.2 前向算法
#### 定义10.2（前向概率）
给定马尔可夫模型$\lambda$，定义到时刻t部分观测序列为$o_1,o_2,\cdots,o_t$且状态为$q_i$的概率为前向概率，记作：
$$
\alpha_t(i) = P(o_1,o_2,\cdots,o_t,i_t = q_i | \lambda)
\tag{10.14}
$$
可以递推地求得前向概率$\alpha_t(i)$及观测序列概率$P(O|\lambda)$

#### 算法10.2（观测序列概率的前向算法）
** 输入：** 隐马尔可夫模型$\lambda$，观测序列O
** 输出：** 观测序列概率$P(O|\lambda)$

1. 初值
$$
\alpha_1(i) = \pi_i b_i (o_1), i=1,2,\cdots,N
\tag{10.15}
$$
2. 递推
对$t=1,2,\cdots,T-1$
$$
\alpha_{t+1}(i) = \left[ \sum_{j=1}^N \alpha_t(j) a_{ji} \right] b_i (o_{t+1}),\, i=1,2,\cdots,N
\tag{10.16}
$$
3. 终止
$$
P(O|\lambda) = \sum_{i=1}^N \alpha_T(i)
\tag{10.17}
$$
解释：

    - (10.15)中
        + $alpha_1(i)$：t=1时，$i_t=i_1=q_i$的概率
        + $\pi_i$：$i_1 = q_i$的概率
        + $b_i(o_1)$：$i=q_i$时，$o_1 = $指定v（也就是$o_1$）的概率
    - (10.16)中
        - $\alpha_{t+1}(i)$：t+1时刻，$i_{t+1}=q_i$且o序列确定的概率
        - $\alpha_t(j)$：t时刻，$i_t = q_i$的概率
        - $a_{ji}$：i从$q_j$变为$q_i$的概率
        - $\sum_{j=1}^N \alpha_t(j) a_{ji}$：所有变成$i_{t+1}=q_{i}$的可能性求和
        - $b_i(o_{t+1})$：$i=q_i$时，$o_{t+1} = $指定v（也就是$o_{t+1}$）的概率
    - (10.17)中
        - 因为
    $$
    \alpha_T(i) = P(o_1,o_2,\cdot,o_T,i_T=q_i | \lambda)
    $$
    所以
    $$
    P(O|\lambda) = \sum_{i=1}^N \alpha_T(i)
    $$
    也就是说，$\alpha_T(i)$是T时刻$i=q_i$，**且前序O已确定**的概率
    求和意味着：所有的可能性求和，即O的总体概率

这时的复杂度，由于可以直接用前一个时间的计算结果，所以复杂度是$O(N^2 T)$阶

### 10.2.3 后向算法
#### 定义10.3（后向概率）
给定隐马尔可夫模型$\lambda$，定义在时刻t状态为$q_i$的条件下，从$t+1$到T的部分，观测序列为$o_{t+1},o_{t+2},\cdots,o_T$的概率为后向概率，记作
$$
\beta_t(i) = P(o_{t+1},o_{t+2},\cdots,o_T|i_t = q_i, \lambda)
\tag{10.18}
$$

可以用递推的方法求得后向概率$\beta_t(i)$及观测序列概率$P(O|\lambda)$

#### 算法10.3（观测序列概率的后向算法）
**输入：** 隐马尔可夫模型$\lambda$，观测序列O
**输出：** 观测序列概率$P(O | \lambda)$

1. $$
\beta_T(i) = 1, i =1,2,\cdots,N
\tag{10.19}
$$
2. 对$t=T-1,T-2,\cdots,1$：
$$
\beta_t(i) = \sum_{j=1}^N \alpha_{ij} b_j(o_{t+1}) \beta_{t+1}(j),\,i=1,2,\cdots,N
\tag{10.20}
$$
3. $$
P(O| \lambda) = \sum_{i=1}^N \pi_i b_i (o_1) \beta_1(i)
\tag{10.21}
$$

利用前向概率和后向概率的定义，可以将观测序列概率$P(O|\lambda)$统一写成：
$$
P(O|\lambda) = \sum_{i=1}^N \sum_{j=1}^N \alpha_(i) a_{ij} b_j(o_{t+1}) \beta_{t+1}(j),\, t=1,2,\cdots,T-1
\tag{10.22}
$$

### 10.2.4 一些概率与期望值的计算
利用前向和后向概率，可以得到关于单个状态和两个状态概率的计算公式

1. 给定模型$\lambda$和观测$O$，在时刻t处于状态$q_j$的概率，记：
$$
\gamma_t(i) = P(i_t = q_i | O,\lambda)
\tag{10.23}
$$
可以通过前向后向概率计算，事实上：
$$
\gamma_t(i) = P(i_t = q_i | O,\lambda) = \frac{P(i_t = q_i, O| \lambda)}{P(O|\lambda)}
$$
于是得到：
$$
\gamma_t(i) = \frac{\alpha_t(i) \beta_t(i)}{P(O|\lambda)}
= \frac{\alpha_t(i) \beta_t(i)}{\sum_{j=1}^N \alpha_t(j) \beta_t(j)}
\tag{10.24}
$$

2. 给定模型$\lambda$和观测O，在时刻t处于状态$q_i$且在时刻t+1处于状态$q_j$的概率，记：
$$
\xi_t(i,j) = P(i_t = q_i, i_{t+1} = q_j | O,\lambda)
\tag{10.25}
$$
可以通过前后向概率计算：
$$
\xi_t(i,j) = \frac{P(i_t = q_i, i_{t+1} = q_j , O|\lambda)}{P(O|\lambda)}
= \frac{P(i_t = q_i, i_{t+1} = q_j , O|\lambda)}{\sum_{i=1}^N \sum_{j=1}^N P(i_t = q_i, i_{t+1} = q_j , O|\lambda)}
$$
而：
$$
P(i_t = q_i, i_{t+1} =q_j ,O | \lambda) = \alpha_t(i) a_{ij} b_j (o_{t+1}) \beta_{t+1}(j)
$$
所以
$$
\xi_{t}(i, j)= \frac {\alpha_{t}(i) a_{ij} b_{j} (o_{t+1} ) \beta_{t+1}(j)} {\sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{t}(i) a_{ij} b_{j} (o_{t+1}) \beta_{t+1}(j)}
\tag{10.26}
$$

3. 将$\gamma_t(i)$和$\xi_t(i,j)$对各个时刻t求和，可以得到一些有用的期望值
    1. 在观测O下状态i出现的期望值
    $$
    \sum_{t=1}^T \gamma_t(i)
    \tag{10.27}
    $$
    2. 在观测O下由状态i转移的期望值
    $$
    \sum_{t=1}^(T-1) \gamma_t(i)
    \tag{10.28}
    $$
    3. 在观测O下由状态i转移到状态j的期望值
    $$
    \sum_{t=1}^{T-1} \xi_t(i,j)
    \tag{10.29}
    $$

## 10.3 学习算法

### 10.3.1 监督学习方法

假设已给训练数据，包含S个长度相同的观测序列和对应的状态序列$\{(O_1,I_1),(O_2,I_2),cdots,(O_S,I_S) \}$
那么可以利用极大似然估计法来估计隐马尔可夫模型的参数
具体方法如下：

1. 转移概率$a_{ij}$的估计
设样本中时刻t出于状态i时刻，t+1转移到状态j的频数为$A_{ij}$，那么状态转移概率$a_{ij}$的估计是：
$$
\hat{a}_{ij} = \frac{A_{ij}}{\sum_{j=1}^{N} A_{ij}} ,\,i=1,2,\cdots,N;j=1,2,\cdots,N
\tag{10.30}
$$
2. 观测概率$b_j(k)$的估计
设样本中状态为j并观测为k的频数是$B_{jk}$，那么状态为j观测为k的概率$b_j{k}$的估计是:
$$
\hat{b_j}(k) = \frac{B_{jk}}{\sum_{k=1}^M B_{jk}},\,j=1,2,\cdots,N;k=1,2,\cdots,M
\tag{10.31}
$$
3. 初始状态概率$\pi_i$的估计$\hat{\pi}_i$，为S个样本中初始状态为$q_i$的概率


### 10.3.2 非监督学习（Baum-Welch算法）

假设给定训练数据只包含S个长度为T的观测$\{O_1,O_2,\cdots,O_S\}$而没有对应的状态序列，目标是学习隐马尔可夫模型$\lambda=(A,B,\pi)$的参数
将观测序列数据看作观测数据O，状态序列数据看作不可观测的隐数据I，那么隐马尔可夫模型事实上是一个含有隐变量的概率模型
$$
P(O|\lambda) = \sum_I P(O | I,\lambda) P(I|\lambda)
\tag{10.32}
$$
它的参数可以由EM算法实现

1. 确定完全数据的对数似然函数
所有观测数据写成$O=(o_1,o_2,\cdots,o_T)$，所有隐数据写成$I=(i_1,i_2,\cdots,i_T)$，完全数据是$(O,I)=(o_1,o_2,\cdots,o_T,i_1,i_2,\cdots,i_T)$
完全数据的对数似然函数是$\log P(O,I | \lambda)$
2. EM算法的E步：求Q函数$Q(\lambda, \bar{\lambda})$
$$
Q(\lambda, \bar{\lambda}) = \sum_I \log P(O,I | \lambda) P(O,I | \bar{\lambda})
\tag{10.33}
$$
注：这里略去了对$\lambda$而言的常数因子$\frac{1}{P(O|\bar{\lambda})}$
其中，$\bar{\lambda}$是隐马尔可夫模型参数的当前估计值，$\lambda$是要极大化的HMM参数
$$
P(O,I | \lambda) = \pi_{i_1} b_{i_1} (o_1) a_{i_1 i_2} b_{i_2} (o_2) \cdots a_{i_{T-1} i_T} b_{i_T} (o_T)
$$
于是函数$Q(\lambda,\bar{\lambda})$可以写成：
$$\begin{align}
Q(\lambda,\bar{\lambda}) =& \sum_I \log \pi_{i_1} P(O,I| \bar{\lambda}) \\
& + \sum_{I} \left( \sum_{t=1}^{T-1} \log a_{i_t i_{t+1}} \right) P(O, I | \bar{\lambda}) \\
&+ \sum_{I} \left( \sum_{t=1}^{T} \log b_{i_{t}} \left( o_{t} \right) \right) P(O, I | \bar{\lambda})
\tag{10.34}
\end{align}$$
式中求和项都是对所有训练数据的总长度T进行的
3. EM算法的M步：极大化Q函数$Q(\lambda,\bar{\lambda})$，求模型参数$A,B,\pi$
由于要极大化的参数在(10.34)中单独出现在了3个项中，所以对各项分别极大化：
    1. 第1项可以写成：
$$
\sum_{I} \log \pi_{i_{1}} P(O, I | \bar{\lambda})=\sum_{i=1}^{N} \log \pi_{i} P\left(O, i_{1}=i | \bar{\lambda}\right)
$$
注意到$\pi_i$满足约束条件$\sum_{i=1}^N \pi_i = 1$，利用拉格朗日乘子法，写出拉格朗日函数：
$$
\sum_{i=1}^{N} \log \pi_{i} P(O, i_{1}=i | \bar{\lambda}) + \gamma \left(\sum_{i=1}^{N} \pi_{i}-1\right)
$$
对其求偏导并令结果为0：
$$
\frac{\partial}{\partial \pi_{i}}\left[\sum_{i=1}^{N} \log \pi_{i} P (O, i_{1}=i | \bar{\lambda}) + \gamma\left(\sum_{i=1}^{N} \pi_{i}-1\right)\right]=0
\tag{10.35}
$$
得：
$$
P(O,i_1=i | \bar{\lambda}) + \gamma \pi_i = 0
$$
对i求和得到$\gamma$
$$
\gamma = -P(O|\bar{\lambda})
$$
代入(10.35)得：
$$
\pi_{i}=\frac{P(O, i_{1}=i | \bar{\lambda})}{P(O | \bar{\lambda})}
\tag{10.36}
$$
    2. 第2项可以写成
$$
\sum_{I} \left( \sum_{t=1}^{T-1} \log a_{i_t i_{t+1}} \right) P(O, I | \bar{\lambda})
= \sum_{i=1}^{N} \sum_{j=1}^{N} \sum_{t=1}^{T-1} \log a_{i j} P(O, i_{t}=i, i_{t+1}=j | \bar{\lambda})
$$
同1，利用约束条件$\sum_{j=1}^N a_{ij} = 1$，由拉格朗日乘子法可以求出：
$$
a_{i j}=\frac{\sum_{i=1}^{T-1} P (O, i_{t}=i, i_{t+1}=j | \bar{\lambda} )} {\sum_{t=1}^{T-1} P (O, i_{t}=i | \bar{\lambda})}
$$
    3. 第3项为
$$
\sum_{I} \left(\sum_{t=1}^{T} \log b_{i_{t}}(o_{t} )\right) P(O, I | \bar{\lambda}) = 
\sum_{j=1}^{N} \sum_{t=1}^{T} \log b_{j}(o_{t}) P(O, i_{t}=j | \bar{\lambda})
$$
同样用约束条件为$\sum_{k=1}^M b_j(k)$的拉格朗日乘子法
注意，只有在$o_t = v_k$时，$b_j(o_t)$对$b_j(k)$的偏导数才不为0.以$I(o_t=v_k)$表示。求得：
$$
b_{j}(k) = 
\frac {\sum_{i=1}^{T} P(O, i_{t}=j | \bar{\lambda} ) I(o_{t}=v_{k} )} {\sum_{i=1}^{T} P(O, i_{t}=j | \bar{\lambda})}
\tag{10.38}
$$

### 10.3.3 Baum-Welch模型参数估计公式
将(10.36)~(10.38)中的各概率分别用$\gamma_t(i)$和$\xi_t(i,j)$表示，可以将对应公式写成：
$$\begin{align}
a_{ij}=\frac{\sum_{i=1}^{T-1} \xi_{t}(i, j)}{\sum_{i=1}^{T-1} \gamma_t(i)} \tag{10.39} \\ 
b_{j}(k)=\frac{\sum_{t=1,o_t=v_k}^{T} \gamma_{t}(j)}{\sum_{t=1}^{T} \gamma_{t}(j)} \tag{10.40} \\ 
\pi_{i}=\gamma_{1}(i) \tag{10.41}
\end{align}$$

#### 算法10.4（Baum-Welch算法）
**输入：** 观测数据$O=(o_1,o_2,\cdots,o_T)$
**输出：** 隐马尔可夫模型参数

1. 初始化
对n=0，选取$a_{ij}^{(0)},b_j(k)^{(0)},\pi_i^{(0)}$，得到模型$\lambda^{(0)}=(A^{0},B^{(0)},\pi^{(0)})$
2. 递推。对n=1,2,\cdot
$$\begin{align}
a_{ij}^{(n+1)} = \frac{\sum_{i=1}^{T-1} \xi_{t}(i,j)} {\sum_{i=1}^{T-1} \gamma_t(i)} \\
b_{j}(k) ^{(n+1)} = \frac{\sum_{t=1,o_t=v_k}^{T} \gamma_{t}(j)}{\sum_{t=1}^{T} \gamma_{t}(j)} \\
\pi_i^{(n+1)} = \gamma_1(i)
\end{align}$$
右端各值按照观测$O=(o_1,o_2,\cdots,o_T)$和模型$\lambda^{(n)}=(A^{n},B^{(n)},\pi^{(n)})$计算
式中$\gamma_t(i)$和$\xi_t(i,j)$由(10.24)和(10.26)给出
3. 终止，得到模型参数$\lambda^{(n+1)}=(A^{n+1},B^{(n+1)},\pi^{(n+1)})$


## 10.4 预测算法

### 10.4.1 近似算法
思想：在每个时刻t选择在该时刻最有可能出现的状态$i_t^*$，从而得到一个状态序列$I^* = (i_1^*, i_2^*, \cdots, i_T^*)$，将它作为预测的结果
给定隐马尔可夫模型$\lambda$和观测序列O，在时刻t处于状态$q_i$的概率$\gamma_t(i)$
是：
$$
\gamma_{t}(i)=\frac{\alpha_{t}(i) \beta_{t}(i)}{P(O | \lambda)}=\frac{\alpha_{t}(i) \beta_{t}(i)}{\sum_{j=1}^{N} \alpha_{t}(j) \beta_{t}(j)}
\tag{10.42}
$$
在每个时刻t最有可能的状态$i_t^*$是：
$$
i_t^* = \arg \max_{1 \le i \le N} \left[\gamma_t(i) \right],\, t=1,2,\cdot,T
\tag{10.43}
$$
从而得到状态序列$I^* = (i_1^*, i_2^*, \cdots, i_T^*)$

近似算法的优点是计算简单，缺点是不能保证预测状态序列整体是最有可能的

### 10.4.2 维特比算法
本质使用动态规划解HMM预测问题，即用动态规划求概率最大路径（最优路径），这时一条路径对应着一个状态序列

思想：
从时刻t=1开始，递推地计算在时刻t状态为i的各条部分路径的最大概率，直到时刻T、状态为i的各条路径的最大概率
时刻t=T的最大概率几位最优路径的概率$P^*$，最优路径的终结点$i_T^*$也同时得到
之后，为了找出最优路径的各个节点，从终结点$i_T^*$开始，由后向前逐步求得节点$i_{T-1}^*,\cdots,i_1^*$，得到最优路径$I^* = (i_1^*, i_2^*, \cdots, i_T^*)$

首先导入两个变量$\delta$和$psi$。定义在时刻t状态为i的所有单个路径$(i_1,i_2,\cdots,i_t)$中概率最大值为
$$
\delta_t(i) = \max_{i_1,i_2,\cdots,i_{t-1}} P(i_t =i,i_{t-1},\cdots,i_1,o_t,\cdots,o_1 | \lambda),\, i=1,2,\cdots,N
\tag{10.44}
$$
由定义可得变量$\delta$的递推公式
$$\begin{align}
\delta_{t+1}(i) 
&= \max_{i_1,i_2,\cdots,i_{t}} P(i_{t+1} =i,i_{t},\cdots,i_1,o_{t+1},o_t,\cdots,o_1 | \lambda) \\
&= \max_{1 \le j \le N} \left[ \delta_t(j) a_{ji} \right] b_i(o_{t+1}) \\
i=1,2,\cdots,N;\ t=1,2,\cdots,T-1
\end{align}
\tag{10.45}
$$
定义在时刻t状态为i的所有单个路径$(i_1,i_2,\cdots,i_{t-1},i)$中，概率最大的路径的第$t-1$个结点（上一个结点）为：
$$
\psi_{t}(i)=\arg \max _{1 \leqslant j \leqslant N}\left[\delta_{t-1}(j) a_{j i}\right],\, i=1,2,\cdots,N; t=1,2,\cdots,T-1
\tag{10.46}
$$

#### 算法10.5（维特比算法）
**输入：** 模型$\lambda = (A,B,\pi)$和观测$O=(o_1,o_2,\cdots,o_T)$
**输出：** 最优路径$I^*=(i_1^*, i_2^*, \cdots, i_T^*)$

1. 初始化
$$\begin{align}
\delta_1(i) \pi_i b_i(o_1),\, i=1,2,\cdots,N \\
\psi_1(i) = 0,\, i=1,2,\cdots,N
\end{align}$$
2. 递推。对t=2,3,\cdots,T
$$
\begin{aligned}
\delta_{t}(i) &=\max _{1 \le j \le N} \left[\delta_{t-1}(j) a_{j i}\right] b_{i} (o_t),\, i=1,2,\cdots,N \\ 
\psi_{t}(i) &=\arg \max _{1 \le j \le N} \left[\delta_{t-1}(j) a_{j i}\right],\, i=1,2,\cdots,N
\end{aligned}
$$
3. 终止
$$\begin{aligned}
P^* = \max_{1 \le i \le N} \delta_T(i) \\
i_T^* = \arg \max_{i \le i le N} [\delta_T(i)]
\end{aligned}$$
4. 最优路径回溯
对$t=T-1,T-2,\cdots,1$
$$
i^* = \psi_{t+1} (i_{t+1}^*)
$$

求得最优路径$I^*=(i_1^*, i_2^*, \cdots, i_T^*)$



