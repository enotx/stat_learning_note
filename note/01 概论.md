# 目录
[TOC]


# 第1章 统计学习方法概论

## 1.1 统计学习

## 1.2 监督学习
### 1.2.1 基本概念
1. 输入空间、特征空间和输出空间
输入实例x的特征向量：
$x = (x^{(1)},x^{(2)},\cdots,x^{(n)})^T$
训练集
$T = \{(x_1, y_1),(x_2, y_2), \cdots, (x_N, y_N)\}$

2. 联合概率分布
X和Y具有联合概率分布$P(X,Y)$是监督学习关于数据的基本假设

3. 假设空间
假设空间是输入输出映射的集合（学习范围）
监督学习的模型可以是概率模型$P(Y|X)$或非概率模型$Y=f(X)$

### 1.2.2 问题的形式化

## 1.3 统计学习三要素
方法=模型+策略+算法

### 1.3.1 模型
假设空间用$\mathcal{F}$表示，假设空间可以定义为决策函数的集合：
$$\mathcal{F} = \{f|Y=f(X)\}
\tag{1.1}
$$
其中X,Y是定义在输入空间$\mathcal{X}$和输出空间$\mathcal{Y}$上的变量
这时$\mathcal{F}$通常是一个响亮参数决定的函数族：
$$\mathcal{F} = \{f|Y=f_\theta(X), \theta \in \mathbb{R}^n\}
\tag{1.2}
$$
参数向量\theta 取值于n维欧氏空间$\mathbb{R}^n$，称为参数空间
也可表示为条件概率的集合
$$\mathcal{F} = \{P|P(Y|X)\}
\tag{1.3}
$$
此时$\mathcal{F}$是条件概率分布族
$$\mathcal{F} = \{ P|P_\theta(Y|X), \theta \in \mathbb{R}^n \}
\tag{1.4}
$$

### 1.3.2 策略
【注】$f(X)$就是$\hat{Y}$吧
#### 1. 损失函数和风险函数
(1)0-1损失函数（0-1 loss function）
$$
L(Y, f(X)) =
\begin{cases}
1,& Y \ne f(X) \\
0,& Y = f(X)
\end{cases}
\tag{1.5}
$$
(2)平方损失函数（quadratic loss function）
$$L(Y, f(X)) = (Y - f(X))^2
\tag{1.6}
$$
(3)绝对损失函数（absolute loss function）
$$L(Y, f(X)) = |Y - f(X)|
\tag{1.7}
$$
(4)对数损失函数（logarithmic loss function）或对数似然损失函数（log-likelihood loss function）
$$L(Y, P(Y|X)) = -\log P(Y|X)
\tag{1.8}
$$

损失函数越小，模型越好，损失函数的期望是：
$$
R_{exp}(f) = E_p[L(Y, f(X))] = \int_{\mathcal{X} \times \mathcal{Y}} {L(y,f(x))P(x,y)dxdy}
\tag{1.9}
$$
【注】曲面向量积分？
【注2】$R_{exp}$应该可以理解为(expected risk)？
对给定的训练集T，其经验风险/经验损失$R_{exp}$:
$$
R_{exp}(f) = \frac{1}{N} \sum_{i=1}^{N}{L(y_i,f(x_i))}
\tag{1.10}
$$


#### 2. 经验风险最小化和结构风险最小化
**经验风险最小化（ERM，empirical risk minimization）**：
ERM的视角：经验风险最小的模型就是最优模型，也就是求解：
$$
\min_{f \in \mathcal{F}} \frac{1}{N} \sum_{i=1}^{N} L(y_i, f(x_i))
\tag{1.11}
$$
**【例】**极大似然估计就是经验风险最小化的例子，模型是条件概率分布，损失函数是对数损失函数时，经验风险最小化就等价于极大似然估计
样本容量很小时，容易过拟合（over-fitting）
**结构风险最小化（SRM, structural risk minimization）**，目的是防止过拟合
结构风险最小化等价于正则化（regularization），在经验风险的基础上加上表示模型复杂度的正则化项（regularizer）或罚项（penalty term）
在假设空间、损失函数及训练集确定的情况下，结构风险
$$
R_{srm}(f) = \frac{1}{N} \sum_{i=1}^N L(y_i,f(x_i)) + \lambda J(f)
\tag{1.12}
$$
其中J(f)为模型的复杂度，是定义在假设空间$\mathcal{F}$上的泛函
$\lambda \leq 0$是参数
**【例】**贝叶斯估计中的最大后验概率估计（maximum posterior probability estimation, MAP）就是结构风险最小化的一个例子
SRM的视角，结构风险最小的模型是最优的模型，也就是求：
$$
\min_{f \in \mathcal{F}} \frac{1}{N} \sum_{i=1}^{N} L(y_i, f(x_i)) + \lambda J(f)
\tag{1.13}
$$


### 1.3.3 算法

## 1.4 模型评估与模型选择
### 1.4.1 训练误差与测试误差
training error和test error
注：学习时采用的损失函数未必是评估时使用的损失函数，但一致较理想
训练误差：
$$
R_{exp}(\hat{f}) = \frac{1}{N} \sum_{i=1}^N L(y_i, \hat{f}(x_i))
\tag{1.14}
$$
测试误差：
$$
e_{test} = \frac{1}{N^t} \sum_{i=1}^{N^t} L(y_i, \hat{f}(x_i))
\tag{1.15}
$$
例如，当损失函数是0-1损失时，测试误差就变成了误差率（error rate）：
$$
e_{test} = \frac{1}{N^t} \sum_{i=1}^{N^t} I(y_i \ne \hat{f}(x_i))
\tag{1.16}
$$
其中I是指示函数，$y \ne \hat{f}(x)$时为1，否则为0
相应的，准确率（accuracy）为：
$$
r_{test} = \frac{1}{N^t} \sum_{i=1}^{N^t} I(y_i = \hat{f}(x_i))
\tag{1.17}
$$

### 1.4.2 过拟合与模型选择


## 1.5 正则化与交叉验证
### 1.5.1 正则化
在经验风险上加上一个正则化项或罚项
【注】正则化项和罚项是同一个东西的不同叫法
正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化项越大。比如：**可以是模型参数向量的范数**
正则化一般具有如下形式：
$$
\min_{f \in \mathcal{F}} \frac{1}{N} \sum_{i=1}^{N} L(y_i, f(x_i)) + \lambda J(f)
\tag{1.19}
$$
其中，$\lambda J(f)$就是正则化项
【例】回归问题中，损失函数是平方损失，正则化项可以是参数向量的$L_2$范数
$$
L(w) = \frac{1}{N} \sum_{i=1}{N}(f(x_i;w)-y_i)^2 + \frac{\lambda}{2} \lVert w \rVert ^2
$$
$w$是向量参数，$\lVert w \rVert$是其$L_2$范数
也可以是$L_1$范数
$$
L(w) = \frac{1}{N} \sum_{i=1}{N}(f(x_i;w)-y_i)^2 + {\lambda} \lVert w \rVert _1
$$


### 1.5.2 交叉验证
1. 简单交叉验证
随机将数据分为两部分：训练集&测试集
2. S折交叉验证
随机地将数据切分成S个互不相交的大小相同的子集，然后利用S-1个子集训练，余下的测试
重复进行S次
3. 留一交叉验证
S折的特殊情形，S=N。在数据缺乏时使用

## 1.6 泛化能力
### 1.6.1 泛化误差
如果学到的模型是$\hat{f}$，那么其对未知数据预测的误差即为泛化误差（generalization error）
$$
\begin{align}
& R_{exp}(\hat{f}) \\
=& E_p{[L(Y,\hat{f}(X))]} \\
=& \int_{\mathcal{X} \times \mathcal{Y}}L(y, \hat{f}(x))P(x,y) dxdy
\end{align}
\tag{1.20}
$$

### 1.6.2 泛化误差上界 
generalization error bound，性质有：

- 它是样本容量的函数，样本容量增加时，泛化上界趋于0
- 它是假设空间容量（capacity）的函数，假设空间容量越大，模型就约难学，泛化误差上界就越大

考虑一个**二类分类问题**，已知训练数据集$T=\{ (x_1,y_1),(x_2,y_2), \cdots ,(x_N, y_N) \}$，它是从联合概率分布$P(X,Y)$独立同分布产生的，$X \in \mathbb{R}^n$，$Y \in \{-1,+1\}$
假设空间是函数的有限集合$\mathcal{F} = \{f_1, f_2, \cdots, f_d \}$
d是函数个数
设f是从$\mathcal{F}中选取的函数，损失函数是0-1损失$
关于$f$的期望风险和经验风险分别是
$$
R(f) = E[L(Y,f(X))]
\tag{1.21}
$$
$$
\hat{R}(f) = \frac{1}{N} \sum_{i=1}^N L(y_i,f(x_i))
\tag{1.22}
$$
经验风险最小化函数是：
$$
f_N = \arg \min_{f \in \mathcal{F}} \hat{R}(f) 
\tag{1.23}
$$
其泛化能力：
$$R(f_N) = E[L(Y,f_N(X))]
\tag{1.24}
$$
的上界由如下定理给出：
**定理1.1 泛化误差上界**
对二类分类问题，当假设空间是有限个函数的集合$\mathcal{F}=\{f_1,f_2,\cdots,f_d\}$，对$\forall f \in \mathcal{F}$，至少以概率$1-\delta，以下不等式成立：
$$
R(f) \le \hat{R}(f) + \epsilon (d, N, \delta)
\tag{1.25}
$$
其中
$$\epsilon (d, N, \delta) = \sqrt{ \frac{1}{N} (\log {d} + \log {\frac{1}{\delta}}) }
\tag{1.26}
$$
不等式(1.25)左端是泛化误差（期望），右端是泛化误差上界
第二项$\epsilon(d,N,\delta)$是

- N的单调递减函数，训练集越大，值越小
- $\sqrt{\log d}$阶函数，空间$\mathcal{F}$包含函数越多（很可能意味着f阶数越高），值越大

引理：Hoeffding不等式
设$S_n = \sum_{i=1}^{n}X_i$是独立随机变量$X_1,X_2, \cdots, X_n$之和，$X_i \in [a_i, b_i]$，则对任意$t > 0$，有以下不等式成立：
$$
P(S_n - ES_n \ge t) \le \exp \left( \frac{-2t^2}{\sum_{i=1}^n(b_i-a_i)^2} \right)
\tag{1.27}
$$
$$
P(ES_n - S_n \ge t) \le \exp \left( \frac{-2t^2}{\sum_{i=1}^n(b_i-a_i)^2} \right)
\tag{1.27}
$$

证明如下：
对任意函数$f \in \mathcal{F}$，$\hat{R}(f)$是N个独立随机变量$L(Y,f(X))$的样本均值，$R(f)$是随机变量$L(Y,f(X))$的期望值
如果损失函数取值于区间$[0,1]$，即，对所有$i, [a_i.b_i]$ = [0,1]
由Hoeffding不等式(1.28)可知，对$\epsilon >0$，有：
$$
P(R(f)-\hat{R}(f) \ge \epsilon) \le \exp (-2N \epsilon ^2)
$$
由于$\mathcal{F}={f_1,f_2, \cdots, f_d}$是有限集合，所以
$$
\begin{align}
P(\exists f \in \mathcal{f}: R(f)-\hat{R}(f) \ge \epsilon) 
&= P(\bigcup_{f \in \mathcal{F}} \left\{ R(f)-\hat{R}(f) \ge \epsilon \right\}) \\
& \le \sum_{f \in \mathcal{F}} P(R(f)-\hat{R}(f) \ge \epsilon) \\
& \le d \exp (-2 N \epsilon ^2)
\end{align}
$$
等价的，对任意$f \in \mathcal{F}$，有
$$
P(R(f)-\hat{R}(f) \lt \epsilon) \ge 1- d \exp(-2 N \epsilon ^2)
$$
令
$$
\delta = d exp(-2N \epsilon ^ 2)
\tag{1.29}
$$
则
$$
P(R(f) \lt \hat{R}(f) + \epsilon) \ge 1- \delta
$$
即，至少以概率$1-\delta$，有$R(f) \lt \hat{R}(f) + \epsilon$，$\epsilon$经(1.29)变化，即为(1.26)

## 1.7 生成模型与判别模型
监督学习的模型一般形式为决策函数$Y=f(X)$，或者条件概率分布$P(Y|X)$
监督学习方法又可以分为生成方法（generative approach）和判别方法（discriminative approach），所学到的模型分别称为生成模型和判别模型

- 生成方法
由数据学习联合概率分布$P(X,Y)$，然后求条件概率分布$P(Y|X)$作为预测的模型，即生成模型：$P(Y|X) = P(X,Y)/P(X)$
典型的如：朴素贝叶斯法，隐马尔可夫模型
特点：收敛速度更快；存在隐变量时仍可用

- 判别模型
由数据直接学习决策函数f(X)或者条件概率分布$P(Y|X)$作为预测的模型
典型的如：k近邻法、感知机、决策树、逻辑斯蒂回归模型、最大熵模型、支持向量机、提升方法和条件随机场
特点：准确率更高；更可抽象、可简化

## 1.8 分类问题
当输出变量$Y$取有限个离散值时，预测问题便成为分类问题
监督学习从数据中学习一个分类模型或分类决策函数，称为分类器（classifier）
分类器从新的输入进行输出预测（prediction），称为分类（classification）
可能的输出称为类（class）
分类问题分为多类分类问题和二类分类问题，包括学习和分类（predict）两个过程
评价分类器性能的指标一般是分类准确率（accuracy），见(1.17)
对于二类分类问题，常用的评价指标是精确率（precision）与召回率（recall）
精确率定义为：
$$P = \frac{TP}{TP+FP}
\tag{1.30}
$$
召回率定义为
$$R = \frac{TR}{TP+FN}
\tag{1.31}
$$
此外还有F1值（精确率和召回率的调和均值），即
$$
\frac{2}{F_1} = \frac{1}{P} + \frac{1}{R} 
\tag{1.32}
$$
$$
F_1 = \frac{2TP}{2TP+FP+FN}
\tag{1.33}
$$
许多统计方法可用于分类，包括：k近邻法、感知机、朴素贝叶斯法、决策树、决策列表、逻辑斯蒂回归模型、支持向量机、提升方法、贝叶斯网络、神经网络、Winnow等

## 1.9 标注问题
标注（tagging）也是监督学习问题
标注问题可以认为是分类问题的一个推广，同时也是更复杂的结构预测（structure prediction）问题的简单形式
输入：观测序列；输出：标记序列或状态序列
分为学习和标注两个过程
给定的训练数据集$T = \{(x_1,y_1),(x_2,y_2), \cdots, (x_N,y_N)\}$
其中$x_i = (x_i^{(1)},x_i^{(2)}, \cdots, x_i^{(n)})^T, i=1,2,\cdots, N$是输入观测序列
$y_i = (y_i^{(1)},y_i^{(2)}, \cdots, y_i^{(n)})^T$是相应的输出标记序列，$n$是序列长度，对不同样本可以不同
学习系统基于训练数据集构建一个模型，表示为条件概率分布：$P(Y^{(1)}, Y^{(2)}, \cdots,Y^{(n)} | X^{(1)}, X^{(2)}, \cdots,X^{(n)})$
对观测序列$x_{N+1} = (x_{N+1}^{(1)}, x_{N+1}^{(2)}, \cdots, x_{N+1}^{(n)})^T$，找到使条件概率$P((y_{N+1}^{(1)}, y_{N+1}^{(2)}, \cdots, y_{N+1}^{(n)})^T|(x_{N+1}^{(1)}, x_{N+1}^{(2)}, \cdots, x_{N+1}^{(n)})^T)$最大的的标记序列$y_{N+1}=(y_{N+1}^{(1)}, y_{N+1}^{(2)}, \cdots, y_{N+1}^{(n)})^T$
评价指标与分类模型一样，常用的由标注准确率、精确率和召回率
常见的统计学习方法有：隐马尔可夫模型、条件随机场
典型的案例如词性标注

## 1.10 回归问题
回归（regression）是监督学习的另一个重要问题，用于预测输入变量（自变量）和输出变量（因变量）之间的关系
回归模型正是输入输出变量之间的映射函数。回归问题的学习等价于函数拟合（见1.4.2节）
回归问题分为学习和预测两个过程
给定一个训练数据集：$T = \{(x_1,y_1),(x_2,y_2), \cdots, (x_N,y_N)\}$
这里，${x_i \in \mathbb{R}^n}$是输入，${y_i \in \mathbb{R}^n}$是输出，$i=1,2,\cdots, N$
学习系统基于训练数据构建一个模型，即函数$Y=f(X)$；对新的输入$x_{N+1}$，根据模型$Y=f(X)$确定相应的输出$y_{N+1}$
回归问题按照输入变量的个数，分为一元回归和多元回归；按照输入变量和输出变量之间关系的类型，即模型的类型，分为线性回归和非线性回归
回归学习最常用的损失函数是平方损失函数，此时，回归问题可以由最小二乘法求解




