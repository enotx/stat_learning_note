[TOC]

$$
\DeclareMathOperator*{\avg}{\text{avg}}
$$

# 第5章 决策树
可以认为是if-then规则的集合；也可以认为是定义在特征空间与类空间上的条件概率分布

## 5.1 决策树模型与学习

### 5.1.1 决策树模型
#### **定义5.1（决策树）**
分类决策树模型是一种描述对实例进行分类的树形结构
决策树由结点（node）和有向边（directed edge）组成
结点有两种类型：内部结点（internal node）和叶结点（leaf node）
内部结点表示一个特征或属性；叶结点表示一个类

### 5.1.2 决策树与if-then规则

### 5.1.3 决策树与条件概率分布
决策树还表示给定特征条件下类的条件概率分布。这一条件概率分布在特征空间的一个划分（partition）上，讲特征空间划分为互不相交的单元（cell）或区域（region），并在每个单元定义了一个类的概率分布就构成了一个条件概率分布

### 5.1.4 决策树学习
假设给定训练数据集
$$
D =  \{ (x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N) \}
$$
其中$x_i = (x_i^{(1)},x_i^{(2)},\cdots,x_i^{(n)})^T$为输入实例（特征向量），n为特征个数，$y_i \in \{ 1,2,\cdots,K \}$为类标记，$i = 1,2,\cdots,N$，N为样本容量
学习的目标是根据给定的训练数据集构建一个决策树模型，使它能够对实例进行正确的分类
决策树学习的损失函数通常是正则化的极大似然函数，策略是以损失函数为目标函数的最小化
从所有可能的决策树中选择最优决策树是NP完全问题，所以常用启发式方法，近似求解这一最优化问题，得到次最优解（sub-optimal）
通常：递归地选择最优特征，并根据该特征对训练数据分割，使各子数据集有最好分类的过程。包含：特征选择、决策树生成、决策树剪枝

## 5.2 特征选择

### 5.2.1 特征选择问题

### 5.2.2 信息增益
先给出熵与条件熵的定义
熵（entropy）表示：随机变量不确定性的度量。设X是一个取有限个值的离散随机变量，其概率分布为：
$$
P(X = x_i) = p_i, i = 1,2,\cdots, n
$$
则随机变量X的熵定义为
$$
H(X) = - \sum_{i=1}^n p_i \log p_i
\tag{5.1}
$$
若$p_i=0$，则定义$0 \log 0 = 0$
通常，（5.1）中的对数以2为底或者以e为底，这时熵的单位分别称作比特（bit）或纳特（nat）
由定义可知，熵只依赖于X的分布，而与X的取值无关，所以也可将X的熵记作$H(p)$，即：
$$
H(p) = - \sum_{i=1}^n p_i \log p_i
\tag{5.2}
$$
熵越大，随机变量的不确定性就越大，从定义可验证
$$
0 \le H(p) \le \log n
\tag{5.3}
$$
当随机变量只取两个值，如1,0时，即X的分布为
$$
P(X=1) = p, P(X=0)=1-p, 0 \le p \le1
$$
熵为
$$
H(p) = -p \log_2 p - (1-p)\log_2(1-p)
\tag{5.4}
$$

设有随机变量(X,Y)，其联合概率分布为：
$$
P(X=x_i, Y=y_i) = p_{ij}，i=1,2,\cdots,n; j=1,2,\cdots,m
$$
条件熵$H(Y|X)$表示在已知随机变量X的条件下随机变量Y的不确定性，定义为X给定条件下Y的条件概率分布的熵对于X的数学期望：
$$
H(Y|X) = \sum_{i=1}^n p_i H(Y|X=x_i)
\tag{5.5}
$$
这里，$p_i = P(X=x_i), i=1,2,\cdots,n$

当熵和条件熵的概率由数据估计（特别是极大似然估计）得到时，对应的熵和条件熵分别称为经验熵（empirical entropy）和经验条件熵（empirical conditional entropy）

信息增益（information gain）表示得知特征X的信息而使得类Y的信息不确定性减少的程度

#### **定义5.2 （信息增益）**
特征A对训练数据集D的信息增益$g(D,A)$，定义为集合D的经验熵H(D)与特征A给定条件下，D的经验条件熵H(D|A)之差，即：
$$
g(D,A) = H(D) - H(D|A)
\tag{5.6}
$$
一般的，熵H(Y)与条件熵H(Y|X)之差称为互信息（mutual information），决策树学习中的信息增益等价于训练数据集中类与特征的互信息

根据信息增益准则的特征选择方法是：对训练数据集（或子集）D，计算其每个特征的信息增益，并比较他们的大小，选择信息增益最大的特征

设训练数据集为D，|D|表示其样本容量，即样本个数。
设有K个类$C_k$，$k = 1,2,\cdots,K$，$|C_k|$为属于类$C_k$的样本个数
设特征A有n个不同的取值$\{ a_1, a_2, \cdots, a_n \}$，根据特征A的取值将D划分为n个子集$D_1, D_2, \cdots, D_n$
$|D_i|$为$D_i$的样本个数，$\sum_{i=1}^n |D_i| = |D|$
记子集$D_i$中属于类$C_k$的样本的集合为$D_{ik}$，即$D_{ik} = D_i \cap C_k$，$|D_{ik}|$为$D_{ik}$的样本个数
信息增益算法如下：

#### **算法5.1 信息增益的算法**
**输入：** 训练数据集D和特征A
**输出：** 特征A对训练数据集D 的信息增益$g(D,A)$

1. 计算数据集D的经验熵$H(D)$
$$
H(D) = - \sum_{k=1}^K \frac{|C_k|}{|D|} \log_2 \frac{|C_k|}{|D|}
\tag{5.7}
$$
2. 计算特征A对数据集的经验条件熵$H(D|A)$
$$
\begin{aligned}
H(D|A) &= \sum_{i=1}^n \frac{|D_i|}{|D|} H(D_i) \\
&= - \sum_{i=1}^n \frac{|D_i|}{|D|} \sum_{k=1}^{K} \frac{|D_{ik}|}{|D_i|} \log_2 \frac{|D_{ik}|}{|D_i|}
\end{aligned}
\tag{5.8}
$$
3. 计算信息增益
$$
g(D,A) = H(D) - H(D|A)
\tag{5.9}
$$

### 5.2.3 信息增益比
以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题。
使用信息增益比(information gain ratio)可以对这一问题进行校正。这是特征选择的另一准则。

#### **定义5.3 （信息增益比）**
特征A对训练数据集D的信息增益比$g_R(D,A)$，定义为其信息增益与训练数据集D的关于特征A的值的熵$H_A(D)$之比：
$$
g_R (D,A) = \frac{g(D,A)}{H_A(D)}
\tag{5.10}
$$
其中
$$
H_A(D) = -\sum_{i=1}^n \frac{|D_i|}{|D|} \log_2 \frac{|D_i|}{|D|}
$$
n是特征A的取值个数

## 5.3 决策树的生成

### 5.3.1 ID3算法
核心是决策树上各个节点上应用信息增益准则选择特征，递归地构建决策树
具体方法：从根结点（root node）开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征为结点的特征，由该特征的不同取值建立子结点；再对子结点递归地调用以上方法，构建决策树；直到所有特征的信息增益均很小或没有特征可以选择为止
ID3相当于用极大似然法进行概率模型的选择

#### **算法5.2（ID3算法）**
**输入：** 训练数据集D，特征集A，阈值$\epsilon$
**输出：** 决策树T

1. 若D中所有实例属于同一类$C_k$，则T为单结点树，并将类$C_k$作为该结点的类标记，返回T
2. 若$A = \emptyset$，则T为单结点树，并将D中实例数量最大的类$C_k$作为该结点的类标记，返回T
3. 否则，按**算法5.1**计算A中各特征对D的信息增益，选择信息增益最大的特征$A_g$
4. 如果$A_g$的信息增益小于阈值$\epsilon$，则置T为单结点树，并将D中实例数量最大的类$C_k$作为该结点的类标记，返回T
5. 否则，对$A_g$的每一可能值$\alpha_i$，依$A_g = \alpha_i$将D分割为若干非空子集$D_i$，将$D_i$中实例数最大的类作为标记，构建子结点，由节点及其子结点构成数T，返回T；
6. 对第i个子结点，以$D_i$为训练集，以$A-\{A_g\}$为特征集，递归地调用1~5步，得到子树$T_i$，返回$T_i$

ID3算法只有树的生成，容易过拟合

### 5.3.2 C4.5的生成算法
基于ID3的改造，C4.5在生成的过程中，用信息增益**比**来选择特征

#### **算法5.3（C4.5的生成算法）**
**输入：** 训练数据集D，特征集A，阈值$\epsilon$
**输出：** 决策树T 

1. 若D中所有实例属于同一类$C_k$，则T为单结点树，并将类$C_k$作为该结点的类标记，返回T
2. 若$A = \emptyset$，则T为单结点树，并将D中实例数量最大的类$C_k$作为该结点的类标记，返回T
3. 否则，按式(5.10)计算A中各特征对D的信息增益比，选择信息增益比最大的特征$A_g$
4. 如果$A_g$的信息增益比小于阈值$\epsilon$，则置T为单结点树，并将D中实例数量最大的类$C_k$作为该结点的类标记，返回T
5. 否则，对$A_g$的每一可能值$\alpha_i$，依$A_g = \alpha_i$将D分割为若干非空子集$D_i$，将$D_i$中实例数最大的类作为标记，构建子结点，由节点及其子结点构成数T，返回T；
6. 对第i个子结点，以$D_i$为训练集，以$A-\{A_g\}$为特征集，递归地调用1~5步，得到子树$T_i$，返回$T_i$

## 5.4 决策树的剪枝

剪枝（pruning）:从已生成的树上裁掉一些子树或叶结点，并将其根结点或父结点作为新的叶结点，从而简化分类树模型

剪枝往往通过纪晓华决策树整体的损失函数或代价函数来实现
设树T的叶结点数为$|T|$，t是树T的叶结点，该叶结点有$N_i$个样本点
其中k类（注：$C_k$类？）的样本点有$N_{ik}$, $k = 1,2, \cdots, K$
$H_i(T)$为叶结点t上的经验熵，$\alpha \ge 0$为参数
则决策树的损失函数可以定义为：
$$
C_{\alpha}(T) = \sum_{t=1}^{|T|} N_t H_t(T) + \alpha |T|
\tag{5.11}
$$
其中经验熵为
$$
H_t(T) = - \sum_k \frac{N_{tk}}{N_t} \log{N_{tk}}{N_t}
\tag{5.12}
$$
在损失函数中，将式(5.11)右端的第一项记作
$$
C(T) = \sum_{t=1}^{|T|} N_t H_t(T) = - \sum_{t=1}^{|T|} \sum_{k=1}^{K} {N_{tk}} \log{N_{tk}}{N_t}
\tag{5.13}
$$
这时有：
$$
C_{\alpha}(T) = C(T) = \alpha |T|
\tag{5.14}
$$

其中，C(T)表示模型对训练数据的预测误差（拟合程度），|T|表示模型的复杂度，参数$\alpha$控制两者之间的影响（越大则树越简单）
剪枝，就是当$\alpha$确定时，选择损失函数最小的模型，即：**损失函数最小的子树**。
(5.11)或(5.14)定义的损失函数的极小化，等价于正则化的极大似然估计

#### **算法5.4（树的剪枝算法）**
**输入：** 生成算法产生的整个树T，参数$\alpha$
**输出：** 修剪后的子树$T_{\alpha}$

1. 计算每个结点的经验熵
2. 递归地从树的叶结点向上回缩
设一组叶结点回缩到其父结点之前和之后的整体树分别是$T_B$和$T_A$，若其对应的损失函数：
$$
C_{\alpha}(T_B) \le C_{\alpha}(T_A)
\tag{5.15}
$$
则进行剪枝，即将其父结点变为新的叶结点
3. 返回2，知道不能继续为止，得到损失函数最小的子树$T_{\alpha}$

注：(5.15)只需要考虑两个树的损失函数的差，其计算可以在局部进行，即可用动态规划算法实现
注2：待理解动态规划算法怎么用于剪枝

## 5.5 CART算法
CART：classification and regression tree，分类与回归树，既可用于分类也可用于回归，以下统称决策树
CART是在给定输入随机变量$X$条件下，输出随机变量$Y$的条件概率分布的学习方法
CART假设决策树是二叉树，内部结点特征的取值为“是”和“否”（左=是），等价于递归地二分每个特征，将输入空间（特征空间）划分为有限个单元，并在这些单元上确定预测的概率分布，也就是在输入给定的条件下输出的条件概率分布

CART算法由一下两步组成：

1. 决策树生成：基于训练数据集生成决策树，生成的决策树要尽量大
2. 决策树剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树，这时用损失函数最小作为剪枝的标准


### 5.5.1 CART生成
决策树的生成就是递归地构建二叉决策树的过程
- 对回归树用平方误差最小化准则
- 对分类树用基尼系数（Gini Index）最小化准则，进行特征选择，生成二叉树

#### 1. 回归树的生成
假设X与Y分别是输入和输出变量，并且Y是连续变量，给定训练数据集
$$
D = \{ (x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N) \}
$$
考虑如何生成回归树

一个回归树对应着输入空间（即特征空间）的一个划分以及在划分的单元上的输出值。假设已将输入空间划分成M个单元：$R_1, R_2, \cdots, R_M$，并且在每个单元$R_m$上有个固定的输出值$c_m$，于是回归树模型可表示为：
$$
f(x) = \sum_{m=1}^M c_m I(x \in R_m)
\tag{5.16}
$$

当输入空间的划分确定时，可以用平方误差$\sum_{x_i \in R_m} (y_i - f(x_i))^2$表示预测误差
易知，单元$R_m$上的$c_m$的最优值$\hat{c}_m$是$R_m$上的所有输入实例$x_i$对应的输出$y_i$的均值，即：
$$
\hat{c}_m = \avg (y_i | x \in R_m)
\tag{5.17}
$$

问题是怎样对输入空间进行划分，这里采用启发式的方法，选择第j个变量$x^{(j)}$和它取的值s，作为切分变量（splitting variable）和切分点（splitting point），并定义两个区域：
$$
\begin{aligned}
& R_1 (j,s) = \{x|x^{(j)} \le s \} \\
和 \ \  & R_2 (j,s) = \{x|x^{(j)} \gt s \} 
\end{aligned}
\tag{5.18}
$$
然后寻找最优切分变量j和最优切分点s。具体地，求解
$$
\min_{j,s} \left[ \min_{c_1} \sum_{x_i \in R_1(j,s)} (y_i - c_1)^2 + \min_{c_2} \sum_{x_i \in R_2(j,s)} (y_i - c_2)^2 \right]
\tag{5.19}
$$
对固定输入变量j可以找到最优切分点s：
$$
\begin{aligned}
& \hat{c}_1 = \avg (y_i | x_i \in R_1(j,s)) \\
和 \ \  & \hat{c}_2 = \avg (y_i | x_i \in R_2(j,s))
\end{aligned}
\tag{5.20}
$$
遍历所有输入变量，找到最优的切分变量j，构成一个对(j,s)，依次将输入空间分为两个两个区域，重复直到满足停止条件为止，得到回归树，称为：最小二乘回归树（least squares regression tree）

#### **算法5.5（最小二乘回归树生成算法）**
**输入：** 训练数据集D
**输出：** 回归树$f(x)$

在训练集数据所在的输入空间中，递归地将每个区域划分成两个子区域并决定每个子区域上的输出值，构建二叉决策树：

1. 选择最优切分变量j与切分点s，求解：
$$
\min_{j,s} \left[ \min_{c_1} \sum_{x_i \in R_1(j,s)} (y_i - c_1)^2 + \min_{c_2} \sum_{x_i \in R_2(j,s)} (y_i - c_2)^2 \right]
\tag{5.21}
$$
遍历变量j，对固定的切分变量j扫描切分点s，选择使(5.21)最小的对(j,s)
2. 用选定的对(j,s)划分区域并决定相应的输出值：
$$
\begin{aligned}
R_1 (j,s) = \{x|x^{(j)} \le s \} \ ，\  R_2 (j,s) = \{x|x^{(j)} \gt s \} \\
\hat{c}_m = \frac{1}{N} \sum_{x_i \in R_m(j,s)} y_i, x \in R_m, m=1,2
\end{aligned}
$$
3. 继续对两个子区域调用步骤1,2，知道满足停止条件
4. 将输入空间划分为M个区域$R_1,R_2,\cdots,R_M$，生成决策树：
$$
f(x) = \sum_{m=1}^{M} \hat{c}_m I(x \in R_m)
$$

【注】最小二乘回归树的5.21式还需要再想下，使c1+c2最小的(j,s)？总觉得有点奇怪


#### 2. 分类树的生成
分类树用基尼指数选择最优特征，同时决定该特征的最优二值切分点

#### **定义5.4（基尼指数）**
假设有K个类，样本点属于第k类的概率为$p_k$，则概率分布的基尼指数定义为：
$$
\text{Gini}(p) = \sum_{k=1}^{K} p_k (1-p_k) = 1- \sum_{k=1}^K p_k^2
\tag{5.22}
$$
对于二类分类问题，若样本点属于第1个类的概率是p，则概率分布的基尼指数为：
$$
\text{Gini}(p) = 2p(1-p)
\tag{5.23}
$$
对于给定的样本集合D，其基尼指数为
$$
\text{Gini}(D) = 1-\sum_{k=1}^K \left( \frac{|C_k|}{|D|} \right) ^2
\tag{5.24}
$$
这里$C_k$是D中属于第k类的样本子集，K是类的个数

如果样本集合D根据特征A是否取某一可能值$\alpha$被分割成$D_1$和$D_2$两部分，即：
$$
D_1 = \left\{ (x,y) \in D | A(x) = \alpha \right\}, D_2 = D - D_1
$$
则在特征A的条件下，集合D的基尼指数定义为
$$
\text{Gini}(D,A) = \frac{|D_1|}{|D|} \text{Gini}(D_1) + \frac{|D_2|}{|D|} \text{Gini}(D_2)
\tag{5.25}
$$
基尼指数Gini(D)表示集合D的不确定性，基尼指数Gini(D,A)表示经$A=\alpha$分割后，集合D的不确定性，基尼指数值越大，样本集合的不确定性越大（与熵相似）

#### **算法5.6（CART生成算法）**
**输入：** 训练数据集D，停止计算的条件
**输出：** CART决策树

根据训练数据集，从根结点开始，递归地对每个结点进行以下操作，构建二叉决策树：
1. 对D计算现有特征对该数据集的基尼指数
对每个特征A，对其可能取的每个值$\alpha$，根据样本点对$A=\alpha$的的测试是“是”和“否”将D分割为$D_1$和$D_2$两部分，利用(5.25)计算$A=\alpha$时的基尼指数
2. 在所有可能的特征A以及他们所有可能的切分点$\alpha$中，选择基尼指数最小的特征及其对应的切分点，作为最优特征和最优切分点，并从现结点生成两个子结点，将训练数据集依特征分配到连个子结点中
3. 对两个子结点递归地调用1&2，直到满足停止条件
4. 生成CART决策树

算法停止计算的条件是：
- 节点中的样本个数小与预定阈值
- 样本集的基尼系数小于预定阈值 = 样本基本属于同一类
- 没有更多特征

注1：例5.4中提到，如果切分点的Gini指数相等时，可任意选一个

### 5.5.2 CART剪枝

CART剪枝算法从“完全生长”的决策树的底端剪去一些子树，使树/模型变简单
由两步组成
- 从生成算法产生的决策树$T_0$底端开始不断剪枝，直到$T_0$的根结点，形成一个子树序列$\{ T_0, T_1, \cdots, T_n \}$
- 通过交叉验证法在独立的验证数据集上对子树序列进行测试，从中选出最优子树

1. 剪枝，形成子树序列
在剪枝过程中，计算子树的损失函数：
$$
C_{\alpha} (T) = C(T) + \alpha |T|
\tag{5.26}
$$
其中，T为任意子树，C(T)为对训练数据的预测误差（如基尼指数），|T|为子树的叶结点个数，$\alpha \ge 0$为参数，$C_{\alpha}(T)$为参数是$\alpha$时的子树T的整体损失，参数$\alpha$用于权衡拟合程度和模型复杂度
从整体树$T_0$开始剪枝，对$T_0$的任意内部结点t，以t为单结点树的损失函数是：
$$
C_{\alpha}(t) = C(t) + \alpha
\tag{5.27}
$$
以t为根结点的子树$T_t$的损失函数是
$$
C_{\alpha}(T_t) = C(T_t) + \alpha |T_t|
\tag{5.28}
$$
当$\alpha = 0$及充分小时，有不等式：
$$
C_{\alpha}(T_t) < C_{\alpha}(t)
\tag{5.29}
$$
当$\alpha$增大时，在某一$\alpha$有：
$$
C_{\alpha}(T_t) = C_{\alpha}(t)
\tag{5.30}
$$
当$\alpha$再增大时，不等式(5.29)反向。只要$\alpha = \frac{C(t) - C(T_t)}{|T_t|-1}$，$T_t$与$t$有相同的损失函数值，而t的结点少，因此t比$T_t$更可取，对$T_t$进行剪枝
为此，对$T_0$中的每一内部结点t，计算：
$$
g(t) = \frac{C(t) - C(T_t)}{|T_t|-1}
\tag{5.31}
$$
它表示剪枝后整体损失函数减少的程度。在T_0中剪去g(t)最小的$T_t$，将得到的子树作为$T_1$，同时将最小的$g(t)$设为$\alpha_1$。$T_1$为区间$[\alpha_1,\alpha_2)$的最优子树
如此剪枝下去，直到得到根结点。在这一过程中，不断增加$\alpha$的值，产生新的区间

2. 在剪枝得到的子树序列$T_0, T_1, \cdots, T_n$中通过交叉验证选取最优子树$T_{\alpha}$
利用独立的验证数据集，测试子树序列中各子树的平方误差或基尼系数，最小的决策树被认为是最优的决策树
子树序列中，每棵子树都对应一个参数$\alpha_1,\alpha_2,\cdots,\alpha_n$，所以，最优子树$T_k$确定时，对应$\alpha_k$也确定，即得到最优决策树$T_{\alpha}$

#### **算法5.7（CART剪枝算法）**
**输入：** CART算法生成的决策树$T_0$
**输出：** 最优决策树$T_{\alpha}$

1. 设$k=0, T=T_0$
2. 设$\alpha = + \infty$
3. 自下而上地对各内部结点t计算$C(T_t)$，$|T_t|$以及
$$
g(t) = \frac{C(t) - C(T_t)}{|T_t|-1}\\
\alpha = \min (\alpha, g(t))
$$
这里，$T_t$表示以t为根结点的子树，$C(T_t)$是对训练数据的预测误差，$|T_t|$是$T_t$的**叶结点**个数
4. 对$g(t) = \alpha$的内部结点t进行剪枝，并对叶结点t以多数表决法决定其类，得到树T
5. 设k=k+1，$\alpha_k = \alpha$，$T_k = T$
6. 如果T不是由根结点及两个叶结点构成的树，回到步骤2，否则令$T_k=T_n$
7. 采用交叉验证法在字数序列$T_0, T_1, \cdots, T_n$中选取最优子树$T_\alpha$
