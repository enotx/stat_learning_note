$$
\DeclareMathOperator*{\sign}{\text{sign}}
$$


[TOC]

# 第8章 提升方法

早分类问题中，它通过改变训练样本的权重，学习多个分类器，并将这些分类器进行线性组合，提高分类的性能

## 8.1 提升方法AdaBoost算法
### 8.1.1 提升方法的基本思路

### 8.1.2 AdaBoost算法
**输入：** 假设给定一个二类分类的训练数据集$T=\{(x_i,y_i),(x_2,y_2),\cdots,(x_N,y_N)\}$，其中$x_i \in \mathcal{X} \subseteq \mathbb{R}^n$，$y_i \in \mathcal{Y} = \{-1,+1\}$；弱学习算法
**输出：** 最终分类器$G(x)$

1. 初始化训练数据的权值分布
$$
D_1 = (w_{11},\cdots,w_{1i},\cdots,w_{1N})，w_{1i} = \frac{1}{N}, i=1,2,\cdots,N
$$
2. 对$m=1,2,\cdots,M$

- (a) 使用具有权值分布$D_m$的训练数据集学习，得到基本分类器
$$
G_m(x): \mathcal{X} \to \{-1,+1\}
$$
- (b) 计算$G_m(x)$在训练数据集上的分类误差率
$$
e_m = \sum_{i=1}^N P(G_m(x_i) \ne y_i) = \sum_{i=1}^N w_{mi} I(G_m(x_i) \ne y_i)
\tag{8.1}
$$
- (c) 计算$G_m(x)$的系数
$$
\alpha_m = \frac{1}{2} \log \frac{1-e_m}{e_m}
\tag{8.2}
$$
（这里的对数是自然对数）
- (d) 更新训练数据集的权值分布
$$\begin{align}
D_{m+1} = (w_{m+1,1},\cdots, w_{m+1,i},\cdots,w_{m+1,N}) \tag{8.3} \\
w_{m+1,i} = \frac{w_{mi}}{Z_m} \exp(-\alpha_m y_i G_m(x_i)),\, i=1,2,\cdots,N \tag{8.4}
\end{align}$$
这里，$Z_m$是规范化因子
$$
Z_m = \sum_{i=1}^N w_{mi} \exp (-\alpha_m y_i G_m(x_i))
\tag{8.5}
$$
它使$D_{m+1}$成为一个概率分布
3. 构建基本分类器的线性组合
$$
f(x) = \sum_{m=1}^M \alpha_m G_m(x)
\tag{8.6}
$$
得到最终分类器
$$
G(x) = \sign(f(x)) = \sign \left( \sum_{m=1}^M \alpha_m G_m(x) \right)
\tag{8.7}
$$

对AdaBoost有以下说明

- 步骤（1） 假设训练数据集具有均匀的权值分布，即每个训练样本在基本分类器的- 学习中作用相同，这一假设保证第1步能够在原始数据上学习基本分类器$G_1(x)$
- 步骤（2） AdaBoost反复学习基本分类器，在每一轮$m=1,2,\cdots,M$顺次地执行下列操作
    + (a) 使用当前分布$D_m$加权的训练数据集，学习基本分类器$G_m(x)$
    + (b) 计算基本分类器$G_m(x)$在加权训练数据集上的分类误差率
$$
e_m = \sum_{i=1}^N P(G_m(x_i) \ne y_i) = \sum_{Gm(x_i) \ne y_i} w_{mi}
\tag{8.8}
$$
这里，$w_{mi}$表示第m轮中第i个实例的权值，$\sum_{i=1}^N w_{mi} = 1$
这表明，$G_m(x)$在加权的训练数据集上的分类误差率，是被$G_m(x)$误分类样本的权值之和。由此可以看出数据权值分布$D_m$与基本分类器$G_m(x)$的分类误差率的关系
    - (c) 计算基本分类器$G_m(x)$的系数$\alpha_m$
$\alpha_m$表示$G_m(x)$在最终分类器中的重要性。由式(8.2)可知，当$e_m \le \frac{1}{2}$时，$\alpha_m \ge 0$，并且$\alpha_m$随着$e_m$的减小而增大。所以分类误差率越小的基本分类器在最终分类器中的作用越大
    - (d) 更新训练数据的权值分布为下一轮作准备。式(8.4)可以写成
$$
w_{m+i,i} = 
\begin{cases}
\frac{w_{mi}}{Z_m} e^{-\alpha_m}, G_m(x_i) = y_i \\
\frac{w_{mi}}{Z_m} e^{\alpha_m}, G_m(x_i) \ne y_i
\end{cases}
$$
即，不断扩大误分类样本的权值
- 步骤（3）线性组合$f(x)$实现M个基本分类器的加权表决。系数$\alpha_m$表示了基本分类器$G_m(x)$的重要性。
- 这里，所有$\alpha_m$之和并不为1，$f(x)$的符号决定实例x的类，$f(x)$的绝对值表示分类的却星都

利用基本分类器的线性组合构建最终分类器是AdaBoost的另一特点

### 8.1.3 AdaBoost的例子

## 8.2 AdaBoost算法的训练误差分析

#### **定理8.1（AdaBoost的训练误差界）**
AdaBoost算法最终分类器的训练误差界为
$$
\frac{1}{N} \sum_{i=1}^N I(G(x_i) \ne y_i) \le \frac{1}{N} \sum_i \exp(-y_i f(x_i)) = \prod_m Z_m
\tag{8.9}
$$
这里，$G(x),f(x)$和$Z_m$分别由式(8.7)、(8.6)、(8.5)给出

定理说明，可以在每一轮选取适当的$G_m$使得$Z_m$最小，从而使训练误差下降最快。对二类分类问题，有如下：

#### **定理8.2（二类分类问题AdaBoost的训练误差界）**
$$\begin{align}
\prod_{m=1}^M Z_m &= \prod_{m=1}^M [2 \sqrt{e_m(1-e_m)}] \\
&= \prod_{m=1}^M \sqrt{(1-4 \gamma_m ^2)} \\
&\le \exp \left( -2 \sum_{m=1}^M \gamma_m^2 \right)
\end{align}$$
这里，$\gamma_m = \frac{1}{2} - e_m$


#### **推论8.1**
如果存在$\gamma > 0$，对所有m有$\gamma_m \ge \gamma$，则
$$
\frac{1}{N} \sum_{i=1}^N I(G(x_i) \ne y_i) \le \exp(-2M \gamma ^2)
\tag{8.12}
$$
表明在此条件下，AdaBoost的训练误差是以指数速率下降的

注意，AdaBoost算法不需要知道下界$\gamma$

## 8.3 AdaBoost算法的解释

### 8.3.1 前向分步算法
考虑加法模型（additive model）
$$
f(x) = \sum_{m=1}^M \beta_m b(x;\gamma_m)
\tag{8.13}
$$
其中，$b(x;\gamma_m)$为基函数，$\gamma_m$为奇函数的参数，$\beta_m$为奇函数的系数。显然，(8.6)是一个加法模型
在给定训练数据及损失函数$L(y,f(x))$的条件下，学习加法模型$f(x)$成为经验风险极小化即损失函数最小化问题：
$$
\min_{\beta_m, \gamma_m} \sum_{i=1}^N L \left( y_i, \sum_{m=1}^M \beta_m b(x_i;\gamma_m) \right)
\tag{8.14}
$$
前向分布算法的思想是：由于学习的是加法模型，如果可以从前向后，每一步只学习一个基函数及其系数，逐步逼近优化目标函数式(8.14)，那么就可以简化复杂度。
具体地，每步只需优化如下损失函数
$$
\min_{\beta, \gamma} \sum_{i=1}^N L(y_i, \beta b(x_i;\gamma))
\tag{8.15}
$$

#### **算法8.2（前向分布算法）**
**输入：** 训练数据集$T=\{(x_i,y_i),(x_2,y_2),\cdots,(x_N,y_N)\}$；损失函数$L(y,f(x))$；基函数集$\{b(x;\gamma)\}$
**输出：** 加法模型$f(x)$

1. 初始化$f_0(x)=0$
2. 对$m=1,2,\cdots,M$
    - 极小化损失函数
$$
(\beta_m, \gamma_m) = \arg \min_{\beta,\gamma} \sum_{i=1}^N L(y_i, f_{m-1}(x_i) + \beta b(x_i:\gamma))
\tag{8.6}
$$
得到参数$\beta_m, \gamma_m$
    - 更新
$$
f_m(x) = f_{m-1}(x) + \beta_m b(x;\gamma_m)
\tag{8.17}
$$
3. 得到加法模型
$$
f(x) = f_M(x) = \sum_{m=1}^M \beta_m b(x;\gamma_m)
\tag{8.18}
$$
这样，前向分布算法将同时求解从m=1到M所有参数$\beta_m,\gamma_m$的优化问题简化为主次求解各个$\beta_m,\gamma_m$的优化问题

### 8.3.2 前向分布算法与AdaBoost
#### **定理8.3**
AdaBoost算法是前向分布加法算法的特例。这时，模型是由基本分类器组成的加法模型，损失函数是指数函数

## 8.4 提升树
### 8.4.1 提升树模型
提升方法实际采用加法模型（即基函数的线性组合）和前向分布算法
以决策树为基函数的提升方法称为提升树（boosting tree）
对分类问题，决策树是二叉分类树；对回归问题，决策树是二叉回归树
$$
f_M(x) = \sum_{m=1}^M T(x; \Theta_m)
\tag{8.23}
$$
其中$T(x; \Theta_m)$表示决策树；$\Theta_m$是决策树的参数；$M$为树的个数

### 8.4.2 提升树算法
提升树算法采用前向分布算法
首先确定初始提升树$f_0(x) = 0$，第m步的模型是
$$
f_m(x) = f_{m-1}(x) + T(x; \Theta_m)
\tag{8.24}
$$
其中$f_{m-1}(x)$为当前模型，通过经验风险极小化确定下一棵决策树的参数$\Theta_m$
$$
\hat{\Theta_m} = \arg \min_{\Theta_m} \sum_{i=1}^N L(y_i, f_{m-1}(x_i) + T(x_i; \Theta_m))
\tag{8.25}
$$

对二类分类问题，提升树算法只需：将算法8.1中的基本分类器限制为二类分类树即可

对回归问题时：
已知一个训练集T。如果将输入空间$\mathcal{X}$划分为J个互不相交的区域$R_1,R_2,\cdots,R_J$，并在每个区域上确定输出的常量$c_j$，那么树可以表示为：
$$
T(x; \Theta) = \sum_{j=1}^{J} c_j I(x \in R_j )
\tag{8.26}
$$
其中，参数$\Theta = \{(R_1,c_1),(R_2,c_2),\cdots,(R_J,c_J)\}$表示数的区域划分和各区域上的常数。J是回归树的复杂度、即叶结点个数
回归问题提升树使用以下前向分步算法：
$$\begin{align}
& f_0(x) = 0 \\
& f_m(x) = f_{m-1}(x) + T(x;\Theta_m), m=1,2,\cdots,M \\
& f_M(x) = \sum_{m=1}^M T(x;\Theta_m)
\end{align}$$
在前向分步算法的第m步，给定当前模型$f_{m-1}(x)$，需求解
$$
\hat{\Theta_m} = \arg \min_{\Theta_m} \sum_{i=1}^N L(y_i, f_{m-1}(x_i) + T(x_i; \Theta_m))
$$
得到$\hat{\Theta_m}$，即第m棵树的参数

当蚕蛹平方损失函数时，
$$
L(y,f(x)) = (y-f(x))^2
$$
其损失变为
$$\begin{align}
& L(y,f_{m-1}(x) + T(x;\Theta_m))
= & [y-f_{m-1}(x) - T(x;\Theta_m)]^2
= & [r - T(x;\Theta_m)]^2
\end{align}$$
这里，
$$
r = y - f_{m-1}(x)
\tag{8.27}
$$
是当前模型你和数据的残差（residual）。所以，对回归问题的提升树算法来说，只需简单你喝当前模型的残差

#### **算法8.3（回归问题的提升树算法）**
**输入：** 训练数据集$T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}$, $x_i \in \mathcal{X} \subseteq \mathbb{R}^n, y_i \in \mathcal{Y} \subseteq \mathbb{R} $
**输出：** 提升树$f_M(x)$

1. 初始化$f_0(x) = 0$
2. 对$m=1,2,\cdots,M$
    - a. 按式8.27计算残差
$$
r_{mi} = y_i - f_{m-1}(x_i), i=1,2,\cdots,N
$$
    - b. 拟合残差$r_{mi}$学习一个回归树，得到$T(x;\Theta_m)$
    - c. 更新$f_m(x) = f_{m-1}(x) + T(x;\Theta_m)$
3. 得到回归问题提升树
$$
f_M(x) = \sum_{m=1}^M T(x;\Theta_m)
$$
        

### 8.4.3 梯度提升
当损失函数不是平方损失和指数损失函数时，每一步的优化可能很难
故有梯度提升（gradient boosting）算法，这是利用最速下降法的近似方法，关键是利用损失函数的**负梯度**在当前模型的值
$$
 - \left[ \frac{\partial L(y,f(x_i))}{\partial f(x_i)} \right]_{f(x) = f_{m-1}(x)}
$$
作为回归问题提升树算法中的残差的近似值，拟合一个回归树

#### **算法8.4（梯度提升算法）**
**输入：** 训练数据集$T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}$, $x_i \in \mathcal{X} \subseteq \mathbb{R}^n, y_i \in \mathcal{Y} \subseteq \mathbb{R} $；损失函数$L(y,f(x))$
**输出：** 回归树$\hat{f}(x)$

1. 初始化
$$
f_0(x) = \arg \min _c \sum_{i=1}^N L(y_i,c)
$$
2. 对$m=1,2,\cdots,M$
    - a. 对$i=1,2,\cdots,N$，计算
$$
r_{mi} = - \left[ \frac{\partial L(y,f(x_i))}{\partial f(x_i)} \right]_{f(x) = f_{m-1}(x)}
$$
    - b. $r_{mi}$拟合一个回归树，得到第m棵树的叶结点区域$R_{mj}$，$j=1,2,\cdots,J$
    - c. 对$j=1,2,\cdots,J$，计算
$$
c_{mj} = \arg \min _c \sum_{x_i \in R_{mj}} L(y_i, f_{m-1}(x_i) +c)
$$
    - d. 更新$f_m(x) = f_{m-1}(x) + \sum_{j=1}^J c_{mj} I(x \in R_{mj})$
3. 得到回归树
$$
\hat{f}(x) = f_M(x) = \sum_{m=1}^M \sum_{j=1}^J c_{mj} I(x \in R_{mj})
$$
        






