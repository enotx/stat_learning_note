[TOC]

# 第6章 逻辑斯谛回归与最大熵模型

## 6.1 逻辑斯谛回归模型

### 6.1.1 逻辑斯谛分布

#### **定义6.1（逻辑斯谛分布）**
设X是连续随机变量，X服从逻辑斯谛分布是指X具有下列分布函数和密度函数：
$$
F(x) = P(X \le x) = \frac{1}{1+e^{-(x-\mu)/\gamma}}
\tag{6.1}
$$
$$
f(x) = F'(x) = \frac{e^{-(x-\mu)/\gamma}}{\gamma(1+e^{-(x-\mu)/\gamma})^2}
\tag{6.2}
$$
式中，$\mu$为位置参数，$\gamma > 0$为形状参数
![](http://cloud.enotx.com/index.php/s/QowzRTg9aGnLgTB/preview)

### 6.1.2 二项逻辑斯谛回归模型
binomial logistic regression model是一种分类模型，由条件概率分布P(Y|X)表示，形式为参数化的逻辑斯谛分布
随机变量X的取值为实数，随机变量Y的取值为1或0
通过监督学习的方法来估计模型参数

#### **定义6.2（逻辑斯谛回归模型）**
二项逻辑斯谛回归模型是如下的条件概率分布：
$$
P(Y=1|x) = \frac{\exp(w \cdot x + b)}{1+ \exp(w \cdot x + b)}
\tag{6.3}
$$
$$
P(Y=0|x) = \frac{1}{1+\exp(w \cdot x + b)}
\tag{6.4}
$$
这里$x \in \mathbb{R}^n$是输入，$Y \in \{0,1\}$是输出，$w \in \mathbb{R}^n和b \in mathbb{R}$是参数
$w$称为权值向量，$b$称为偏置，$w \cdot x$是内积

有时为了方便，将权值向量和输入向量加以扩充（把偏置也加入向量）
即：$w= (w^{(1)},w^{(2)},\cdots,w^{(n)},b)^T$，$x= (x^{(1)},x^{(2)},\cdots,x^{(n)},1)^T$，此时，回归模型可表示为：
$$
P(Y=1|x) = \frac{\exp(w \cdot x)}{1+ \exp(w \cdot x)}
\tag{6.5}
$$
$$
P(Y=0|x) = \frac{1}{1+\exp(w \cdot x)}
\tag{6.5}
$$

一个事件的几率（odds）是指该事件发生的概率与该事件不发生概率的比值(odds = $\frac{p}{1-p}$）。该事件的对数几率（log odds）或logit函数是：
$$
\text{logit}(p) = \log \frac{p}{1-p}
$$
对逻辑斯谛回归而言，由(6.5)和(6.6)可得：
$$
\log \frac{P(Y=1|x)}{1-P(Y=1|x)} = w \cdot x
$$
也就是说，在逻辑斯谛回归模型中，输出Y=1的对数几率是输入x的线性函数；或者说，输出Y=1的对数几率是由输入x的线性函数表示的模型，即逻辑斯谛回归模型

### 6.1.3 模型参数估计
逻辑斯谛回归模型学习时，对于给定的训练数据集$T=\{ (x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N) \}$，其中$x_i \in \mathbb{R}^n$，$y_i \in \{0,1\}$，可以应用极大似然估计法估计模型参数

设：$P(Y=1|x) = \pi (x), P(Y=0|x) = 1 - \pi (x)$
似然函数为：
$$
\prod_{i=1}^N \left[ \pi(x_i) \right] ^ {y_i} \left[ 1- \pi(x_i) \right] ^{1-y_i}
$$
对数似然函数为：
$$
\begin{aligned}
L(w) &= \sum_{i=1}^{N} \left[ y_i \log \pi(x_i) + (1-y_i) \log(1- \pi (x_i)) \right] \\
&= \sum_{i=1}^{N} \left[ y_i \log \frac{\pi(x_i)}{1-\pi(x_i)} + \log(1- \pi(x_i)) \right] \\
&= \sum_{i=1}^{N} \left[y_i(w \cdot x_i) - \log(1+\exp(w \cdot x_i)) \right]
\end{aligned}
$$
对$L(w)$求极大值，得到$w$的估计值
问题就变成了以对数似然函数为目标函数的最优化问题，逻辑斯谛回归学习中通常采用的方法是梯度下降法及拟牛顿法
假设$w$的极大似然估计值是$\hat{w}$，那么学到的逻辑斯谛回归模型为
$$
P(Y=1|x) = \frac{\exp(\hat{w} \cdot x)}{1+ \exp(\hat{w} \cdot x)} \\
P(Y=0|x) = \frac{1}{1+ \exp(\hat{w} \cdot x)}
$$

### 6.1.4 多项逻辑斯谛回归
multi-nominal logistic regression model，是二项逻辑斯谛的推广，用于多类分类
假设离散型随机变量Y的取值集合是$\{1,2,\cdots,K\}$，那么多项逻辑斯谛回归模型是：
$$
P(Y=k|x) = \frac{\exp(w_k \cdot x)}{1+\sum_{k=1}^{K-1} \exp(w_k \cdot x)}, k=1,2,\cdots, K-1
\tag{6.7}
$$
$$
P(Y=K|x) = \frac{1}{1+ \sum_{k=1}^{K-1} \exp (w_k \cdot x)}
\tag{6.8}
$$
这里，$x \in \mathbb{R}^{n+1}, w_K \in \mathbb{R}^{n+1}$
二项逻辑斯谛回归的参数估计法也可以推广到多项逻辑斯谛回归

【注】：这里不理解Y=K为什么是独立一种情况，要再查资料

## 6.2 最大熵模型
maximum entropy model是由最大熵原理推导实现

### 6.2.1 最大熵原理
最大熵原理认为：学习概率模型时，在所有可能的概率模型（分布）中，熵最大的模型是最好的模型。
通常用约束条件来确定概率模型的集合，所以，最大熵原理也可以表述为在满足约束条件的模型集合中，选取熵最大的模型

假设离散随机变量X的概率分布是$P(X)$，则其熵（参考5.2.2）是：
$$
H(P) = - \sum_x P(x) \log P(x)
\tag{6.9}
$$
熵满足下列不等式：
$$
0 \le H(P) \le \log |X|
$$
式中，|X|是X的取值个数。当且仅当X的分布是均匀分布时右边的等号成立，即，熵最大

### 6.2.2 最大熵模型的定义
假设分类模型是一个条件概率分布$P(Y|X)$，$X \in \mathcal{X} \subseteq \mathbb{R}^n$表示输入，$Y \in \mathcal{Y}$表示输出
这个模型表示的是对于给定的输入X，以条件概率$P(Y|X)$输出Y

给定一个训练数据集$T=\{ (x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N) \}$
学习的目标是用最大熵原理选择最好的分类模型

首先考虑模型应该满足的条件。给定训练数据集，可以确定联合分布$P(X,Y)$的经验分布和边缘分布P(X)的经验分布，分别以$\tilde{P}(X,Y)和\tilde{P}(X)$表示：
$$
\tilde{P}(X,Y) = \frac{v(X=x), Y=y}{N} \\
\tilde{P}(X) = \frac{v(X=x)}{N}
$$
其中v()表示频数，N表示样本容量

用特征函数（feature function）$f(x,y)$描述输入x和输出y之间的某一个事实，其定义是：
$$
f(x,y) = 
\begin{cases}
1,  & x和y满足某一事实 \\
0,  & 否则
\end{cases}
$$
它是一个二值函数，当x和y满足这个事实时取值为1，否则为0
特征函数$f(x,y)$关于经验分布$\tilde{P}(X,Y)$的期望值，用$E_{\tilde{P}}(f)$表示
$$
E_{\tilde{P}}(f) = \sum_{x,y} \tilde{P}(X,Y) f(x,y)
$$
特征函数$f(x,y)$关于模型$P(Y|X)$与经验分布$\tilde{P}(X)$的期望值，用$E_P (f)$表示
$$
E_P(f) = \sum_{x,y} \tilde{P}(x) P(y|x) f(x,y)
$$
如果模型能够获取训练数据中的信息，那么就可以假设这两个期望值相等，即：
$$
E_P(f) = E_{\tilde{P}}(f)
\tag{6.10}
$$
或
$$
\sum_{x,y} \tilde{P}(x) P(y|x) f(x,y) = \sum_{x,y} \tilde{P}(X,Y) f(x,y)
\tag{6.11}
$$
我们将(6.10)或(6.11)作为模型学习的约束条件
假如有n个特征函数$f_i(x,y), i=1,2,\cdots, n$，那么就有n个约束条件

#### **定义6.3（最大熵模型）**
假设满足所有约束条件的模型集合为
$$
\mathcal{C} = \{ P \in \mathcal{P} | E_P (f_i)  = E_{\tilde{P}}(f_i) , i= 1,2,\cdots,n \}
\tag{6.12}
$$
定义在条件概率分布$P(Y|X)$上的条件熵
$$
H(P) = - \sum_{x,y} \tilde{P}(x) P(y|x) \log P(y|x)
\tag{6.13}
$$
则模型集合$\mathcal{C}$中熵$H(P)$最大的模型称为最大熵模型，式中的对数为自然对数

### 6.2.3 最大熵模型的学习
最大熵模型的学习过程就是求解最大熵模型的过程，可以形式化为约束最优化的问题

对于给定的训练数据集$T=\{ (x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N) \}$，以及特征函数$f_i(x,y), i=1,2,\cdots,n$，最大熵模型的学习等价于约束最优化问题：
$$
\max_{\mathcal{P} \in \mathcal{C}} \  H(P) = - \sum_{x,y} \tilde{P}(x) P(y|x) \log P(y|x) \\
s.t. \  E_P(f_i) = E_{\tilde{P}}(f_i), i = 1,2,\cdots,n \\
\sum_y P(y|x) = 1
$$
将最大化问题改写为等价的求最小值问题：
$$
\max_{\mathcal{P} \in \mathcal{C}} \ - H(P) = \sum_{x,y} \tilde{P}(x) P(y|x) \log P(y|x)
\tag{6.14}
$$
$$
s.t. \  E_P(f_i) = E_{\tilde{P}}(f_i), i = 1,2,\cdots,n
\tag{6.15}
$$
$$
\sum_y P(y|x) = 1
\tag{6.16}
$$

求解约束最优化问题(6.14)~(6.16)，所得出的解，就是最大熵学习模型的解

这里，将约束最优化的原始问题，转化为无约束最优化的对偶问题，通过求解对偶问题求解原始问题

首先，引进拉格朗日乘子$w_0,w_1,w_2,\cdots,w_n$，定义拉格朗日函数$L(P,w)$：

注：即$L(x,y,w) = f(x,y) + w (g(x,y)-c)$形式的改写，把约束条件变成一个凸优化问题
有些地方会把$w$写作$\lambda$
具体原理可以看这篇文章 [如何理解拉格朗日乘子](https://www.matongxue.com/madocs/939.html)

$$
\begin{aligned}
L(P,w) = 
& -H(P) + w_0 \left(1-\sum_y P(y|x) \right) + \sum_{i=1}^n w_i (E_{\tilde{P}}(f_i) - E_P(f_i)) \\
=& \sum_{x,y} {\tilde{P}}(x) P(y|x) \log P(y|x) + w_0 \left(1-\sum_y P(y|x) \right) +\\
&\sum_{i=1}^n w_i \left( \sum_{x,y} \tilde{P}(x,y) f_i(x,y) - \sum_{x,y} \tilde{P}(x) P(y|x) f_i(x,y) \right)
\end{aligned}
\tag{6.17}
$$
最优化的原始问题是：
$$
\min_{\mathcal{P} \in \mathcal{C}} \max_w L(P,w)
\tag{6.18}
$$
对偶问题是
$$
\max_w \min_{\mathcal{P} \in \mathcal{C}} L(P,w)
\tag{6.19}
$$
由于拉格朗日函数$L(P,w)$是P的凸函数，原始问题(6.18)和(6.19)是等价的
求解对偶问题(6.19)内部的极小化问题$\min_{\mathcal{P} \in \mathcal{C}} L(P,w)$，是关于$w$的函数，记作：
$$
\Psi(w) = \min_{\mathcal{P} \in \mathcal{C}} L(P,w) = L(P_w, w)
\tag{6.20}
$$
$\Psi(w)$称为对偶函数。同时将其解记作：
$$
P_w = \arg \min_{\mathcal{P} \in \mathcal{C}} L(P,w) = P_w(y|x)
\tag{6.21}
$$
具体地，求L(P,w)对P(y|x)的偏导数
$$
\begin{aligned}
\frac{\partial L(P,w)}{\partial P(y|x)} 
=& \sum_{x,y} \tilde{P}(x) (\log P(y|x) +1) - \sum_y w_0 - \sum_{x,y} \left( \tilde{P}(x) \sum_{i=1}^n w_i f_i(x,y)  \right) \\
=& \sum_{x,y} \tilde{P}(x) \left( \log P(y|x) +1 - w_0 -\sum_{i=1}^n w_i f_i (x,y) \right)
\end{aligned}
$$
令偏导数等于0，在$\tilde{P}(x) \gt 0$的情况下，解得
$$
P(y|x) = \exp \left( \sum_{i=1}^n w_i f_i(x,y) +w_0 -1 \right) = \frac {\exp \left( \sum_{i=1}^n w_i f_i (x,y) \right) }{\exp(1-w_0)}
$$
由于$\sum_y P(y|x) = 1$，得：
$$
P_w (y|x) = \frac{1}{Z_w(x)} \exp \left( \sum_{i=1}^n w_i f_i (x,y) \right)
\tag{6.22}
$$
其中，
$$
Z_w(x) = \sum_y \exp \left( \sum_{i=1}^n w_i f_i(x,y) \right)
\tag{6.23}
$$
$Z_w(x)$称为规范化因子；$f_i(x,y)$是特征函数；$w_i$是特征的权值
由式(6.22)和(6.23)表示的模型$P_w = P_w(y|x)$就是最大熵模型
$w$是最大熵模型中的参数向量

之后，求解对偶问题外部的极大化问题
$$
\max_w \Psi(w)
\tag{6.24}
$$
将其解记为$w^*$，即：
$$
w^*  = \arg \max_w \Psi(w)
\tag{6.25}
$$
这就是说，可以应用最优化算法，求对偶函数$\Psi(w)$的极大化，得到$w^*$，用来表示$P^* \in \mathcal{C}$

### 6.2.4 极大似然估计
最大熵模型是由(6.22)(6.23)表示的条件概率分布
下面证明对偶函数的极大化等价于最大熵模型的极大似然估计

已知训练数据的经验概率分布$\tilde{P}(X,Y)$，条件概率分布$P(Y|X)$的对数似然函数表示为：
$$
L_{\tilde{P}} (P_w) = \log \prod_{x,y} P(y|x)^{\tilde{P}(x,y)} = \sum_{x,y} \tilde{P}(x,y) \log P(y|x)
$$
当条件概率分布$P(y|x)$是最大熵模型(6.22)和(6.23)时，对数似然函数$L_{\tilde{P}}(P_w)$为：
$$
\begin{aligned}
L_{\tilde{P}}(P_w)
=& \sum_{x,y} \tilde{P}(x,y) \log P(y|x) \\
=& \sum_{x,y} \tilde{P}(x,y) \sum_{i=1}^n w_i f_i(x,y) - \sum_{x,y} \tilde{P}(x,y) \log Z_w(x) \\
=& \sum_{x,y} \tilde{P}(x,y) \sum_{i=1}^n w_i f_i(x,y) - \sum_{x} \tilde{P}(x) \log Z_w(x)
\end{aligned}
\tag{6.26}
$$

再看对偶函数$\Psi(w)$，由(6.17)(6.20)可得
$$
\begin{aligned}
\Psi(w) =& \sum_{x,y} {\tilde{P}}(x) P_w(y|x) \log P_w(y|x) + \sum_{i=1}^n w_i \left( \sum_{x,y} \tilde{P}(x,y) f_i(x,y) - \sum_{x,y} \tilde{P}(x) P_w(y|x) f_i(x,y) \right) \\
=& \sum_{x,y} {\tilde{P}}(x,y)  \sum_{i=1}^n w_i f_i(x,y) + \sum_{x,y} {\tilde{P}}(x) P_w(y|x) \left( \log P_w(y|x) - \sum_{i=1}^n w_i f_i(x,y) \right) \\
=& \sum_{x,y} {\tilde{P}}(x,y)  \sum_{i=1}^n w_i f_i(x,y) - \sum_{x,y} {\tilde{P}}(x) P_w(y|x) \log Z_w(x)
=& \sum_{x,y} {\tilde{P}}(x,y)  \sum_{i=1}^n w_i f_i(x,y) - \sum_{x} {\tilde{P}}(x) \log Z_w(x)
\end{aligned}
\tag{6.27}
$$

比较(6.26)和(6.27)，可得
$$
\Psi(w) = L_{\tilde{P}}(P_w)
$$
即证明，最大熵模型学习中的对偶函数极大化等价于最大熵模型的极大似然估计
这样，最大熵模型的学习问题就转化为具体求解对数似然函数极大化，或对偶函数极大化的问题
可以将最大熵模型写成更一般的形式
$$
P_w(y|x) = \frac{1}{Z_w(x)} \exp \left( \sum_{i=1}^n w_i f_i(x,y)  \right)
\tag{6.28}
$$
其中，
$$
Z_w(x) = \sum_y \exp \left( \sum_{i=1}^n w_i f_i(x,y)  \right)
\tag{6.29}
$$
这里，$x \in \mathbb{R}^n$为输入，$y \in \{1,2,\cdots,K \}$为输出，$w \in \mathbb{R}^n$为权值向量，$f_i(x,y), i=1,2,\cdots,n$为任意实值特征函数
最大熵模型与逻辑斯谛模型由类似的形式，它们又称为对数线性模型（lgo linear model）
模型学习就是在给定的训练数据条件下对模型进行极大似然估计或者正则化的极大似然估计

## 6.3 模型学习的最优化算法
逻辑斯谛回归模型、最大熵模型学习归结为以似然函数为目标函数的最优化问题，通常通过迭代算法求解

### 6.3.1 改进的迭代尺度法
improved iterative scaling，IIS是一种最大熵模型学习的最优化算法
已知最大熵模型为
$$
P_w(y|x) = \frac{1}{Z_w(x)} \exp \left( \sum_{i=1}^n w_i f_i(x,y)  \right)
$$
其中
$$
Z_w(x) = \sum_y \exp \left( \sum_{i=1}^n w_i f_i(x,y)  \right)
$$
对数似然函数为：
$$
L(w) = \sum_{x,y} \tilde{P}(x,y) \sum_{i=1}^n w_i f_i (x,y) - \sum_x \tilde{P}(x) \log Z_w(x)
$$
目标是通过极大似然估计学习模型参数，即求对数似然函数的极大值$\hat{w}$

IIS思想：假设最大熵模型当前的参数向量是$w=(w_1,w_2,\cdots,w_n)^T$，希望找到一个新的参数向量$w+\delta = (w_1+\delta,w_2+\delta, \cdots, w_n+\delta)^T$，使得模型的对数似然值增大
如果有这样一种向量更新的方法：$\tau: w \to w+\delta$，就可以重复使用，直到找到最大值

对于给定的经验分布$\tilde{P}(x,y)$，模型参数从$w$到$w+\delta$，对数似然函数的改变量是：
$$
\begin{aligned}
L(w+\delta)-L(w) 
&=\sum_{x, y} \tilde{P}(x, y) \log P_{w+\delta}(y | x)-\sum_{x, y} \tilde{P}(x, y) \log P_{w}(y | x) \\ &=\sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n} \delta_{i} f_{i}(x, y)-\sum_{x} \tilde{P}(x) \log \frac{Z_{w+\delta}(x)}{Z_{w}(x)} 
\end{aligned}
$$
利用不等式
$$
-\log \alpha \ge 1 - \alpha, \alpha \gt 0
$$
建立对数似然函数改变量的下界：
$$
\begin{aligned}
L(w+\delta)-L(w) 
& \ge \sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n} \delta_{i} f_{i}(x, y)+1-\sum_{x} \tilde{P}(x) \frac{Z_{w+\delta}(x)}{Z_{w}(x)} \\
&=\sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n} \delta_{i} f_{i}(x, y)+1-\sum_{x} \tilde{P}(x) \sum_{y} P_{w}(y | x) \exp \sum_{i=1}^{n} \delta_{i} f_{i}(x, y) 
\end{aligned}
$$
将右边记为
$$
A(\delta | w) = \sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n} \delta_{i} f_{i}(x, y)+1-\sum_{x} \tilde{P}(x) \sum_{y} P_{w}(y | x) \exp \sum_{i=1}^{n} \delta_{i} f_{i}(x, y) 
$$
于是有
$$
L(w+\delta) - L(w) \ge A(\delta | w)
$$
即，$A(\delta | w)$是对数似然函数改变量的下界。但$A(\delta | w)$中的$\delta$有多个变量，不易同时优化，IIS试图一次只优化其中一个变量$\delta_i$，而固定其他变量$\delta_j, i \ne j$
为达到这一目的，IIS进一步降低下界$A(\delta | w)$。具体地，IIS引进一个量$f^\#(x,y)$
$$
f^\#(x,y) = \sum_i f_i(x,y)
$$
则有；
$$
A(\delta | w)=
\sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n} \delta_{i} f_{i}(x, y)+1-\sum_{x} \tilde{P}(x) \sum_{y} P_{w}(y | x) \exp \left(f^\#(x, y) \sum_{i=1}^{n} \frac{\delta_{i} f_{i}(x, y)}{f^\#(x, y)}\right)
\tag{6.30}
$$
利用指数函数的凸性，以及对任意i，有$\frac{f_i(x,y)}{f^\# (x,y)} \ge 0$且 $\sum_{i=1}^n \frac{f_i(x,y)}{f^\# (x,y)} =1$。根据Jensen不等式，有：
$$
\exp \left(\sum_{i=1}^{n} \frac{f_{i}(x, y)}{f^\#(x, y)} \delta_{i} f^\#(x, y)\right\} \le \sum_{i=1}^{n} \frac{f_{i}(x, y)}{f^\#(x, y)} \exp \left(\delta_{i} f^\#(x, y)\right)
$$
(6.30)可以改写为：
$$
A(\delta | w) \ge \sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n} \delta_{i} f_{i}(x, y)+1-\sum_{x} \tilde{P}(x) \sum_{y} P_{w}(y | x) \sum_{i=1}^{n}\left(\frac{f_{i}(x, y)}{f^\#(x, y)}\right) \exp \left(\delta_{i} f^\#(x, y)\right)
\tag{6.31}
$$
记右端为：
$$
B(\delta | w) = \sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n} \delta_{i} f_{i}(x, y)+1-\sum_{x} \tilde{P}(x) \sum_{y} P_{w}(y | x) \sum_{i=1}^{n}\left(\frac{f_{i}(x, y)}{f^\#(x, y)}\right) \exp \left(\delta_{i} f^\#(x, y)\right)
$$
得到：
$$
L(w+\delta) - L(w) \ge B(\delta | w)
$$
这里$B(\delta | w)$是对数似然函数**改变量**的新的、相对不紧的下界
求$B(\delta | w)$对$\delta_i$的偏导数
$$
\frac{\partial B(\delta | w)}{\partial \delta_{i}} = \sum_{x,y} \tilde{P}(x,y) f_{i}(x,y) - \sum_x \tilde{P}(x) \sum_y P_{w}(y|x) f_{i}(x,y) \exp (\delta_i f^\#(x,y) )
\tag{6.32}
$$
(6.32)中，除$\delta_i$外不含其它任何变量，令偏导数为0得到：
$$
\sum_{x,y} \tilde{P}(x) P_{w}(y|x) f_{i}(x,y) \exp (\delta_i f^\#(x,y) ) = E_{\tilde{P}}(f_i)
\tag{6.33}
$$
于是，依次对$\delta_i$求解方程(6.33)可以求出$\delta$

#### **算法6.1（改进的迭代尺度算法IIS）**
**输入：** 特征函数$f_1, f_2, \cdots, f_n$；经验分布$\tilde{P}(X,Y)$，模型$P_w(y|x)$
**输出：** 最优参数值$w_i^*$；最优模型$P_{w^*}$

1. 对所有$i \in \{ 1,2,\cdots,n \}$，取初值$w_i=0$
2. 对每一$i \in \{ 1,2,\cdots,n \}$：
(a) 令$\delta_i$是方程
$$
\sum_{x,y} \tilde{P}(x) P_{w}(y|x) f_{i}(x,y) \exp (\delta_i f^\#(x,y) ) = E_{\tilde{P}}(f_i)
$$
的解，这里：
$$
f^\#(x,y) = \sum_i f_i(x,y)
$$
(b) 更新$w_i$值：$w_i \leftarrow w_i + \delta_i$
3. 如果不是所有$w_i$都收敛，重复2

这一算法的关键是2.a，即求方程(6.33)中的$\delta_i$。如果$f^\#(x,y)$是常数，即对任何x,y，有$f^\#(x,y) = M$，那么$\delta_i$可以显式地表示成：
$$
\delta_i = \frac{1}{M} \log \frac{E_{\tilde{P}}(f_i)}{E_P(f_i)}
\tag{6.34}
$$
如果$f^\#(x,y)$不是常数，那么必须通过数值计算求$\delta_i$，简单有效的方式是牛顿法
以$g(\delta_i) = 0$表示方程(6.33)，牛顿法通过迭代求得$\delta_i^*$，使得$g(\delta_i)=0$
迭代公式是：
$$
\delta_i^{(k+1)} = \delta_i^k - \frac{g(\delta_i^{(k)})}{g^\prime (\delta_i^{(k)})}
\tag{6.35}
$$

### 6.3.2 拟牛顿法

对于最大熵模型而言
$$
P_w(y|x) = \frac{\exp \left(\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right)}{\sum_{y} \exp \left(\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right)}
$$
目标函数：
$$
\min_{w \in \mathbb{R}^n} f(w) = 
\sum_x \tilde{P}(x) \log \sum_y \exp \left( \sum_{i=1}^n w_i f_i(x, y) \right)
- \sum_{x,y} \tilde{P}(x,y) \sum_{i=1}^{n} w_i f_i(x,y)
$$

梯度：
$$
g(w) = \left( \frac{\partial f(w)}{\partial w_1}, \frac{\partial f(w)}{\partial w_2}, \cdots, \frac{\partial f(w)}{\partial w_n} \right)^T
$$
其中
$$
\frac{\partial f(w)}{\partial w_i} = 
\sum_{x,y} \tilde{P}(x) P_w(y|x) f_i(x,y) - E_{\tilde{P}}(f_i), i=1,2,\cdots,n
$$
相应的拟牛顿法BFGS算法如下

#### **算法6.2（最大熵模型学习的BFGS算法）**
**输入：** 特征函数 $f_1,f_2, \cdots, f_n$；经验分布$\tilde{P}(x,y)$，目标函数$f(w)$，梯度$g(w) = \nabla f(w)$，精度要求$\epsilon$；
**输出：** 最优参数值$w^*$；最优模型$P_{w^*}(y|x)$

1. 选定初始点$w^{(0)}$，取$B_0$为正定对称矩阵，置$k=0$
2. 计算$g_k = g(w^{(k)})$。若$\lVert g_k \rVert < \epsilon$，则停止计算，则$w^* = w^{(k)}$；否则转3
3. 由$B_k p_k = -g_k$求出p_k
4. 一维搜索：求$\lambda_k$使得
$$
f(w^{(k)} + \lambda_k p_k) = \min_{\lambda \ge 0} f(w^{(k)} + \lambda p_k)
$$
5. 置$w^{(k+1)} = w^{(k)} + \lambda_k p_k$
6. 计算$g_{k+1} = g(w^{(k+1)})$，若$\lVert g_{k+1} \rVert < \epsilon $，则停止计算，得$w^* = w^{(k+1)}$；否则，按下式求出$B_{k+1}$：
$$
B_{k+1} = B_k + \frac{y_k y_k^T}{y_k^T \delta_k} - \frac{B_k \delta_k \delta_k^T B_k}{\delta_k^T B_k \delta_k}
$$
其中，$y_k = g_{k+1} - g_k，\delta_k = w^{(k+1)} - w^{(k)}$
7. 置$k=k+1$，转3


