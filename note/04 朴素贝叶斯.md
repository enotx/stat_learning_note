# 目录
[TOC]





# 第4章 朴素贝叶斯法

## 4.1 朴素贝叶斯法的学习与分类

### 4.1.1 基本方法
设输入空间$\mathcal{X} \subseteq \mathbb{R}^n$为n维向量的集合，输出空间为类标记集合$\mathcal{Y} = \{ c_1, c_2, \cdots, c_K \}$。输入为特征向量$x \in \mathcal{X}$，输出为类标记（class label）$y \in \mathcal{Y}$。X是定义在输入空间$\mathcal{X}$上的随机向量，Y是定义在输出空间$\mathcal{Y}$上的随机变量。$P(X,Y)$是X和Y的联合概率分布。
训练数据集
$$
T = \{ (x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N) \}
$$
由$P(X,Y)$独立同分布产生

朴素贝叶斯法通过训练数据集学习联合概率分布$P(X,Y)$。具体地，学习以下先验概率分布及条件概率分布
先验概率分布：
$$
P(Y=c_k), k= 1,2, \cdots, K
\tag{4.1}
$$

条件概率分布
$$
P(X = x|Y = c_k) = P(X^{(1)} = x^{(1)}, \cdots, X^{(n)} = x^{(n)} | Y = c_k), k=1,2, \cdots, K
\tag{4.2}
$$
于是学习到联合分布$P(X,Y)$

朴素贝叶斯对条件概率分布作了条件独立性假设：
$$
\begin{aligned}
P(X=x|Y=c_k) &= P(X^{(1)} = x^{(1)}, \cdots, X^{(n)} = x^{(n)} | Y = c_k), k=1,2, \cdots, K \\
&= \prod_{j=1}^n P(X^{(j)} = x^{(j)} | Y = c_k)
\end{aligned}
\tag{4.3}
$$

朴素贝叶斯分类时，对给定的输入x，通过学习到的模型计算后验概率分布$P(Y=c_k|X=x)$，将后验概率最大的类作为x的类输出。
后验概率计算根据贝叶斯定理进行：
$$
P(Y=c_k|X=x) = \frac{P(X=x|Y=c_k)P(Y=c_k)}{\sum_k P(X=x|Y=c_k)P(Y=c_k)}
\tag{4.4}
$$

将(4.3)带入(4.4)有：
$$
P(Y=c_k|X=x) = \frac{P(Y=c_k) \prod_j P(X^{(j)} = x^{(j)} | Y = c_k)} {\sum_k P(Y=c_k) \prod_j P(X^{(j)} = x^{(j)} | Y = c_k)} , k = 1,2, \cdots, K
\tag{4.5}
$$
这是朴素贝叶斯分类的基本公式。于是，朴素贝叶斯分类器可表示为：
$$
y = f(x) = \arg \max_{c_k} \frac{P(Y=c_k) \prod_j P(X^{(j)} = x^{(j)} | Y = c_k)} {\sum_k P(Y=c_k) \prod_j P(X^{(j)} = x^{(j)} | Y = c_k)}
\tag{4.6}
$$
注意到，在(4.6)中，分母对所有$c_k$都相同，故：
$$
y = f(x) = \arg \max_{c_k} P(Y=c_k) \prod_j P(X^{(j)} = x^{(j)} | Y = c_k)
\tag{4.7}
$$

### 4.1.2 后验概率最大化的含义
朴素贝叶斯将实例分到后验概率最大的类中，等价于期望风险最小化


## 4.2 朴素贝叶斯法的参数估计

### 4.2.1 极大似然估计
在朴素贝叶斯法中，学习意味着估计$P(Y=c_k)$和$P(X_{(j)} = x^{(j)} | Y=c_k)$
可以应用极大似然估计法估计相应的概率。先验概率$P(Y=c_k)$的极大似然估计是：
$$
P(Y=c_k) = \frac {\sum_{i=1}^N I(y_i = c_k)} {N}， k = 1,2, \cdots, K
\tag{4.8}
$$
设第j个特征$x^{(j)}$可能的取值集合为$\{ \alpha_{j1},\alpha_{j2}, \cdots, \alpha_{j S_j} \}$，条件概率$P(X^{(j)} = \alpha_{jl} | Y = c_k)$的极大似然估计是：
$$
P(X^{(j)} = \alpha _ {jl} | Y = c_k) = \frac {\sum_{i=1}^N I(X^{(j)} = \alpha_{jl} , Y = c_k)} {\sum_{i=1}^N I(y_i = c_k)} \\
j = 1,2, \cdots, n; l = 1,2, \cdots, S_j; k = 1,2,\cdots,K
\tag{4.9}
$$
式中，$x_i^{(j)}$是第i个样本的第j个特征；$\alpha_{jl}$是第j个特征可能取的第l个值；I为指示函数

【注】如果 $\theta$ 是已知确定的， x 是变量，这个函数叫做概率函数(probability function)，它描述对于不同的样本点 x ，其出现概率是多少。

如果 x 是已知确定的， $\theta$ 是变量，这个函数叫做似然函数(likelihood function), 它描述对于不同的模型参数，出现 x 这个样本点的概率是多少。

### 4.2.2 学习与分类算法 

#### **算法4.1 （朴素贝叶斯算法（naive Bayes algorithm））**

**输入：** 训练数据$T = \{ (x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N) \}$
其中$x_i = (x_i^{(1)},x_i^{(2)},\cdots,x_i^{(n)})^T$，$x_i^{(j)}$是第i个样本的第j个特征，$x_i^{(j)} \in \{ \alpha_{j1}, \alpha_{j2}, \cdots, \alpha_{j S_j} \}$，$\alpha_{jl}$是第j个特征可能取的第l个值，$j=1,2,\cdots,n$，$l=1,2,\cdots, S_j$，$y \in \{ c_1, c_2, \cdots, c_K \}$；实例x；
**输出：** 实例x的分类

1. 计算先验概率及条件概率
$$
P(Y=c_k) = \frac {\sum_{i=1}^N I(y_i = c_k)} {N}， k = 1,2, \cdots, K \\
P(X^{(j)} = \alpha _ {jl} | Y = c_k) = \frac {\sum_{i=1}^N I(X^{(j)} = \alpha_{jl} , Y = c_k)} {\sum_{i=1}^N I(y_i = c_k)}\\
j=1,2,\cdots,n; l=1,2,\cdots, S_j; k = 1,2, \cdots, K
$$
2. 对于给定的实例$x = (x^{(1)},x^{(2)},\cdots,x^{(n)})^T$，计算
$$
P(Y=c_k) \prod_{j=1}^n P(X^{(j)} = x^{(j)} | Y = c_k), k=1,2,\cdots,K
$$
3. 确定实例x的类
$$
y = \arg \max_{c_k} P(Y=c_k) \prod_{j=1}^n P(X^{(j)} = x^{(j)} | Y = c_k)
$$


### 4.2.3 贝叶斯估计
用极大似然估计可能会出现所要估计的概率值为0的情况，可能会影响后验概率的计算，使分类产生偏差，解决方法是贝叶斯估计
条件概率的贝叶斯估计是：
$$
P_{\lambda} (X^{(j)} = \alpha_{jl} | Y = c_k)) = \frac {\sum_{i=1}^{N} I(x^{(j)} = \alpha _{jl}, y_i = c_k) + \lambda} {\sum_{i=1}^N I(y_i = c_k) + S_j \lambda}
\tag{4.10}
$$
式中$\lambda \ge 0$
等价与在随机变量各个取值的频数上赋予一个正数$\lambda > 0$
$\lambda = 0$时就是极大似然估计；常取$\lambda = 1$，称为拉普拉斯平滑（Laplace smoothing）
显然，对任何$l = 1,2,\cdots, S_j$，$k=1,2,\cdots,K$，有
$$
P_{\lambda} (X^{(j)} = \alpha_{jl} | Y = c_k)) \gt 0 \\
\sum_{i=1}^{S_j} P (X^{(j)} = \alpha_{jl} | Y = c_k)) = 1
$$
表明，式(4.10)确为一种概率分布。同样，先验概率的贝叶斯估计是：
$$
P_{\lambda} (Y = c_k) = \frac{\sum_{i=1} ^N I(y_i=c_k) + \lambda}{N+K \lambda}
\tag{4.11}
$$

