# 目录
[TOC]


# 第2章 感知机
感知机（perception）是二类分类的线性分类模型，输出取$\{-1,+1\}$
感知机对应于输入控件，将实例划分为正负两类的分离超平面，属于判别模型

## 2.1 感知机模型
#### **定义2.1（感知机）**
假设输入空间（特征空间）是$\mathcal{X} \subseteq \mathbb{R}*n$，输出空间是$\mathcal{Y} = \{+1, -1\}$，输入$x \in \mathcal{X}$表示实例的特征向量，对应于输入空间（特征空间）的点；输出$y \in \mathcal{Y}$表示实例的类别
由输入空间到特征空间的如下函数：
$$
f(x) = \text{sign}(w \cdot x+b)
\tag{2.1}
$$
称为感知机
其中，$w,b$为感知机模型参数。$w \in \mathbb{R}^n$叫做权值（weight）或权值向量（weight vector），$b \in \mathcal{R}$叫做偏置（bias），$w \cdot x$表示$w和x$的内积，sign是符号函数，即:
$$
\text{sign} (x) = 
\begin{cases}
+1, & {x \ge 0}\\
-1, & {x \lt 0}
\end{cases}
\tag{2.2}
$$
感知机的假设空间是定义在特征空间中的所有线性分类模型/线性分类器，即函数集合：$\{f| f(x) = w \cdot x +b \}$
感知机的几何解释：
线性方程
$$
w \cdot x + b = 0
\tag{2.3}
$$
对应于特征空间$\mathbb{R}^n$中的一个超平面S，其中$w$是超平面的法向量，$b$是超平面的截距。超平面将特征空间分为两部分（正、负），称为分离超平面（separasting hyperplane）

## 2.2 感知机学习策略
### 2.2.1 数据集的线性可分性
#### **定义2.2（数据集的线性可分性**）
给定的数据集$T = \{ (x_1,y_1),(x_2,y_2), \cdots, (x_N,y_N) \}$
其中$x_i \in \mathcal{x} = \mathbb{R}^n, y_i \in \mathcal{Y} = \{ +1, -1 \}, i = 1,2, \cdots, N$
如果存在某个超平面S：$w \cdot x + b = 0$，可以将数据集的正实例点和负实例点完全划分，则称$T$线性可分，否则为线性不可分

### 2.2.2 感知机学习策略
损失函数：误分类点到超平面的总距离：
任意一点x_0到超平面的距离：$\frac{1}{\lVert w \rVert} | w \cdot x_0 +b| \\$
误分类数据：$-y_i (w \cdot x_i +b) \gt 0$
故，误分类点$x_i$到超平面S的距离是：
$$
- \frac{\Vert w \Vert} y_i (w \cdot x_i + b)
$$
其中，$\Vert w \Vert$是$w$的$L_2$范数
不考虑$\frac{1}{\Vert w \Vert}$，就得到感知机的损失函数：
$$
L(w,b) = - \sum_{x_i \in M}y_i(w \cdot x_i + b)
\tag{2.4}
$$
其中M为误分类点的集合

## 2.3 感知机学习算法
### 2.3.1 感知机学习算法的原始形式
对给定的训练集T，求参数w,b，使满足：
$$
\min{w,b}L(w,b) =  - \sum_{x_i \in M}y_i(w \cdot x_i + b)
\tag{2.5}
$$
感知机学习算法是误分类驱动的，具体采用随机梯度下降法（stochastic gradient descent）求解。极小化的过程不是一次使M中所有的误分类点的梯度下降，而是**一次随机选取一个误分类点使其梯度下降**
假设误分类点集合M固定，损失函数$L(w,b)$的梯度由：
$$
\nabla_w L(w,b) = -\sum_{x_i \in M}y_i x_i \\
\nabla_b L(w,b) =  -\sum_{x_i \in M}y_i
$$
给出。
随机选取误分类点$(x_i, y_i)$，对$w,b$进行更新：
$$
w := w + \eta y_i x_i
\tag{2.6}
$$
$$
b := b + \eta y_i
\tag{2.7}
$$
其中$\eta(0 \lt \eta \le 1)$是步长，又称为学习率（learning rate）


#### **算法2.1（感知机学习算法的原始形式）**
输入：训练数据集$T = \{ (x_1,y_1),(x_2,y_2), \cdots, (x_N,y_N) \}$，其中$x_i \in \mathcal{x} = \mathbb{R}^n, y_i \in \mathcal{Y} = \{ +1, -1 \}, i = 1,2, \cdots, N$；学习率$\eta(0 \lt \eta \le 1)$
输出：$w,b$；感知机模型$f(x) = \text{sign}(w \cdot x + b)$

1. 选取初值$w_0, b_0$
2. 在训练集中选取数据$(x_i, y_i)$
3. 如果$y_i(w \cdot x_i + b) \le 0$：
$$
w := w + \eta y_i x_i \\
b := b + \eta y_i
$$
4. 转到2，直到训练集中**没有**误分类点

【注】这里可以看出，感知机模型对数据集的要求非常高：严格要求是线性可分的

感知机模型根据初值或者误分类点选择不同，解可以不同

### 2.3.2 算法的收敛性
为便于推导，令：
$$
\hat{w} = (w^T, b)^T
\hat{x} = (x^T, 1)
$$
这样，$\hat{w}, \hat{x} \in \mathbb{R}^{n+1}，\hat{w} \cdot \hat{x} = w \cdot x + b$

**定理2.1 （Novikoff）**
设训练数据集$T = \{ (x_1,y_1),(x_2,y_2), \cdots, (x_N,y_N) \}$是线性可分的，其中$x_i \in \mathcal{x} = \mathbb{R}^n, y_i \in \mathcal{Y} = \{ +1, -1 \}, i = 1,2, \cdots, N$，则：

1. 存在满足条件$\Vert \hat{w}_{opt} \Vert = 1$的超平面$\hat{w}_{opt} \cdot \hat{x} = w_{opt} \cdot x + b_{opt} = 0$将T完全正确分开；且存在$\gamma > 0$，对所有$i = 1,2, \cdots, N$：
$$
y_i(\hat{w}_{opt} \cdot \hat{x}) = y_i (w_{opt} \cdot x + b_{opt}) \ge \gamma
\tag{2.8}
$$
2. 令$R = \max_{1 \le i \le N}{\Vert \hat{x_i} \Vert}$，则感知机算法2.1在训练集上的误分类次数k满足不等式
$$
k \le \left( \frac{R}{\gamma} \right) ^2
\tag{2.9}
$$

定理表明，误分类次数$k$有上界，即迭代是收敛的


### 2.3.3 感知机学习算法的对偶形式
对偶形式的基本思想是：将$w$和$b$表示为实例$x_i$和标记$y_i$的线性组合的显示，通过求解其稀疏二求得$w$和$b$
可设算法2.1中初值$w_0, b_0$均为0，对误分类点$(x_i, y_i)$通过
$$
w := w + \eta y_i x_i \\
b := b + \eta y_i
$$
逐步修改$w,b$。设修改$n$次，则$w,b$关于$(x_i,y_i)$的增量分别是$\alpha_i y_i x_i$和$\alpha_i y_i$，这里$\alpha_i = n_i \eta$
最后学习到的$w,b$可以分别表示为：
$$
w = \sum_{i=1}{N} \alpha_i y_i x_i
\tag{2.14}
$$
$$
b = \sum_{i=1}{N} \alpha_i y_i x_i
\tag{2.15}
$$
这里$\alpha_i \ge 0, i = 1,2,\cdots, N$，当$\eta = 1$时，表示第i个实例由于误分而更新的次数，实例点更新次数越多，意味着它距离分离超平面越近，也就越难正确分类

#### **算法2.2 （感知机学习算法的对偶形式）**
输入：训练数据集$T = \{ (x_1,y_1),(x_2,y_2), \cdots, (x_N,y_N) \}$，其中$x_i \in \mathcal{x} = \mathbb{R}^n, y_i \in \mathcal{Y} = \{ +1, -1 \}, i = 1,2, \cdots, N$；学习率$\eta(0 \lt \eta \le 1)$
输出：$\alpha ,b$；感知机模型$f(x) = \text{sign} \left( \sum_{j=1}^N \alpha_j y_j x_j \cdot x + b \right)$
其中$\alpha = (\alpha_1, \alpha_2, \cdots \alpha_N) ^T$

1. $\alpha := 0, b := 0$
2. 在训练集中选取数据$(x_i, y_i)$
3. 如果$y_i \left( \sum_{j=1}^N \alpha_j y_j x_j \cdot x + b \right) \le 0$
$$
\alpha_i := \alpha_i + \eta \\
b := b + \eta y_i
$$
4. 转至2直至没有误分类数据

对偶形式中，训练实例仅以内积的形式出现，为可以预先计算。这个矩阵就是Gram矩阵（Gram matrix）：
$$
G = \left[ x_i \cdot x_j \right]_{N \times N}
$$

和原始形式一样，对偶形式迭代也收敛、存在多个解


